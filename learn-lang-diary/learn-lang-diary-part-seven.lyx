#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage{url} 
\usepackage{slashed}
\end_preamble
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding utf8
\fontencoding global
\font_roman "times" "default"
\font_sans "helvet" "default"
\font_typewriter "cmtt" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures false
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks true
\pdf_pdfborder true
\pdf_colorlinks true
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry false
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 1
\use_package esint 0
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 0
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\listings_params "basicstyle={\ttfamily},basewidth={0.45em}"
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Language Learning Diary - Part Seven
\end_layout

\begin_layout Date
March 2022 - present
\end_layout

\begin_layout Author
Linas Vepštas
\end_layout

\begin_layout Abstract
The language-learning effort involves research and software development
 to implement the ideas concerning unsupervised learning of grammar, syntax
 and semantics from corpora.
 This document contains supplementary notes and a loosely-organized semi-chronol
ogical diary of results.
 The notes here might not always makes sense; they are a short-hand for
 my own benefit, rather than aimed at you, dear reader!
\end_layout

\begin_layout Section*
Introduction
\end_layout

\begin_layout Standard
Part Seven of the diary on the language-learning effort opens the door to
 next steps.
 The last round of experiments appear to be successful, and there do not
 seem to be any nagging unresolved issues.
 What comes next?
\end_layout

\begin_layout Section*
Summary Conclusions
\end_layout

\begin_layout Standard
A summary of what is found in this part of the diary:
\end_layout

\begin_layout Itemize
No summary yet.
\end_layout

\begin_layout Section*
The Possibilities
\end_layout

\begin_layout Standard
The last round of experiments appear to be successful, and there do not
 seem to be any nagging unresolved issues.
 What comes next? Here's a list of possbilities.
\end_layout

\begin_layout Itemize

\series bold
Accuracy Evaluation.

\series default
 Compare dictionaries to the hand-crafted LG English dict.
 This is a bit tedious and boring, since it seems unlikely to yeild anything
 interesting.
 It seems inevitable, as its the kind of thing other people want to see.
 The only benefit is that it is a way of perhaps characterizing the the
 effects of different parameter choices.
 In current runs, the 
\begin_inset Quotes eld
\end_inset

noise
\begin_inset Quotes erd
\end_inset

 parameter is the most highly explored: but what setting yeilds the best
 results? Comparing to LG should reveal the answer.
 Estimate a few weeks to a month of sustained effort.
\end_layout

\begin_layout Itemize

\series bold
Accuracy-Guided Exploration.
 
\series default
An automated accuracy-comparison system, comparing two different dictionaries,
 whatever their sources may be, could serve as a guide for exploration.
 For example, comparing learned dictionaries to the hand-crafted LG dict
 helps identify parameter regions that are effective.
 By contrast, comparing two different auto-generated dictionaries can indicate
 when two dicts diverge, and how sensitive they are to given parameter setttings.
\end_layout

\begin_layout Itemize

\series bold
Data Cleanup.

\series default
 During pair-counting and/or MPG parsing, there is a bug that repeatedly
 escapes backslashes, leading to a cascade of backslashes in the dataset.
 This is just junk, and should be fixed.
 Fixing it will surely improve quality.
 It's tedious and boring.
 Two ways to fix: (1) start from scratch (2) hunt out multiple backslashes,
 and perform a custom merge, just like a word-class merge, but without forming
 a word-class.
 Option (2) is maybe easier and faster, but requires crafting custom code.
 Maybe a few weeks to write this code, another few weeks to fully debug
 it.
 Option (1) is foundationally better but tedious and time consuming.
 Estimate a month of keeping a watchful eye on the progress of the data
 processing.
 Yuck, either way.
 
\end_layout

\begin_layout Itemize

\series bold
Morphology.

\series default
 We've ignored the morphological structure of English.
 Morphology is crucial for most Indoeuropean and Arabic langauges, and so
 coverage could be vastly improved by putting together code for automatic
 morphology detection/processing.
 Diary Part One already sketched how this could be done, including a worked
 example confirming that the idea will provide good results.
 Implementing this in code, and then performing the experiments to confirm
 it, is a relatively straight-forward affair.
 Time-consuming, but well within reach.
 Estimate six months of sustained effort; more time if interrupted.
 A motivated grad student could do this, might take 12-18 months.
\end_layout

\begin_layout Itemize

\series bold
Reiterate classification.

\series default
 Given the initial dictionaries, the corpus can be parsed with the LG parser,
 using those dictionaries.
 The result of such parsing is again a collection of disjuncts, much like
 the ones from MPG parsing, but with different observation counts.
 After collecting such counts, the classification step can be perfomed again,
 presumably resulting in a somewhat different classification, perhaps one
 that is more accurate?
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

This appears to be a technically easy step to take, as it just follows well-trod
 ground, mostly.
 A few weeks or a month of close supervision of the training runs.
\end_layout

\begin_deeper
\begin_layout Itemize
A variant of the above is to use the initial category assignment of the
 word as a 
\begin_inset Quotes eld
\end_inset

word-sense
\begin_inset Quotes erd
\end_inset

, and to tag the new disjunct with that word-sense.
 One way to do this would be to treat the initial disjunct as a 
\begin_inset Quotes eld
\end_inset

subscript
\begin_inset Quotes erd
\end_inset

, and so the same text-word, but with two different subscripts, is treated
 as two distinct words.
 Counts and further clustering continue to treat these as two different
 words, until/unless the second round of clustering erases the distinction.
 Handling this subscript-tagging requires new code; it is perhaps similar
 to cross-sensory tagging, e.g.
 if/when correlating with audio, video data.
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Refactorization.

\series default
 The disjuncts from above run provides a dataset that can be compared to
 the MPG-deived classes, and be used to refactor those, in several different
 ways.
 Perhaps some Sections are never used; they could be dropped.
 Perhaps a block-diagonal structure can be discovered.
 That is, a word-disjunct pair, the disjunct having N connectors, can be
 viewed s an N+1-rank tensor.
 Perhaps the collection of these tensors has some obvious diagonal structure.
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

Refactoring in this way feels like it might be both theoretically challenging,
 as well as presenting practical difficulties of discovering high-quality
 algorithms and then debugging them.
 This could easily take more than a few months.
 Compared to just re-iterating, this seems more difficult, more error-prone,
 and less robust.
\end_layout

\begin_layout Itemize

\series bold
Entities and References.

\series default
 A word-vector, for a given word, can be viewed in two ways.
 One way is to say that the disjunct describes the textual environment of
 the word: it's N-gram or skip-gram.
 Another way to think of it is that it captures the semantic embedding of
 the word; its a list of all of the 
\begin_inset Quotes eld
\end_inset

facts
\begin_inset Quotes erd
\end_inset

 known about that word.
 This is even more powerful, when the word is sense-tagged, i.e.
 tagged with the initial word-category.
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

There are two types of entitites: common entities and text-specific entities.
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
I want to write 
\begin_inset Quotes eld
\end_inset

common nouns
\begin_inset Quotes erd
\end_inset

, but in fact, the entities may be specific events in time, i.e.
 verbs.
 It would be awkward to write 
\begin_inset Quotes eld
\end_inset

common noun or common verb
\begin_inset Quotes erd
\end_inset

, so we'll just call them 
\begin_inset Quotes eld
\end_inset

entities
\begin_inset Quotes erd
\end_inset

.
\end_layout

\end_inset

 Common entites hold across all texts, such as 
\begin_inset Quotes eld
\end_inset

cat
\begin_inset Quotes erd
\end_inset

, 
\begin_inset Quotes eld
\end_inset

dog
\begin_inset Quotes erd
\end_inset

, 
\begin_inset Quotes eld
\end_inset

run
\begin_inset Quotes erd
\end_inset

, 
\begin_inset Quotes eld
\end_inset

jump
\begin_inset Quotes erd
\end_inset

.
 Text-specific entities occur in one text but not another: 
\begin_inset Quotes eld
\end_inset

John
\begin_inset Quotes erd
\end_inset

, which might be a different 
\begin_inset Quotes eld
\end_inset

John
\begin_inset Quotes erd
\end_inset

 in each text.
 
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

The most interesting/valuable task would be reference detection and reference
 resolution.
 How could this be done? A naive algo is to gather up a subset of a vector,
 specific to one text, and look for high-MI transitive relations.
 For example, 
\begin_inset Quotes eld
\end_inset

John ran the engine.
 It ran fine
\begin_inset Quotes erd
\end_inset

 has the relations 
\begin_inset Quotes eld
\end_inset

ran engine
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

it ran
\begin_inset Quotes erd
\end_inset

, which form a transitive relation between 
\begin_inset Quotes eld
\end_inset

it
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

engine
\begin_inset Quotes erd
\end_inset

.
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
Of course, the pairing of 
\begin_inset Quotes eld
\end_inset

John
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

it
\begin_inset Quotes erd
\end_inset

 can also be deduced.
\end_layout

\end_inset

 For this to work well, though, stems are needed, or, more properly speaking,
 lexical functions.
\end_layout

\begin_layout Itemize

\series bold
Long-distance correlations, Time.
 
\series default
Entity detection can be simplified if one introduces a time dimension, and,
 for each input stimulous (word), a decaying 
\begin_inset Quotes eld
\end_inset

activation
\begin_inset Quotes erd
\end_inset

.
 For example, if a word appears only in one text, but not another, and then
 reappears in a third text, then perhaps this is a different, unrelated
 entity? If some word has not been seen in a long time, then the new occurances
 can be assigned a distinct label.
 Input processing proceeds as before, accumulating stats for the new occurrrance.
 Later, during the classification phase, it can be determined if the various
 entities seem to be the same, of not.
\end_layout

\begin_layout Itemize

\series bold
Scenes; Limnal Spaces; Identifying Transitions.

\series default
 Humans conventionally organize knowledge into contextual groupings (how
 else could it be?) In theatre, these are scenes; in books, chapters with
 titles.
 
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

Scene detection can be hard-coded, in the processing input stage.
 It might also be detectable, as a zone where there are many activation
 changes (as measured in the previous bullet.) Inputs can be classified into
 
\begin_inset Quotes eld
\end_inset

eras
\begin_inset Quotes erd
\end_inset

 in this way, with different pheonmena in different eras presumably belonging
 to different 
\begin_inset Quotes eld
\end_inset

regimes
\begin_inset Quotes erd
\end_inset

.
\end_layout

\begin_layout Itemize

\series bold
Lexical Functions.

\series default
 This seems eminently important, but how? 
\end_layout

\begin_layout Itemize

\series bold
Synonymous Phrases.

\series default
 Word-classes are already a form of weak synonymy; how can one form strong
 synonymy? By appliying more stringent membership requirements? Based on
 current results, it appears that this would be enough, and that it would
 work fairly well.
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

Synonymous phrases require the ability to compare collections of partially-assem
bled disjuncts, to see how the connectors compare.
 This risks a combinatorial explosion.
 It does require new code.
\end_layout

\begin_deeper
\begin_layout Itemize
It might be possible and worthwhile to simultaneously fish for synonyms
 as well as grammar.
 Synonyms are already going to behave the same way grammatically, whereas
 part-of-speech groupings are much looser.
 This would result in a 
\begin_inset Quotes eld
\end_inset

multi-scale
\begin_inset Quotes erd
\end_inset

 dictionary, where each part-of-speech grouping can be further subdivided
 into synonym collections.
 Implementing this requires altering the 
\begin_inset Quotes eld
\end_inset

WordClass
\begin_inset Quotes erd
\end_inset

 construction to be marked with a class-type: a loose part-of-speech; a
 tighter synonym designation.
 This requires rejiggering the code a little bit; seems like a great idea.
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Set Phrases, Institutional Phrases, Idioms.

\series default
 These are groupings of words that occur more frequently together, than
 apart.
 How can these be identified? Why would we be interested in performing such
 an identification? Is it a stepping stone to something better?
\end_layout

\begin_layout Itemize

\series bold
Antonyms.

\series default
 A famous deficiency in neural net approaches is the inability to identify
 antonyms.
 The current code & theory is equally blind to antonyms.
 Yet this is deeply, fundamentally important for understanding.
\end_layout

\begin_layout Itemize

\series bold
Sound, Pictures, Blueprints, Video
\series default
 The approach to this is sketched elsewhere, already.
 This is a huge, multi-year project.
 Intersting, too.
 Will it impress anyone in the short term? Probably not? Who has time to
 do this? How can I nurture it along? At any rate, code should be altered
 to at least allow multi-sensory data streams, which is not possible right
 now.
 
\end_layout

\begin_layout Itemize

\series bold
Common-sense Reasoning.
 
\series default
This is the holy grail.
 I had some insights into this.
 How did that go, again? Something about large-scale correlations.
 This is combinatorially-explosive territory, again.
 How can it be tackled?
\end_layout

\begin_layout Itemize

\series bold
System Interaction.

\series default
 Currently, only I can perceive results within the knowledge graph.
 How can it be exposed so that it can be viewed by outsiders? Even shallow
 perusal would help build interst and support.
\end_layout

\begin_layout Subsection*
Favorites
\end_layout

\begin_layout Standard
Lets narrow down the above.
 Favorite next tasks are:
\end_layout

\begin_layout Itemize
Reiterate classification.
 Run it a second time.
 This includes implementing word-sense tagging.
 Shouldn't be too hard.
 Interesting, and the generalization seems useful, anyway.
\end_layout

\begin_layout Itemize
Multi-scale clustering.
 (aka synonyms) This requires developing multi-scale WordClass infrastructure.
 Shouldn't be too hard.
 Seems useful, anyway.
\end_layout

\begin_layout Itemize
Add support for multi-sensory data streams.
 This is a refactorization of the current code, to allow it to operate on
 more general data streams.
 Might fit well with the multi-scale work, above.
\end_layout

\begin_layout Itemize
Add time-stamp tagging and decaying activation; start new entitites when
 needed.
 This requires an indirection: statistics are to be gathered for the entity
 in the current era, which needs to be treated as distinct, despite having
 the same spelling.
 That is, we need to distinguish between words and word-instances.
 The code base needs significant modification to handle this.
\end_layout

\begin_layout Standard
Favorite theoretical activities:
\end_layout

\begin_layout Itemize
Lexical Functions.
 This seems important, but don't yet have a cler vision on how to do this.
 This needs to be developed.
 Perhaps this can be a heavily-abstracted synonym thingy? 
\end_layout

\begin_layout Itemize
Antonyms.
 This is important.
 But how? Anti-correlations are not the same thing as non-correlations.
 Words that are antonyms will apper near each other, so naive correlation
 will not work.
\end_layout

\begin_layout Subsubsection*
Coding tasks
\end_layout

\begin_layout Standard
The following coding tasks lie ahead:
\end_layout

\begin_layout Itemize
It is no longer appropriate, under any circumstances, to store counts on
 the TV.
 This will churn the code a bit.
 (Or is it? AtomSpace Frames seem to alleviate the pressure...)
\end_layout

\begin_layout Section*
Tokenization (Morphology)
\end_layout

\begin_layout Standard
Tokenization is the problem of taking a sequence of input symbols (individual
 letters) and breaking them up into words (and/or morphemes).
 Kolonin reports that 
\begin_inset Quotes eld
\end_inset

transition freedom
\begin_inset Quotes erd
\end_inset

 is the best way of doing this:
\end_layout

\begin_layout Itemize
Anton Kolonin, 
\begin_inset Quotes eld
\end_inset

Unsupervised Tokenization Learning
\begin_inset Quotes erd
\end_inset

, (2022)
\end_layout

\begin_layout Standard
He cites the following as an 
\begin_inset Quotes eld
\end_inset

exhaustive review of different tokenization techniques
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Itemize
Logan R.
 Kearsley.
 2016.
 A Hybrid Approach to Cross-Linguistic Tokenization: Morphology with Statistics.
 Brigham Young University.
 Theses and Dissertations.
\end_layout

\begin_layout Standard
The idea of 
\begin_inset Quotes eld
\end_inset

transition freedom
\begin_inset Quotes erd
\end_inset

 is introduced in
\end_layout

\begin_layout Itemize
Jesse O.
 Wrenn, Peter D.
 Stetson, and Stephen B.
 Johnson.
 2007.
 An Unsupervised Machine Learn- ing Approach to Segmentation of Clinician-Entere
d Free Text.
 PubMed Central.
 AMIA Annu Symp Proc.
 2007; 2007: 811–815.
 PMCID: PMC2655800 PMID: 18693949
\end_layout

\begin_layout Standard
Transition freedom is defined as the 
\begin_inset Quotes eld
\end_inset

number of symbolic states (characters, letters or N-grams) that can be following
 after the current state or preceding the current state.
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Standard
Rather than attempting to manually discover a specific best tokenization
 algorithm, is there a way of discovering a tokenization algorithm automatically
? That is, can we explore the space of all possible algorithms, and select
 a handful of them? How can this be done, without manual supervision (i.e.
 without comparing the segmented results to a desired outcome or reference
 corpus?)
\end_layout

\begin_layout Standard
We muse on this here.
 First, a review of the known algos, to give a hint of what we are looking
 for.
 Next, a review of the a minimal data strcture needed to represent a sequential
 time series, so that automated algrotihm exploration (a la MOSES) can be
 applied.
 That is, what would the MOSES primitives be, for a time series? Third,
 some thoughts about what kinds of output one might expect from a segmentation
 algo.
 Fourth, how these outputs can be used to obtain a measure of quality or
 interestingness.
\end_layout

\begin_layout Subsection*
Transition Freedom
\end_layout

\begin_layout Standard
Lets start with examples of the kinds of algos we expect to learn, automatically.
 First up, transition freedom.
\end_layout

\begin_layout Subsection*
Data Stream
\end_layout

\begin_layout Standard
How should a stream of tokens be represented in Atomese? Sadly, this is
 a difficult question.
 Suggest:
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

   EvaluationLink
\end_layout

\begin_layout Plain Layout

       LinkTypeNode 
\begin_inset Quotes eld
\end_inset

stream-id-42
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Plain Layout

       TokenInstance 
\begin_inset Quotes eld
\end_inset

A@uuid-hexadecimal
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Plain Layout

       TokenInstance 
\begin_inset Quotes eld
\end_inset

B@uuid-hexadecimal
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

   TokenLink
\end_layout

\begin_layout Plain Layout

       TokenInstance 
\begin_inset Quotes eld
\end_inset

A@uuid-hexadecimal
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Plain Layout

       Token 
\begin_inset Quotes eld
\end_inset

A
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

   TokenLink
\end_layout

\begin_layout Plain Layout

       TokenInstance 
\begin_inset Quotes eld
\end_inset

B@uuid-hexadecimal
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Plain Layout

       Token 
\begin_inset Quotes eld
\end_inset

B
\begin_inset Quotes erd
\end_inset


\end_layout

\end_inset


\begin_inset Newline newline
\end_inset

The vocabulary of the stream consists of the TokenNodes.
 The TokenLinks are required to distinguish multiple occurances of tokens
 in the stream.
 The uuid-hexadecimal could be dynamically generated, or they could be timestamp
s.
\end_layout

\begin_layout Standard
The above is the minimum-viable format.
 It resembles the current word-stream format (viz.
 WordInstance, etc.) It would be better if instead, Tokens were described
 as jigsaws, having two connectors: previous, and next.
 There would be only one link type, 
\begin_inset Quotes eld
\end_inset

nearest-neighbor
\begin_inset Quotes erd
\end_inset

.
 For now, we avoid questions of how to generalize to a more generic jigsaw
 subassembly, and optimize for this special case.
\end_layout

\begin_layout Subsection*
Statistics Gathering
\end_layout

\begin_layout Standard
Above requires that statstics be gathered.
 This includes a count of nearest-neighbor token pairs:
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

   EvaluationLink
\end_layout

\begin_layout Plain Layout

       LinkTypeNode 
\begin_inset Quotes eld
\end_inset

nearest-neighbor
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Plain Layout

       Token 
\begin_inset Quotes eld
\end_inset

A
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Plain Layout

       Token 
\begin_inset Quotes eld
\end_inset

B
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset Newline newline
\end_inset

After a tokenization is proposed, then a count of token pairs is needed.
 
\end_layout

\begin_layout Subsection*
Filter Primitives
\end_layout

\begin_layout Standard
The above datastream requires a collection of 
\begin_inset Quotes eld
\end_inset

filter primitives
\begin_inset Quotes erd
\end_inset

 that can be automatically assembled into an abstract syntax tree (AST).
 A filter primitive is then a lambda, with specific input types, and specific
 output types, thus providing a grammar from which the AST can be built.
\end_layout

\begin_layout Itemize
Single-token recognizer.
\end_layout

\begin_layout Section*
The End
\end_layout

\begin_layout Standard
This is the end of Part Seven of the diary.
 
\end_layout

\end_body
\end_document
