#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage{url} 
\usepackage{slashed}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding utf8
\fontencoding global
\font_roman "times" "default"
\font_sans "helvet" "default"
\font_typewriter "cmtt" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures false
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks true
\pdf_pdfborder true
\pdf_colorlinks true
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry true
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 1
\use_package esint 0
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 0
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 11page%
\topmargin 12pheight%
\rightmargin 11page%
\bottommargin 12pheight%
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\listings_params "basicstyle={\ttfamily},basewidth={0.45em}"
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Purely Symbolic Induction of Structure
\end_layout

\begin_layout Date
February 2022
\end_layout

\begin_layout Author
Linas Vepstas
\end_layout

\begin_layout Abstract
Techniques honed for the induction of grammar from text corpora can be extended
 to visual, auditory and other sensory domains, providing a structure for
 such senses that can be understood in terms of symbols and grammars.
 This simultaneously solves the classical 
\begin_inset Quotes eld
\end_inset

symbol grounding problem
\begin_inset Quotes erd
\end_inset

 while also providing a pragmatic approach to developing practical software
 systems that can articulate the world around us in a symbolic, communicable
 fashion.
\end_layout

\begin_layout Section*
Introduction
\end_layout

\begin_layout Standard
The symbolic approach to cognition is founded on the idea that observed
 nature can be categorized into distinct entities which are involved in
 relationships with one another.
 In this approach, the primary challenges are to recognize entities, and
 to discover what relationships there are between them.
 
\end_layout

\begin_layout Standard
The recognition problem is to be applied to sensory input.
 That is, we cannot know nature directly, as it is, but only by means of
 observation and sensing.
 Conventionally, this can be taken to be the classical five senses: hearing,
 touch, smell, vision, taste; or, more generally, scientific instruments
 and engineered detectors.
 Such sensors generate collections of data; this may be time-ordered, or
 simply a jumbled bag of datapoints.
 
\end_layout

\begin_layout Standard
Out of this jumble of data, the goal of entity detection is to recognize
 groupings of data that 
\emph on
always
\emph default
 occur together.
 The adverb 
\begin_inset Quotes eld
\end_inset


\emph on
always
\emph default

\begin_inset Quotes erd
\end_inset

 here is key: entities are those things that are not events: they have existence
 over extended periods of time (Heidegger's 
\begin_inset Quotes eld
\end_inset

Dasein
\begin_inset Quotes erd
\end_inset

).
 The goal of relationship detection is to determine both the structure of
 entities (part-whole relationships) as well as events (statistical co-occurance
s and causation).
\end_layout

\begin_layout Standard
If one is somehow able to detect and discern entities, and observe frequent
 relationships between them, then the path to symbolic processing becomes
 accessible.
 Each entity can be assigned a symbol (thus resolving the famous 
\begin_inset Quotes eld
\end_inset

symbol grounding problem
\begin_inset Quotes erd
\end_inset

), and conventional ideas about information theory can be applied to perform
 reasoning, inference and deduction.
 Here, the words 
\begin_inset Quotes eld
\end_inset

information theory
\begin_inset Quotes erd
\end_inset

 are taken in the broadest sense: not just signal processing and finite
 state transducers, but also Bayesian nets, theorem provers and Turing machines;
 the whole of what is computationally accessible in the current era.
\end_layout

\begin_layout Standard
The goal of this paper is to develop a general theory for the conversion
 of sensory data into symbolic relationships.
 It is founded both on a collection of mathematical formalisms and also
 on a collection of experimental results.
 The experimental results are presented in a companion text; this text focuses
 on presenting the mathematical foundations in as simple and direct a fashion
 as possible.
\end_layout

\begin_layout Standard
The organization is as follows.
 First, the general relationship between graphs and grammars is sketched
 out, attempting to illustrate just how broad, general and all-encompassing
 this is.
 Next, it is shown how this symbolic structure can be extended to visual
 and auditory perception.
 After this comes a mathematical deep-dive, reviewing how statistical principles
 can be used to discern relationships between entities.
 Working backwards, a practical algorithm is presented for extracting entities
 themselves.
 To conclude, a collection of hypothesis and wild speculations are presented.
\end_layout

\begin_layout Section*
From Graphs to Grammar
\end_layout

\begin_layout Standard
Assuming that sensory data can be categorized into entities and relationships,
 the natural information-theoretic setting would seem to be graphs: each
 entity is represented by a vertex, each relationship is represented by
 an edge.
 In the most simplistic terms, vertexes are labelled with symbols, and edges
 with symbol pairs.
 An example of this kind of naive symbolic graphical relationship is illustrated
 below.
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../reco-image/sparse-cut.eps
	width 50col%

\end_inset


\end_layout

\begin_layout Standard
The figure on the left shows a graph-theoretic sparse graph of relationships
 between entities.
 On the right is the same graph, with some of the edges cut into half-edges,
 with the half-edge connectors labelled with what they can connect to.
 The connectors are drawn with distinct shapes, intentended to convey what
 they are allowed to connect to.
 Such vertices, together with a collection of connectors, can be imagined
 to be jigsaw puzzle pieces, waiting to be connected.
\end_layout

\begin_layout Standard
The simplicity of the above diagram is deceptive.
 In fact, there is a deep and broad mathematical foundation for them.
 Such jigsaw pieces are the elements of a category-theoretic 
\begin_inset Quotes eld
\end_inset

monoidal category
\begin_inset Quotes erd
\end_inset

.
 The connectors themselves are type-theoretic types.
 The puzzle pieces themselves are the syntactical elements of a grammar,
 formally defined.
 These last three statements arise from a relatively well-known generalization
 of Curry–Howard correspondance: for every category, there is a type theory,
 a grammar and a logic; from each, the others can be determined.
\begin_inset CommandInset citation
LatexCommand cite
key "Baez2009"
literal "false"

\end_inset

 The mathematics of these relationships is foundational and quite challenging
 to the uninitiated.
\end_layout

\begin_layout Standard
The paradigm of jigsaw pieces has long been known in linguistics, and has
 been repeatedly rediscovered.
 The diagram below is taken from the first paper describing Link Grammar,
 a kind of dependency grammar.
\begin_inset CommandInset citation
LatexCommand cite
key "Sleator1991"
literal "false"

\end_inset

 Grammatically-valid (syntactically valid) sentences are formed whenever
 jigsaw connectors can be mated, leaving non unconnected.
 This fashion of specifying a syntax and a grammar may feel a bit foreign
 and unusual; be aware that such grammars can be automaticallly (i.e.
 algorithmically) transformed into equivalent HPSG, DG, CG, LFG, 
\emph on
etc.

\emph default
 style grammars.
 
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../reco-image/link-grammar.png
	lyxscale 60
	width 85col%

\end_inset


\end_layout

\begin_layout Standard
A recent rediscovery of the jigsaw-puzzle paradigm can be found in the work
 of Bob Coecke.
\begin_inset CommandInset citation
LatexCommand cite
key "Coecke2010"
literal "false"

\end_inset

 Notable results include a linear-time quantum algorithm for parsing natural
 language.
\begin_inset CommandInset citation
LatexCommand cite
key "Coecke2016"
literal "false"

\end_inset

 This work is notable in that it provides a good bridge between concepts
 in linguistics and the hard science of mathematics.
 It is not the first such; perhaps one of the earliest formal algebraic
 treaments of linguistics can be found in Marcus.
\begin_inset CommandInset citation
LatexCommand cite
key "Marcus1967"
literal "false"

\end_inset


\end_layout

\begin_layout Subsection*
Compositionality and Sheaves
\end_layout

\begin_layout Standard
The naive replacement of entitites by vertexes and relationships by edges
 seems to itself have a problem with well-foundedness.
 If an entity is made of parts, does this mean that a vertex is made of
 parts? What are those parts made of? Is there an infinite regress? How
 might one indicate the fact that some entity has a composite structure?
 The answers to these questions are resolved by observing that a partially-assem
bled jigsaw puzzle resembles a singular jigsaw price: it externalizes some
 number of unconnected connectors, while also describing the connectivty
 of the fully-asssembled territory.
 This is how the part-whole relationship is established: the 
\begin_inset Quotes eld
\end_inset

whole
\begin_inset Quotes erd
\end_inset

 entity is a partialliy assembled jigsaw; the parts are the individual pieces,
 and the way that the entity can interact with other entities is through
 the as-yet unconnected connectors.
\end_layout

\begin_layout Standard
Sheaf theory is the branch of axioms that explores how such part-whole structure
s behave.
 The 
\begin_inset Quotes eld
\end_inset

sheaf axioms
\begin_inset Quotes erd
\end_inset

 provide a handful of simple rules that describe how a valid sheaf can be
 created.
 In simplistic terms, the sheaf axioms describe how jigsaw pieces connect.
\begin_inset CommandInset citation
LatexCommand cite
key "Vepstas2017sheaves,Spivak2018"
literal "false"

\end_inset

 When sheaf theory is taken in it's full glory, it has been offered up as
 an alternative to sset theory as a foundation for all of mathematics.
\begin_inset CommandInset citation
LatexCommand cite
key "MacLane1992"
literal "false"

\end_inset

 This is because the sheaf axioms suffice not only to be a foundation for
 topology (as a generalization of frames and locales), but also for logic
 (via the extended Curry–Howard correspondance mentioned above).
 These are formidable claims with an impeccable mathematical pedigree; the
 general mathemaitcal theory seems entirely adequate as a foundation for
 AGI.
 Armed with this, one can move forward.
\end_layout

\begin_layout Standard
In philosophy, the question of part-whole relationships, identity, existance
 and thing-ness is studied under the name of mereology.
\begin_inset CommandInset citation
LatexCommand cite
key "Varzi2003"
literal "false"

\end_inset

 Some of the key concepts of mereology include ideas such as 
\begin_inset Quotes eld
\end_inset

contact
\begin_inset Quotes erd
\end_inset

, 
\begin_inset Quotes eld
\end_inset

fastentation
\begin_inset Quotes erd
\end_inset

, 
\begin_inset Quotes eld
\end_inset

cohesion
\begin_inset Quotes erd
\end_inset

, 
\begin_inset Quotes eld
\end_inset

fusion
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

brutal composition
\begin_inset Quotes erd
\end_inset

.
 A shallow understanding of these concepts is enough to convince one that
 jigsaw pieces fit the bill.
\begin_inset CommandInset citation
LatexCommand cite
key "Vepstas2020mere"
literal "false"

\end_inset

 Why mention philosophy in the same breath as mathematics? The function
 of philosophy is to wrestle with vague, phantasmic questions, to thrash
 about in the fog, hoping to find a purchase on something solid.
 Whenever such contact can be made, the scientists take over: this is how
 
\begin_inset Quotes eld
\end_inset

natural philosophy
\begin_inset Quotes erd
\end_inset

 morphed into 
\begin_inset Quotes eld
\end_inset

physics
\begin_inset Quotes erd
\end_inset

.
 Insofar as AGI wishes to be a foundational theory of cognitive-everything,
 the role of philosophy in suggesting where to look remains invaluable.
 In this particular case, the ideas of mereology provide the requisite bridge
 from the abstract to the concrete.
\end_layout

\begin_layout Subsection*
Pervasiveness
\end_layout

\begin_layout Standard
After becoming familiar with the jigsaw paradigm, it becomes evident that
 it is absolutely pervasive.
 This is illustrated in the two figures below.
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../reco-image/citric-acid-cycle.png
	lyxscale 50
	width 28col%

\end_inset


\begin_inset space \qquad{}
\end_inset


\begin_inset space \qquad{}
\end_inset


\begin_inset Graphics
	filename ../reco-image/puzzle.eps
	lyxscale 50
	width 30col%

\end_inset


\end_layout

\begin_layout Standard
The figure on the left is the Krebs cycle (the citric acid cycle in biochemistry
).
 It has an obvious graphical structure, and it is not difficult to see how
 it is to be decomposed into jigsaw pieces.
 What is notable here is that some of the jigsaw pieces find a literal embodimen
t in the three-dimensional shape of molecules.
 Diagrams of DNA are often shown with jigsaw connectors standing in for
 the amino acids ATGC.
\end_layout

\begin_layout Standard
The diagram on the left is more abstract, and is meant to illustrate composition
 or beta reduction in term algebra.
 A term is 
\begin_inset Formula $f\left(x\right)$
\end_inset

 or an 
\begin_inset Formula $n$
\end_inset

-ary function symbol 
\begin_inset Formula $f\left(x_{1},x_{2},\cdots,x_{n}\right)$
\end_inset

.
 Variables are denoted as 
\begin_inset Formula $x,y,z,\cdots$
\end_inset

 while constants are type instances, such as 
\begin_inset Formula $42$
\end_inset

 or the string 
\begin_inset Quotes eld
\end_inset

foobar
\begin_inset Quotes erd
\end_inset

.
 Beta reduction is the act of 
\begin_inset Quotes eld
\end_inset

pluging in
\begin_inset Quotes erd
\end_inset

: 
\begin_inset Formula $f\left(x\right):42\mapsto f\left(42\right)$
\end_inset

.
 Re-interpreted as jigsaw connectors, the term 
\begin_inset Formula $f\left(x\right)$
\end_inset

 is the jigsaw on the left, and 42 is the jigsaw on the right.
 In order to connect, the two types must match.
 The type-theoretical type of the variable 
\begin_inset Formula $x$
\end_inset

 must match the type of of what is plugged in.
 This kind of plugging-in or composition is rampant throughout mathematics.
 Prime examples can be found in proof theory,
\begin_inset CommandInset citation
LatexCommand cite
key "Troelstra1996"
literal "false"

\end_inset

 lambda calculus,
\begin_inset CommandInset citation
LatexCommand cite
key "Barendregt1981"
literal "false"

\end_inset

 term algebras
\begin_inset CommandInset citation
LatexCommand cite
key "Baader1998"
literal "false"

\end_inset

 and model theory.
\begin_inset CommandInset citation
LatexCommand cite
key "Hodges1997"
literal "false"

\end_inset


\end_layout

\begin_layout Subsection*
Vision and Sound
\end_layout

\begin_layout Standard
Shapes have a structural grammar, too.
 The connectors can specify location, color, shape, texture.
 The key point of this structural ddecomposition is that it is 
\emph on
not about pixels
\emph default
! The structural decomposition is scale-invariant (more or less, unless
 some connector fixes the scale) and rotationally invariant (unless some
 connector fixes direction).
 The structural grammar captures the morophology of the shape, it's general
 properties, omitting details when they are impertinent, and capturing them
 when they are important.
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../reco-image/traffic-lights.jpg
	lyxscale 80
	width 5col%

\end_inset


\begin_inset space \qquad{}
\end_inset


\begin_inset space \qquad{}
\end_inset


\begin_inset space \qquad{}
\end_inset


\begin_inset Graphics
	filename ../reco-image/traffic-light-grammar.eps
	width 25col%

\end_inset


\end_layout

\begin_layout Standard
Not only do two-dimensional photographs have a structure grammar, but so
 do sounds.
 On the left is a spectrogram of a whale song.
 It is ostensibly 
\begin_inset Quotes eld
\end_inset

one-dimensional
\begin_inset Quotes erd
\end_inset

, with time as the primary dimension, but is more accurately multi-dimensional:
 the vertical axis shows frequency, the colors encode a third dimension,
 the intensity.
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../reco-image/noaa-fisheries-humpback.jpg
	width 40col%

\end_inset


\begin_inset space \qquad{}
\end_inset


\begin_inset Graphics
	filename ../reco-image/audio-graph.eps
	width 15col%

\end_inset


\end_layout

\begin_layout Standard
On the right is a graphical representation of the midsection of the song.
 It shows the number of repetitions (six), as well as the shape of the frequency
 distribution (its a chirp, which can be discovered with a chirp filter,
 a certain kind of digitial signal processing filter.) Individual repetitions
 ccan be spotted with a finite impulse response filter.
 The point here is that basic sensory information can also be described
 in grammatical terms.
\end_layout

\begin_layout Section*
Learning
\end_layout

\begin_layout Standard
In order for a graphical, sheaf-theoretic, grammatical theory of structure
 to serve as a foundation stone for AGI, there most be a practical algorithm
 for extracting such structure from sensory data.
 Such an algorithm is sketched below.
 It consists of three interacting parts.
 The first step is chunking, the division of sensory data into candidate
 entities and candidate interactions that might possibly be identified as
 entities or inteeractions by the second step.
 The second step takes a collection of candidate graphs, splits them into
 jigsaw pieces, and then attepts to classify jigsaw pieces into common categorie
s, based on thier commonalities.
 The third step is a recursive step, tto repeat the process again, but this
 time taking the discovered structure as the sensory input.
 It is meant to be a hierarchical crawl up the semantic ladder.
\end_layout

\begin_layout Standard
Experimentally, the second step has been the most thoroughly explored.
 An implementation exists within the OpenCog system,
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
See the 
\begin_inset Quotes eld
\end_inset


\begin_inset CommandInset href
LatexCommand href
name "learn project"
target "https://github.com/opencog/learn"
literal "false"

\end_inset


\begin_inset Quotes erd
\end_inset

 in github.
\end_layout

\end_inset

 with extensive research diaries logging results.
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
See the 
\begin_inset CommandInset href
LatexCommand href
name "diaries"
target "https://github.com/opencog/learn/tree/master/learn-lang-diary"
literal "false"

\end_inset

 in the afrementioned project.
\end_layout

\end_inset

 A summary of these results is presented as a companion paper to this one.
 Explorations of the first and third steps have hardly begun.
 It is easiest to describe the second step first.
\end_layout

\begin_layout Subsection*
Grammatical Induction
\end_layout

\begin_layout Standard
In linguistics, one is presented with a pre-chunked sequence of words; the
 conversion of raw sound into phonemes and then words is presumed to have
 already occurred.
 The task is to extract a more-or-less conventional grammar given a corpus
 of text.
 The first step is to perform a Maximum Spanning Trees (MST) parse; the
 second step is to split the MST parse into jigsaw pieces, and classify
 those pieces into lexical vectors.
 The generation of the MST parse requires collecting statistics on a corpus,
 based on maximum entropy principles; this has a long and deep tradition
 in Corpus Linguistics, going as far back as Biblical Concordances in earlier
 centuries, before linguistics was a distinct field of study.
 Lexical semantics also a deep and broad research literature, much more
 modern.
 Of note here is that this vector-based description has more than a few
 similarities to the theory of neural-nets, and yet is fundamentally different,
 because it is ultimately lexical (
\emph on
i.e.

\emph default
 is symbolic.) It is not obvious if deep-learning techniques can be applied
 to obtain these lexical vectors; what is described below is a simpler,
 plainer approach.
\end_layout

\begin_layout Subsubsection*
Maximum Planar Graph Parsing
\end_layout

\begin_layout Standard
The MST parsing algorithm is described by Yuret, as follows.
\begin_inset CommandInset citation
LatexCommand cite
key "Yuret1998"
literal "false"

\end_inset

 Starting with a corpus, maintain a count 
\begin_inset Formula $N\left(u,w\right)$
\end_inset

 of nearby word-pairs 
\begin_inset Formula $\left(u,w\right)$
\end_inset

.
 Here, 
\begin_inset Quotes eld
\end_inset

nearby
\begin_inset Quotes erd
\end_inset

 usually means 
\begin_inset Quotes eld
\end_inset

in a window of width six
\begin_inset Quotes erd
\end_inset

.
 A frequentist probability 
\begin_inset Formula $p\left(u,w\right)=N\left(u,w\right)/N\left(*,*\right)$
\end_inset

 is just the count of a given word-pair divided by the total count of all
 word-pairs.
 The star indicates a marginal sum or marginal probability, so that 
\begin_inset Formula $p\left(u,*\right)=\sum_{w}p\left(u,w\right)=\sum_{w}N\left(u,w\right)/N\left(*,*\right)=N\left(u,*\right)/N\left(*,*\right)$
\end_inset

.
 Given these frequencies, defines the Lexical Attraction between word-pairs
 as
\begin_inset Formula 
\[
MI\left(u,w\right)=\log_{2}\frac{p\left(u,w\right)}{p\left(u,*\right)p\left(*,w\right)}
\]

\end_inset

This lexical attraction is just the mutual information; it has a somewhat
 unusual form, as word-pairs are necessarily not symmetric: 
\begin_inset Formula $\left(u,w\right)\ne\left(w,u\right)$
\end_inset

 and concentional texts on probability theory usually do not address this
 non-symmetric situation.
 Note that the MI may be negative! The range of values depends on the size
 of the corpus; for a 
\begin_inset Quotes eld
\end_inset

typical
\begin_inset Quotes erd
\end_inset

 corpus, it ranges frm -10 to +30.
\end_layout

\begin_layout Standard
Given a table of 
\begin_inset Formula $MI\left(u,w\right)$
\end_inset

 obtained by counting, one obtains an MST parse of a sentence by considering
 all possible trees that connect all of the words, and selecting the one
 tree that has the largest possible total 
\begin_inset Formula $MI$
\end_inset

.
 An example of such a tree is shown below, taken from Yuret's thesis.
 The numbers in the links are the MI between the indicated words.
\end_layout

\begin_layout Frame
\align center
\begin_inset Graphics
	filename ../reco-image/Yuret.png
	width 40col%

\end_inset


\end_layout

\begin_layout Standard
There exist a number of fast algorithms for obtaining maximal spanning trees.
 Variants include those that generate only planar trees, or even planar
 graphs (graphs with loops, but no intersecting links).
 Maximal planar graphs (MPG) appear to offer experimentally-observable advantage
s over trees, as they appear to constrain the grammar more tightly.
 In this sense, they offer some of the advantages seen in catena-based lingustic
 theory.
\begin_inset CommandInset citation
LatexCommand cite
key "Osborne2012"
literal "false"

\end_inset


\end_layout

\begin_layout Standard
At any rate, the point here is that the MST parses are linguistically plausible:
 it is fairly straightforward to verify that they correspond, more or less,
 to what trained linguists would write down for a parse.
 The accuracy is reasonably high.
 For the present algorithms, perfect accuracy is not needed, as later stages
 make up for this.
 Yuret indicates that the best results are obtained when one accumulates
 at least a million sentences.
 This is not outrageous: work in child psychology indicates that human babies
 hear several million sentences by the age of two years.
\end_layout

\begin_layout Subsubsection*
Lexical Entries
\end_layout

\begin_layout Standard
Given an MST or MPG parse, the lexis is constructed by chopping up the parse
 into jigsaw pieces, and then accumulating the counts on the jigsaw pieces.
 This is shown below.
\end_layout

\begin_layout Frame
\align center
\begin_inset Graphics
	filename ../reco-image/disjunct-cut.eps
	lyxscale 60
	width 32col%

\end_inset


\end_layout

\begin_layout Standard
There are multiple diferent notations possible for the above.
 There is a tensorial notation, popular in quantum approaches; the lexical
 entry would be written as 
\begin_inset Formula $\mbox{ball}:\left|\overleftarrow{\mbox{the}}\right\rangle \otimes\left|\overleftarrow{\mbox{throw}}\right\rangle $
\end_inset

 while in Link Grammar, it is denoted as 
\begin_inset Formula $\mathtt{ball:the-\&\;throw-}$
\end_inset

 where the minus signs indicate connections to the left.
 The ampersand is the conjunction operator from a framgment of linear logic;
 it demands that both connector be present.
 Linear logic is the logic of tensor algebras (by the aforementioned Curry–Howar
d correspondance.) Unlike tensor algebras, natural language has a distinct
 left-right asymmetry, and so the corresponding logic (of the monoidal category
 of natural language) is just a fragment of linear logic.
 Note that all of quantum mechanics lies inside of the tensor algebra; this
 explains why assorted quantum concepts seem to recur in natural language
 discussions.
 
\end_layout

\begin_layout Standard
The connector sequences 
\begin_inset Formula $\mathtt{the-\&\;throw-}$
\end_inset

 or 
\begin_inset Formula $\left|\overleftarrow{\mbox{the}}\right\rangle \otimes\left|\overleftarrow{\mbox{throw}}\right\rangle $
\end_inset

 often called 
\begin_inset Quotes eld
\end_inset

disjuncts
\begin_inset Quotes erd
\end_inset

: different connector sequences 
\begin_inset Formula $d$
\end_inset

 are disjoined from one-another.
 Given a word 
\begin_inset Formula $w$
\end_inset

, a lexical entry consists of all word-disjunct pairs 
\begin_inset Formula $\left(w,d\right)$
\end_inset

 together with thier observed count 
\begin_inset Formula $N\left(w,d\right)$
\end_inset

.
 Given this count, one may define a frequency 
\begin_inset Formula $p\left(w,d\right)=N\left(w,d\right)/N\left(*,*\right)$
\end_inset

 where this time, 
\begin_inset Formula $N\left(*,*\right)$
\end_inset

 is the sum over all word-disjunct pairs.
 A lexical entry is thus a sparse skip-gram-like vector:
\begin_inset Formula 
\[
\overrightarrow{w}=p\left(w,d_{1}\right)\widehat{e_{1}}+\cdots+p\left(w,d_{n}\right)\widehat{e_{n}}
\]

\end_inset

One can use the logical disjunction 
\begin_inset Quotes eld
\end_inset

or
\begin_inset Quotes erd
\end_inset

 in place of the plus sign; this would be the 
\begin_inset Quotes eld
\end_inset

choice
\begin_inset Quotes erd
\end_inset

 operator in linear logic (as in 
\begin_inset Quotes eld
\end_inset

menu choice
\begin_inset Quotes erd
\end_inset

: pick one or another).
 The basis vectors 
\begin_inset Formula $\widehat{e_{k}}$
\end_inset

 are one and the same thing as the skip-gram disjuncts 
\begin_inset Formula $\left|\overleftarrow{\mbox{the}}\right\rangle \otimes\left|\overleftarrow{\mbox{throw}}\right\rangle $
\end_inset

, just offering a short-hand notation.
\end_layout

\begin_layout Subsubsection*
Similarity
\end_layout

\begin_layout Standard
The lexis generated above contains individual words with connectors to other,
 specific words.
 Although each vector is sparse, and the lexis, as a whole, taken as a matrix
 is sparse, it is still quite large.
 By contrast, conventional linguistic grammars talk about nouns and verbs
 and adjectives.
 Part of the learning process is then to automatically find similar categories:
 to organize words into groups based on similarities.
 A very conventional similarity metric is the cosine distance, given as
 
\begin_inset Formula 
\[
\cos\theta=\overrightarrow{w}\cdot\overrightarrow{v}=\sum_{d}p\left(w,d\right)p\left(v,d\right)
\]

\end_inset

This similarity metric fails.
 The space spanned by these vectors is 
\emph on
not Euclidean space
\emph default
! It does not have rotational symmetry! Properly, it is a probability space,
 a simplex, as one must preserve unit-length probability vectors: 
\begin_inset Formula $1=\sum_{w,d}p\left(w,d\right)$
\end_inset

.
 The correct information-theoretic similarity is the mutual information:
\begin_inset Formula 
\[
MI\left(w,v\right)=\log_{2}\frac{\overrightarrow{w}\cdot\overrightarrow{v}}{\left(\overrightarrow{w}\cdot\overrightarrow{*}\right)\left(\overrightarrow{*}\cdot\overrightarrow{v}\right)}
\]

\end_inset

where 
\begin_inset Formula 
\[
\overrightarrow{w}\cdot\overrightarrow{*}=\sum_{d}p\left(w,d\right)p\left(*,d\right)
\]

\end_inset

Unlike the MI for word-pairs, this MI is symmetric: 
\begin_inset Formula $MI\left(w,v\right)=MI\left(v,w\right)$
\end_inset

.
 Experimentally, the distribution of the MI for word pairs appears to be
 Gaussian, as shown below.
\end_layout

\begin_layout Frame
\align center
\begin_inset Graphics
	filename ../en-sims-p3/mi-dist-tsup.eps
	width 30col%

\end_inset


\end_layout

\begin_layout Standard
The above is obtained from MPG parsing and statistics gathering as described
 above.
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
See the Language Learning Diary Part Three, 
\emph on
op.
 cit.
\end_layout

\end_inset

 Note the graph is on a semi-log scale: normal distributions become parabolas
 when graphed this way.
 
\end_layout

\begin_layout Subsubsection*
Classification
\end_layout

\begin_layout Standard
Clustering words into grammatical categories based on similarity may seem
 straight-forward, but can founder on several different details.
 First, the MI, as defined above, has the property that words with the very
 highest MI tend to be very infrequent, rare.
 As a practical detail, one wishes to first cluster the most frequent words.
 To do this, one must add some kind of offset, so as to recommend common
 words first.
 Such an offset is provided by the average of the logarithm of the frequency
 of the two words.
 This leads to the definition of 
\begin_inset Formula 
\begin{align*}
MI_{\mbox{ranked}}\left(w,v\right)= & MI\left(w,v\right)+\frac{1}{2}\left[\log_{2}p\left(w,*\right)+\log_{2}p\left(v,*\right)\right]\\
= & \log_{2}\frac{\overrightarrow{w}\cdot\overrightarrow{v}}{\sqrt{\left(\overrightarrow{w}\cdot\overrightarrow{*}\right)\left(\overrightarrow{*}\cdot\overrightarrow{v}\right)}}
\end{align*}

\end_inset

The shape of the Gaussian above is unaffected, although it is shifted to
 the right; clearly, the ranking is completely different.
 
\end_layout

\begin_layout Standard
A second practical difficulty is that it is not enough just to cluster some
 words toegther; one must also re-write the connectors to make use of the
 set of words in the cluster.
 This prevents the use of off-the-shelf clustering algorithms that can be
 found on the Internet.
 The rewriting of connector sequences is subtle and non-trivial, and easily
 error-prone.
 The primary constraint is that one must preserve 
\begin_inset Quotes eld
\end_inset

detailed balance
\begin_inset Quotes erd
\end_inset

: the grand total counts must remain the same both before and after.
 This is much the same as detailed balance in chemistry, where the grand-total
 number of atoms (and thier types) in a chemical reaction is preserved.
 Again, this is a place where the sheaf axioms make an appearance.
\end_layout

\begin_layout Subsubsection*
Factorization
\end_layout

\begin_layout Standard
The clustering described above can be understood to be a form of matrix
 factorization.
\end_layout

\begin_layout Subsection*
Chunking
\end_layout

\begin_layout Subsection*
Recursion
\end_layout

\begin_layout Subsection*
Lexical Semantics
\end_layout

\begin_layout Itemize
Lexical Implication Rules
\begin_inset CommandInset citation
LatexCommand cite
key "Ostler1991"
literal "false"

\end_inset


\end_layout

\begin_layout Itemize
WordNet 
\begin_inset CommandInset citation
LatexCommand cite
key "Mihalcea2005"
literal "false"

\end_inset


\end_layout

\begin_layout Standard
*
\end_layout

\begin_layout Section*
The End
\end_layout

\begin_layout Standard
This is the end.
 
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
btprint "btPrintCited"
bibfiles "/home/linas/src/novamente/src/learn-git/learn-lang-diary/lang"
options "tufte"

\end_inset


\end_layout

\end_body
\end_document
