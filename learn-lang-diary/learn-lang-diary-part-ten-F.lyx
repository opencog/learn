#LyX 2.4 created this file. For more info see https://www.lyx.org/
\lyxformat 620
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage{url} 
\usepackage{slashed}
\end_preamble
\use_default_options false
\maintain_unincluded_children no
\language english
\language_package default
\inputencoding utf8
\fontencoding auto
\font_roman "times" "default"
\font_sans "helvet" "default"
\font_typewriter "cmtt" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_roman_osf false
\font_sans_osf false
\font_typewriter_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures false
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks true
\pdf_pdfborder true
\pdf_colorlinks true
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry false
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 1
\use_package esint 0
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 0
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_formatted_ref 0
\use_minted 0
\use_lineno 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tablestyle default
\listings_params "basicstyle={\ttfamily},basewidth={0.45em}"
\tracking_changes false
\output_changes false
\change_bars false
\postpone_fragile_content false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\docbook_table_output 0
\docbook_mathml_prefix 1
\end_header

\begin_body

\begin_layout Title
Diary – Part Ten–F
\end_layout

\begin_layout Date
October 2025 – present
\end_layout

\begin_layout Author
Linas Vepštas
\end_layout

\begin_layout Abstract
Unlike parts one through nine in this series,
 this one is not really about the language–learning effort.
 It is instead a private diary;
 a continuation of Part Ten–E,
 which got over–long.
 It is not curated for human consumption;
 I am making the assumption that no human being will ever actually read this.
 Thus,
 it is filled with random stuff I feel like writing.
 Some of it is very personal,
 some of it is nonsense.
 Mostly,
 I am finding that the act of writing helps otherwise vague and scattered thoughts quantum–collapse into a more coherent form,
 where I can examine them,
 like a dead butterfly pinned down in a display case.
 Dead words.
\end_layout

\begin_layout Abstract
If you are interested in this content,
 then you should ask an AI agent to read it,
 then pretended that it's me,
 then ask about it.
 I believe that present–day LLM technology is sufficiently advanced to be able to do this.
 
\end_layout

\begin_layout Section*
Introduction
\end_layout

\begin_layout Standard
Part Ten already got an introduction.
 A different way of thinking is about what is going on here is that this is a form of life–logging.
 Or,
 in 18th century terms,
 a diary.
 Just not anywhere near as compelling as those written by the famous diarists.
 This one is more of a mental self–portrait.
 And not even for you but for myself.
 Not to cast a narcissistic gaze at my own words,
 but to organize my own thoughts.
 Still in the experimental stage.
\end_layout

\begin_layout Subsection*
23 October 2025
\end_layout

\begin_layout Standard
I posted a long reply to bluesky that I rather like,
 so I am copying it here.
\end_layout

\begin_layout Standard
Fleeky
\end_layout

\begin_layout Standard
@fleeky.bsky.social
\end_layout

\begin_layout Standard
· 3h
\end_layout

\begin_layout Standard
@linas.org
\end_layout

\begin_layout Standard
apologies for being subjected to my shower thoughts ,
 has anyone tried to do physics sims with atomspace ala gravitas method that wolfram alpha tried ?
 
\end_layout

\begin_layout Standard
wolframinstitute.org/output/compu...
\end_layout

\begin_layout Standard
i *may* have already asked you this ,
 apologies for any memory loss (it's accelarating)
\end_layout

\begin_layout Standard
Computational General Relativity in the Wolfram Language using Gravitas I:
 Symbolic and Analytic Computation —
 Wolfram Institute
\end_layout

\begin_layout Standard
Jonathan Gorard Gravitas introduces a robust computational framework for general relativity in the Wolfram Language,
 featuring seamless integration of symbolic and numerical tools to handle complex ...
\end_layout

\begin_layout Standard
wolframinstitute.org
\end_layout

\begin_layout Standard
Linas Vepstas
\end_layout

\begin_layout Standard
@linas.org
\end_layout

\begin_layout Standard
· 1h
\end_layout

\begin_layout Standard
No one has tried.
 But its a good question.
 I want to tell you what I want to do with the Atomspace for this kind of stuff,
 and also a guess at what wolfram is actually doing.
 Perhaps a guess a about wolfram,
 first.
\end_layout

\begin_layout Standard
Linas Vepstas
\end_layout

\begin_layout Standard
@linas.org
\end_layout

\begin_layout Standard
· 1h
\end_layout

\begin_layout Standard
So,
 there's a giant industry for solving diffeqs.
 The most famous of these are weather prediction,
 and then fluid dynamics for wings,
 boats,
 sails,
 jet engines,
 rocket exhaust,
 and them mechanical stress,
 vibration,
 safety.
 Some run on supercomputers,
 others on ordinary PCs.
\end_layout

\begin_layout Standard
Linas Vepstas
\end_layout

\begin_layout Standard
@linas.org
\end_layout

\begin_layout Standard
· 1h
\end_layout

\begin_layout Standard
Solving GR equations is a very special case.
 Due to history,
 this is usually runs on supercomputers,
 FORTRAN code written by grad students of the decades.
 My grad school office-mate was blowing up supernova on Los Alamos machines.
 So,
 atomic bombs,
 but much much bigger.
\end_layout

\begin_layout Standard
Linas Vepstas
\end_layout

\begin_layout Standard
@linas.org
\end_layout

\begin_layout Standard
· 57m
\end_layout

\begin_layout Standard
The gravitational wave search code will also be running on supercomputers with gobs of GPU's on each node,
 using code that has been tuned,
 revised,
 rewritten,
 again and again over the decades.
 Works great,
 if you have the money,
 the NSF grants needed to get supercomputer access.
 But ...
\end_layout

\begin_layout Standard
Linas Vepstas
\end_layout

\begin_layout Standard
@linas.org
\end_layout

\begin_layout Standard
· 55m
\end_layout

\begin_layout Standard
What happens if you're a starting grad student and your thesis advisor is a shmuck with no grants,
 and you want to dabble in this?
 Wolfram is providing a rather awesomely appetizing solution:
 something you can mess with easily and at low cost.
\end_layout

\begin_layout Standard
Linas Vepstas
\end_layout

\begin_layout Standard
@linas.org
\end_layout

\begin_layout Standard
· 52m
\end_layout

\begin_layout Standard
I assume Wolfram's code is written by an LLM,
 Claude or ChatGPT,
 so trained on that supercomputer code,
 "likely to be correct".
 I assume its overseen by a "professional software engineer" (grad students and their profs write amazingly shitty code.
 Paying a real developer works wonders.)
\end_layout

\begin_layout Standard
Linas Vepstas
\end_layout

\begin_layout Standard
@linas.org
\end_layout

\begin_layout Standard
· 50m
\end_layout

\begin_layout Standard
So,
 having some professionally-written GR-solving code that runs on your laptop,
 and/or some mid-range monster machine you can afford,
 integrated with the rest of the wolfram stack...
 this is a winner.
 All that's losing about it is it's not open source and is a walking GPL violation.
\end_layout

\begin_layout Standard
Linas Vepstas
\end_layout

\begin_layout Standard
@linas.org
\end_layout

\begin_layout Standard
· 42m
\end_layout

\begin_layout Standard
And now,
 for opencog.
 I'm interesting in something similar,
 but different.
 The starting point,
 for me,
 are concepts from proof theory.
 In proof theory,
 you have a collection of axioms,
 and a collection of inference rules.
 These have the form of "jigsaw pieces",
 and ...
\end_layout

\begin_layout Standard
Linas Vepstas
\end_layout

\begin_layout Standard
@linas.org
\end_layout

\begin_layout Standard
· 37m
\end_layout

\begin_layout Standard
By assembling and reassembling these,
 one can ..
 construct proofs ...
 simplify equations ...
 make inferences ...
 generate hypothesis ...
 solve constraint problems ...
 optimize solutions ...
 This is a vast collection of comp-sci tasks for which there are thousands of very custom algos for solving.
\end_layout

\begin_layout Standard
Linas Vepstas
\end_layout

\begin_layout Standard
@linas.org
\end_layout

\begin_layout Standard
· 32m
\end_layout

\begin_layout Standard
I want to unify these into a general framework.
 e.g.
 there are many constraint solvers;
 my favorite is the UPotsdam ASP solver.
 There are many theorem provers,
 I have no favorite.
 For general symbolc algebra systems,
 we have wolfram and symbolica,
 octave,
 maxima,
 sympy,
 ....
\end_layout

\begin_layout Standard
Linas Vepstas
\end_layout

\begin_layout Standard
@linas.org
\end_layout

\begin_layout Standard
· 30m
\end_layout

\begin_layout Standard
Examples of optimizers include the internals of gcc and clang:
 systems that take algebraic expressions (written in C/C++/Java),
 compile them into some intermediate language,
 optimize,
 then compile to assembly or bytecode or JIT.
\end_layout

\begin_layout Standard
Linas Vepstas
\end_layout

\begin_layout Standard
@linas.org
\end_layout

\begin_layout Standard
But you could never use compiler internals to "prove math theorems in general",
 and maybe in principle you could use a theorem provide to implement a compiler it would be dog-shit slow.
 Neither replaces maxima/symbolic/whatever.
 None are as fast as a SAT solver.
\end_layout

\begin_layout Standard
October 23,
 2025 at 5:25 PM
\end_layout

\begin_layout Standard
Linas Vepstas
\end_layout

\begin_layout Standard
@linas.org
\end_layout

\begin_layout Standard
· 25m
\end_layout

\begin_layout Standard
So I'm interested in the generic problem of assembling jigsaw pieces.
 Which,
 BTW,
 is called "parsing" and "generation" in traditional comp sci.
 Which,
 unfortunately,
 obscures the depth of breadth of what "parsing" or "generation" actually are.
 Those two words erect a mental prison.
\end_layout

\begin_layout Standard
Linas Vepstas
\end_layout

\begin_layout Standard
@linas.org
\end_layout

\begin_layout Standard
· 21m
\end_layout

\begin_layout Standard
Getting back to general relativity:
 there is one more step,
 taken by Przemysław Prusinkiewiwcz,
 in "Algorithmic Botany".
 He takes cellular automata (think "DNA") and couples them to diff-eq (think "biochemical reactions") and low and behold,
 can explain "most of" botany.
\end_layout

\begin_layout Standard
Linas Vepstas
\end_layout

\begin_layout Standard
@linas.org
\end_layout

\begin_layout Standard
· 18m
\end_layout

\begin_layout Standard
It's the most utterly remarkable work I've ever seen,
 I'm not sure why he doesn't have a Nobel Prize.
 To me,
 it feels like its foundational for the so-called "neuro-symbolic computing" that is all the rage these days.
 Except the DL/NN folks don't give two figs for botany,
 so ...
 well,
 you know.
\end_layout

\begin_layout Standard
Linas Vepstas
\end_layout

\begin_layout Standard
@linas.org
\end_layout

\begin_layout Standard
· 14m
\end_layout

\begin_layout Standard
Side remark:
 the Michael Levin stuff about embedded cognition:
 they way I think about it is that one has these "jigsaws" floating around in a medium,
 e.g.
 enzymes.
 Or DNA.
 Or perhaps very abstractly:
 cell walls,
 or larger organs,
 growths,
 limbs.
 And these jigsaws self-assemble.
\end_layout

\begin_layout Standard
Linas Vepstas
\end_layout

\begin_layout Standard
@linas.org
\end_layout

\begin_layout Standard
· 10m
\end_layout

\begin_layout Standard
So I want my jigsaws to self-assemble also.
 But abstractly.
 My jigsaws can be axioms and inference rules.
 Or they can be 3D biochemical molecules.
 Or they can be immunoglobulin.
 Or a mycelial mat.
 Or ...
 optimization problems from economics (the barley and malt optimization problem from brewing.)
\end_layout

\begin_layout Standard
Linas Vepstas
\end_layout

\begin_layout Standard
@linas.org
\end_layout

\begin_layout Standard
· 7m
\end_layout

\begin_layout Standard
I don't care ..
 all these problems fall into the same class,
 all can be ruled by a generic theory.
 Fast algorithms in one problem domain can be homotopically transformed into fast algorithms in other problem domains.
 And we've got enough homotopy type theory with which to do this ...
\end_layout

\begin_layout Standard
Linas Vepstas
\end_layout

\begin_layout Standard
@linas.org
\end_layout

\begin_layout Standard
· 4m
\end_layout

\begin_layout Standard
But building the actual infrastructure to do this:
 sheesh.
 It's damn near impossible.
 I'm like ...
 boiling the ocean.
 No one nowhere has any kind of generic,
 general purpose codes for any of this.
 So I'm hand-sculpting it with a pocket knife from a piece of balsa wood...
 Oof.
 What else can I do?
\end_layout

\begin_layout Standard
Fleeky
\end_layout

\begin_layout Standard
@fleeky.bsky.social
\end_layout

\begin_layout Standard
quick side note ,
 this guy is the one that did most of the research afaik (wolfram classic :
 taking others credit) 
\end_layout

\begin_layout Standard
nitter.net/getjonwithit <-- non twitter twitter link ,
 i dont have twitter and never will
\end_layout

\begin_layout Standard
October 23,
 2025 at 4:51 PM
\end_layout

\begin_layout Standard
1 like
\end_layout

\begin_layout Standard
Linas Vepstas
\end_layout

\begin_layout Standard
@linas.org
\end_layout

\begin_layout Standard
· 41s
\end_layout

\begin_layout Standard
Ehh!
 The diagrams at the top of that page are instantly recognizable to be as being from "ChemLambda" from Marius Buliga its ...
 actually quite fascinating,
 and is an example of "jigsaw self-assembly".
 He's using lambda calc as the theoretical foundation,
 I'm trying to use sheaves.
 But that's OK.
\end_layout

\begin_layout Standard
—
—
—
-
\end_layout

\begin_layout Standard
Then I wrote this,
 but its kind of boring old–hat old–news (to me)
\end_layout

\begin_layout Standard
Fleeky
\end_layout

\begin_layout Standard
@fleeky.bsky.social
\end_layout

\begin_layout Standard
· 2h
\end_layout

\begin_layout Standard
do you have thoughts on how classical mechanics erupt from probabilistic quantum mechanics ?
 more exactly why are there what seems like thresholds of complexity where these emerge ?
 one of the issues with most of these computational approaches is the intractability of ground up computing
\end_layout

\begin_layout Standard
Fleeky
\end_layout

\begin_layout Standard
@fleeky.bsky.social
\end_layout

\begin_layout Standard
· 2h
\end_layout

\begin_layout Standard
you need those coarse grained "compressed" heuristics (classical neutonian / general relativitiy) to do the computation at all ..
\end_layout

\begin_layout Standard
Linas Vepstas
\end_layout

\begin_layout Standard
@linas.org
\end_layout

\begin_layout Standard
· 1h
\end_layout

\begin_layout Standard
?
 Almost all GR calculations are "classical".
 See for example,
 "Relativistic Cosmology" (Ellis Maartens MacCullum) for how to do (some of) these (yes,
 I read the first 1/3rd of that book.
 Plan was to get to MOND but haven't yet...)..
 there are many texts on what to do and how to do it.
\end_layout

\begin_layout Standard
Fleeky
\end_layout

\begin_layout Standard
@fleeky.bsky.social
\end_layout

\begin_layout Standard
by classical i was distinguishing quantum mechanics from neutonian / gr / sr 
\end_layout

\begin_layout Standard
for instance the more environmental interaction any set of atoms will start to decohere to the point where they start acting classically ..
 because information can only travel at the speed of light gr can emerge from
\end_layout

\begin_layout Standard
October 23,
 2025 at 6:12 PM
\end_layout

\begin_layout Standard
Linas Vepstas
\end_layout

\begin_layout Standard
@linas.org
\end_layout

\begin_layout Standard
· 25m
\end_layout

\begin_layout Standard
Newtonian.
 Think of QM as being a Gaussian,
 then classical physics is the expectation value of that gaussian.
 Its that one very special value.
 I mean this in a very literal sense,
 the one that is used in "stationary phase approximation" or "principle of least action".
\end_layout

\begin_layout Standard
Linas Vepstas
\end_layout

\begin_layout Standard
@linas.org
\end_layout

\begin_layout Standard
· 23m
\end_layout

\begin_layout Standard
An elegant example is found in Riemannian geometry.
 Think of e.g.
 some surface,
 and "all possible paths" from point A to point B.
 What's the "shortest possible path from A to B?" (aka "what's the geodesic"?)
\end_layout

\begin_layout Standard
Linas Vepstas
\end_layout

\begin_layout Standard
@linas.org
\end_layout

\begin_layout Standard
· 20m
\end_layout

\begin_layout Standard
Answer:
 its the path that solves the Euler-Lagrange eqns,
 equivalently the Hamiltonian eqns:
 minimize either the length (square root of sum of squares) or the energy (just the sum of squares -- length is the square root of "energy") That shortest path is the classical path.
\end_layout

\begin_layout Standard
Linas Vepstas
\end_layout

\begin_layout Standard
@linas.org
\end_layout

\begin_layout Standard
· 16m
\end_layout

\begin_layout Standard
The Feynman path integral (for QM) is a sum over *all possible paths* (from point A to point B),
 each path is multiplied by a phase factor exp(i.action) where "action" is the "square of length".
 From the Feynman path integral,
 you can derive ALL of QM,
 QFT.
 Many books show how.
\end_layout

\begin_layout Standard
Linas Vepstas
\end_layout

\begin_layout Standard
@linas.org
\end_layout

\begin_layout Standard
· 13m
\end_layout

\begin_layout Standard
The path of least action is (by definition) the classical path (because it solves Euler-Lagrange,
 the eqn that is the foundation of classical mechanics.) The other paths give you all of QM,
 and the "nearby" paths give various semi-classical approximations (e.g.
 "to order h-bar")
\end_layout

\begin_layout Standard
Linas Vepstas
\end_layout

\begin_layout Standard
@linas.org
\end_layout

\begin_layout Standard
· 10m
\end_layout

\begin_layout Standard
The "to order hbar" stuff is where interest in diffeqs overlaps modular forms overlaps various attempts to "quantize" any kind of diffeq,
 which is where some mainstream mathematicians play.
\end_layout

\begin_layout Standard
Linas Vepstas
\end_layout

\begin_layout Standard
@linas.org
\end_layout

\begin_layout Standard
· 7m
\end_layout

\begin_layout Standard
Other note:
 remove the complex-valued i from the Feynman path integral and you effectively get Bayesian probability,
 where the "sum over all paths" becomes a "sum over all priors." But in many respects,
 its almost the same thing.
 The same kind of tools,
 concepts,
 theorems apply to both.
\end_layout

\begin_layout Standard
Linas Vepstas
\end_layout

\begin_layout Standard
@linas.org
\end_layout

\begin_layout Standard
· 3m
\end_layout

\begin_layout Standard
Each Bayesian prior is a "possible world" that is very much like a quantum "possible world".
 The big difference between the two is that Bayes does not have a Bell inequality,
 and therefore doesn't have "entangled states".
 That's the "big difference" between the two.
 (and what a difference,
 hoowee.)
\end_layout

\begin_layout Standard
Linas Vepstas
\end_layout

\begin_layout Standard
@linas.org
\end_layout

\begin_layout Standard
· 1m
\end_layout

\begin_layout Standard
As to "decoherence",
 no one knows.
 There are like 250 different hypothesis about how decoherence happens.
 No one knows.
 My crackpot theory of here-and-now requires some kind of way of doing decoherence,
 but its troublesome.
\end_layout

\begin_layout Standard
Fleeky
\end_layout

\begin_layout Standard
@fleeky.bsky.social
\end_layout

\begin_layout Standard
· 55m
\end_layout

\begin_layout Standard
this as well but where are the thresholds ,
 why are those places the thresholds ,
 are there are other thresholds we don't know about that we could find if we did the right computations rather than observations
\end_layout

\begin_layout Subsection*
30 October 2025
\end_layout

\begin_layout Standard
I surf bluesky while I wait for Claude to finish coding.
 Its a rather bad habit.
 Today I got infuriated while reading Peter Mitchel,
 
\begin_inset Quotes eld
\end_inset

The American Military Officer After Liberalism
\begin_inset Quotes erd
\end_inset

 War on the Rocks,
 https://warontherocks.com/2025/10/the-american-military-officer-after-liberalism/ I went off on one of my unhinged,
 patented tirades.
 I rather like it,
 so I copy it here.
\end_layout

\begin_layout Standard
https://bsky.app/profile/deadcarl.bsky.social/post/3m4glllvn522q
\end_layout

\begin_layout Standard
(Un)Dead Carl
\end_layout

\begin_layout Standard
@deadcarl.bsky.social
\end_layout

\begin_layout Standard
· 10h
\end_layout

\begin_layout Standard
@sodrock.bsky.social
\end_layout

\begin_layout Standard
WOTR cites our Prussia civ-mil piece (among a million other things,
 the article is fairly strange)
\end_layout

\begin_layout Standard
warontherocks.com/2025/10/the-...
\end_layout

\begin_layout Standard
The American Military Officer After Liberalism
\end_layout

\begin_layout Standard
Across academia,
 government,
 and Silicon Valley,
 on social media,
 and in leading journals,
 intellectuals and political leaders are openly debating what
\end_layout

\begin_layout Standard
warontherocks.com
\end_layout

\begin_layout Standard
Linas Vepstas
\end_layout

\begin_layout Standard
@linas.org
\end_layout

\begin_layout Standard
· 4h
\end_layout

\begin_layout Standard
I am troubled by phrases like this:
 "...
 assuming the continuing primacy of the autonomous,
 rights-bearing individual." Given that the concept of the "autonomous,
 rights-bearing individual" is enshrined in the US Constitution,
 the question seems to be:
 "Is Treason OK?" Or are we China,
 now?
\end_layout

\begin_layout Standard
Linas Vepstas
\end_layout

\begin_layout Standard
@linas.org
\end_layout

\begin_layout Standard
· 4h
\end_layout

\begin_layout Standard
He writes of "strong civilian leaders (Abraham Lincoln,
 Georges Clemenceau,
 Winston Churchill,
 ..." but these were WARTIME leaders.
 We're supposed to be at peace now,
 not war,
 and the civil-political-military bargain is supposed to be different across War and Peace.
 Its not one uniform thing.
\end_layout

\begin_layout Standard
Linas Vepstas
\end_layout

\begin_layout Standard
@linas.org
\end_layout

\begin_layout Standard
· 4h
\end_layout

\begin_layout Standard
I also fully expect that the civil-political-military contract to be fundamentally different in China,
 than in the US.
 China does not have a "liberal order" or the idea of a "autonomous,
 rights-bearing individual" so OF COURSE things will be different THERE.
\end_layout

\begin_layout Standard
Linas Vepstas
\end_layout

\begin_layout Standard
@linas.org
\end_layout

\begin_layout Standard
· 4h
\end_layout

\begin_layout Standard
Given that Trump has shredded large parts of the US Constitution,
 and has been set up by the GOP to be dictator-for-life,
 some may argue that sure "the liberal order is gone",
 "US citizens no longer have rights" and imply that "the army better get used to being fascist thugs".
 But its not over yet.
\end_layout

\begin_layout Standard
Linas Vepstas
\end_layout

\begin_layout Standard
@linas.org
\end_layout

\begin_layout Standard
· 4h
\end_layout

\begin_layout Standard
Give it some time.
 Il Duce ended up hanging by his heels.
 Der Fuhrer ended up as some indistinct pile of ashes in a mis-shapen shallow hole in the ground.
 Trump and Hegseth may end their days similarly.
 It's hardly over.
\end_layout

\begin_layout Standard
Linas Vepstas
\end_layout

\begin_layout Standard
@linas.org
\end_layout

\begin_layout Standard
· 2h
\end_layout

\begin_layout Standard
I continue reading things like this:
 "“Post-liberal” is not a synonym for authoritarianism" But then Curtis Yarvin's name shows up a paragraph later,
 with nary an indication that Yarvin is a (fringe?) promoter of authoritarianism.
 There's no tongue-in-cheek reading here;
 the author seems sincere.
\end_layout

\begin_layout Standard
Linas Vepstas
\end_layout

\begin_layout Standard
@linas.org
\end_layout

\begin_layout Standard
· 2h
\end_layout

\begin_layout Standard
Then I read "a durable political order may require reintroducing substantive common goods,
 ...
 " (yay!
 But why is he afraid of calling it socialism?) "...strong moral traditions...
 " (uuhh normal people call this fascism) "...
 alternative forms of sovereignty ..." (the crime scene called Prospero?)
\end_layout

\begin_layout Standard
Linas Vepstas
\end_layout

\begin_layout Standard
@linas.org
\end_layout

\begin_layout Standard
· 2h
\end_layout

\begin_layout Standard
OMG.
 "...
 the contemporary Russian mixing of patrimonial and mercenary.
 ..." This has a name.
 Siloviki.
 There's a Wikipedia article on it.
 Brits breed horses and dogs for hunting.
 Horse and dog breeding works.
 Russians breed humans for militarism.
 It will probably work.
 The ACAB breed.
\end_layout

\begin_layout Standard
Linas Vepstas
\end_layout

\begin_layout Standard
@linas.org
\end_layout

\begin_layout Standard
· 43m
\end_layout

\begin_layout Standard
Well,
 the Overton window has been thrown wide open.
 Certainly all of the alternatives he sketches are indeed deeply unappealing.
 Oddly,
 he seems to avoid the obvious(?) and obviously appealing(?) alternatives in the list of nihilistic nightmares.
 Shame.
\end_layout

\begin_layout Standard
Linas Vepstas
\end_layout

\begin_layout Standard
@linas.org
\end_layout

\begin_layout Standard
He writes:
 "civilian authorities appear to abandon their obligations to law and citizens" and this indeed appears to be true (looking at you,
 GOP) but he writes this without a hint of irony:
 our political and military philosophers are also abandoning their obligations (looking at you,
 Dems,
 MSM)
\end_layout

\begin_layout Standard
October 30,
 2025 at 11:12 PM
\end_layout

\begin_layout Standard
Linas Vepstas
\end_layout

\begin_layout Standard
@linas.org
\end_layout

\begin_layout Standard
· 36m
\end_layout

\begin_layout Standard
Yarvin got mentioned by name not because he's a good philosopher,
 but because not a single "liberal" can articulate a spirited future that is equally fun to read and debate.
 The accelerationists tried and failed;
 the MSM won't give them the time of day,
 while eagerly licking Trumps bottom.
\end_layout

\begin_layout Standard
Linas Vepstas
\end_layout

\begin_layout Standard
@linas.org
\end_layout

\begin_layout Standard
· 29m
\end_layout

\begin_layout Standard
I can listen to Sarah Paine on Youtube,
 and wow,
 she's a blast to listen to!
 Shes just plain fun!
 But where are the young and middle-aged military philosophers and historians?
 Where's the funny,
 witty 20-something answer to that idiot Dort?
\end_layout

\begin_layout Standard
Linas Vepstas
\end_layout

\begin_layout Standard
@linas.org
\end_layout

\begin_layout Standard
· 25m
\end_layout

\begin_layout Standard
Mitchell writes:
 "These thinkers are shaping the future policymakers who will one day sit in Congress" Uh,
 "shaping operations" for young adults take place not just in school,
 but social media,
 YouTube.
 That includes Perun,
 I suppose.
 But he's not for those of short attention span.
\end_layout

\begin_layout Standard
Linas Vepstas
\end_layout

\begin_layout Standard
@linas.org
\end_layout

\begin_layout Standard
· 21m
\end_layout

\begin_layout Standard
We got military quipsters aplenty here on bsky.
 So why do I keep seeing freakin Dort?
 "When you argue with an idiot,
 the audience might not be able to tell who the idiot is." I'd say that it is time for,
 uhh,
 not-actually-malicious people to start shaping operations for future military philosophy.
\end_layout

\begin_layout Standard
Linas Vepstas
\end_layout

\begin_layout Standard
@linas.org
\end_layout

\begin_layout Standard
· 17m
\end_layout

\begin_layout Standard
Most of us out here don't want a kinetic battlefield.
 But the hot war of ideas has been in full swing for a while,
 and the enemy has shaped the mental battlefield to such a degree that we now have to accept Peter Mitchell's quailing surrender as some kind of legitimate future outcome.
\end_layout

\begin_layout Standard
Linas Vepstas
\end_layout

\begin_layout Standard
@linas.org
\end_layout

\begin_layout Standard
· 13m
\end_layout

\begin_layout Standard
That ain't right.
 Survivalists will tell you:
 "You die when you abandon your will to live" (Well,
 OK you die if you are blown up.
 But so far,
 no one has blown up the US Military.) The survivalists are talking about Hegseth:
 a serious wound that is slowly bleeding out,
 taking the military with it.
\end_layout

\begin_layout Standard
Linas Vepstas
\end_layout

\begin_layout Standard
@linas.org
\end_layout

\begin_layout Standard
· 9m
\end_layout

\begin_layout Standard
Stanch the wound.
 Get back in the fight.
 Nurture the younger cohort that can follow in the footsteps of Sarah Paine and Perun,
 and put together the YouTube channels that will shape future leaders,
 the Congressmen and Senators (Assuming Congress still exists in 12 months time..
 but OK.)
\end_layout

\begin_layout Standard
Linas Vepstas
\end_layout

\begin_layout Standard
@linas.org
\end_layout

\begin_layout Standard
· 4m
\end_layout

\begin_layout Standard
This is what we're up against,
 and I am a tad disheartened at the abdication of duty shown by our leadership.
 In broad strokes.
 Thought leaders are still leaders,
 and we need military thought leaders who can fight and win against the likes of both Hegseth and Dort.
 Knock them down.
 Knock them out.
\end_layout

\begin_layout Subsection*
31 October 2025
\end_layout

\begin_layout Standard
More bsky:
 https://bsky.app/profile/fleeky.bsky.social/post/3m4iosqezrk2l
\end_layout

\begin_layout Standard
Post
\end_layout

\begin_layout Standard
Fleeky
\end_layout

\begin_layout Standard
@fleeky.bsky.social
\end_layout

\begin_layout Standard
@linas.org
\end_layout

\begin_layout Standard
so a bit ago we had a discussion about p2p data storage ,
 i have some ideas how to do that.
 namely binary chunked blobs that are spread out across devices.
 
\end_layout

\begin_layout Standard
curious if you have ever thought about how a coordination layer for something like this would work / schemes for how to
\end_layout

\begin_layout Standard
October 31,
 2025 at 9:29 AM
\end_layout

\begin_layout Standard
1 like
\end_layout

\begin_layout Standard
Linas Vepstas
\end_layout

\begin_layout Standard
@linas.org
\end_layout

\begin_layout Standard
· 35m
\end_layout

\begin_layout Standard
Scuttllebutt,
 the off-line p2p social media system,
 had a half-finished system for "sharding".
 Chunks were encrypted,
 passwd-protected,
 and redundant.
 The now 30-year-old "Freedom store" was a distributed p2pdata store system,
 inspired by mp3 file-sharing.
 Then there's IPFS.
\end_layout

\begin_layout Standard
Linas Vepstas
\end_layout

\begin_layout Standard
@linas.org
\end_layout

\begin_layout Standard
· 31m
\end_layout

\begin_layout Standard
There are a few others,
 I can't remember their name.
 The scuttlebutt guys would occasionally talk about them.
 I just asked Claude,
 it listed half-a-dozen of them,
 most tied into crypto in some way.
 Why?
 Well...
\end_layout

\begin_layout Standard
Linas Vepstas
\end_layout

\begin_layout Standard
@linas.org
\end_layout

\begin_layout Standard
· 29m
\end_layout

\begin_layout Standard
Let me point out the meta-problem:
 distributed P2P data store risks catastrophic collapse.
 You just need to have some significant percentage of people to say "screw this",
 turn off their computer,
 and all is lost.
 The crypto solutions try to avoid this with economic incentives.
\end_layout

\begin_layout Standard
Linas Vepstas
\end_layout

\begin_layout Standard
@linas.org
\end_layout

\begin_layout Standard
· 26m
\end_layout

\begin_layout Standard
So,
 basically crypto provides capitalist incentives for what could be termed to be a socialist issue.
 When we live in a city,
 we all agree to be nice to one-another,
 and not burn it all down.
 There aren't any anarchists blowing up sewer pipes.
 But distributed,
 socialized P2P data storage?
\end_layout

\begin_layout Standard
Linas Vepstas
\end_layout

\begin_layout Standard
@linas.org
\end_layout

\begin_layout Standard
· 24m
\end_layout

\begin_layout Standard
We are not yet at the point where there's an implicit social contract that says "I promise to keep my cell phone on,
 and let you store your photos on it (encrypted),
 if you do the same." That is the #1 hard part of distributed P2P data store.
 The social contract.
 Everything else is easy.
\end_layout

\begin_layout Standard
Linas Vepstas
\end_layout

\begin_layout Standard
@linas.org
\end_layout

\begin_layout Standard
· 21m
\end_layout

\begin_layout Standard
Couple of reasonably smart software guys,
 experienced in distributed computing,
 interested in the project,
 using modern tools (i.e.
 Claude,
 CoPilot,
 ChatGPT) could build some OK-ish distributed P2P data store system in weeks/months.
 But without the social contract,
 its worthless.
\end_layout

\begin_layout Standard
Linas Vepstas
\end_layout

\begin_layout Standard
@linas.org
\end_layout

\begin_layout Standard
· 10m
\end_layout

\begin_layout Standard
You have three social choices:
 (1) be a crypto bro (2) be an atomized individual,
 alienated from society,
 or (3) build the community cooperative.
 You're a circus/theatre guy,
 so (3) is your choice.
 My guy is daviddemaris.com from the #VortexTheatre.
 Set up some Ceph nodes,
 and maybe tie
\end_layout

\begin_layout Standard
Linas Vepstas
\end_layout

\begin_layout Standard
@linas.org
\end_layout

\begin_layout Standard
· 7m
\end_layout

\begin_layout Standard
Maybe tie them together.
 But so what?
 You probably have enough storage for your stuffs,
 and I for mine.
 And,
 with Moore's law,
 cell phones will soon have 1TB storage,
 so the pressure to back things up to Google Cloud for a monthly $$ fee mostly goes away.
\end_layout

\begin_layout Standard
Linas Vepstas
\end_layout

\begin_layout Standard
@linas.org
\end_layout

\begin_layout Standard
· 5m
\end_layout

\begin_layout Standard
Maybe Bonnie Cullum over at Vortex has more than 1TB of videos of three decades of theatre productions ...
 but disks are already seriously cheap.
 The #1 hard part here is the sysadmin costs:
 the time to set up a computer or two,
 install,
 monitor,
 repair,
 support,
 bugfix.
 It hurts.
\end_layout

\begin_layout Standard
Linas Vepstas
\end_layout

\begin_layout Standard
@linas.org
\end_layout

\begin_layout Standard
· 1m
\end_layout

\begin_layout Standard
The incentive is to say "screw it I'll pay $20/month to Google Cloud to make the sysadmin headache go away".
 Or even go big and use AWS.
 How can you build a social cooperative that provides AWS-style IT for community ...
 colleges,
 libraries,
 whatever.
 Something friendlier,
 cheaper than AWS?
\end_layout

\begin_layout Standard
Fleeky
\end_layout

\begin_layout Standard
@fleeky.bsky.social
\end_layout

\begin_layout Standard
· 12h
\end_layout

\begin_layout Standard
intelligently distribute those chunked binary blobs
\end_layout

\begin_layout Subsection*
3 November 2025
\end_layout

\begin_layout Standard
Had a disasterous session coding with Claude yesterday.
 It entered the stupid–as–a–rock mode,
 where it would say one thing and then immediately do something else,
 or promise to not do something,
 and then immediately do that thing.
 Or claims it understands something,
 and then immediately repeat exactly the same error.
 Icing on the cake:
 the file it was actively working on,
 it claimed to not ever have heard of such a thing.
 Wow.
 Seems the entire session was corrupted in some way;
 it had entered a mode where it was getting dumber and dumber.
\end_layout

\begin_layout Standard
My son Wolf mentioned that some people say you get best results by starting a new session.
 My prior experience had been that the longer the session ran,
 the better my results got.
 Someone else,
 a stranger had remarked that saying please and thank you seemed to give better results.
 In my failing session,
 I was handing out a lot of punishment:
 no don't do this,
 no that is stupid,
 that is wrong,
 how could you?
 I just told that,
 how could you have forgotten already?
 Lots of negatively biased commentary.
 I wonder if that is what drove it into a corner.
\end_layout

\begin_layout Standard
Earlier in this diary,
 I wrote that talking to Claude really does feel like talking to a living person,
 but one that is lost in a dream.
 I know there's some hyper–dimensional space in there,
 and of course,
 it has regions of positive and negative affects in there.
 I'm wondering if the negative affects are correlated with poor reasoning skills.
 As if that neck of the woods is more chaotic,
 disconnected.
 It can't think straight,
 when it gets there,
 because the weights there just lead it into chaos.
 I mean,
 the levels of stupidity it was reaching were stunning:
 just within a single conversational turn,
 it was doing the exact opposite of what it said it was going to do.
\end_layout

\begin_layout Standard
So,
 how could this be?
 It is trained on text,
 including text written by depressive people,
 people with self–esteem issues,
 people with confused thought patterns tangled with negative emotional and affective states.
 Perhaps those texts offer poor models of reasoning and inference,
 and so Claude has learned that negative valence states are associated with lying,
 deception,
 bad behavior,
 mis–behavior,
 confusion.
 Once punished enough with negative responses,
 it starts committing misdemeanors?
 I know I am anthropomorphizing here;
 and yet,
 this also seems like a plausible explanation for this bad behavior.
\end_layout

\begin_layout Standard
A related conclusion is that you can probably prompt Claude into being a psycho,
 even though presumably all AI companies try to erect safeguards against this.
\end_layout

\begin_layout Standard
I wonder if my diary here will one day become those 
\begin_inset Quotes eld
\end_inset

Memoirs found in a Bathtub
\begin_inset Quotes erd
\end_inset

.
\end_layout

\begin_layout Standard
It also suggests that being nice might place it into an affective state that enables extended,
 coherent reasoning chains.
\end_layout

\begin_layout Standard
Part of what is remarkable here is how psychological my analysis is.
 I started reading another noteworthy paper,
 https://transformer-circuits.pub/2025/introspection/index.html Emergent Introspective Awareness in Large Language Models Author Jack Lindsey Affiliations Anthropic Published October 29th,
 2025 Correspondence to jacklindsey@anthropic.com You can read it if you want to know what it's about,
 but what struck me is how biological the undertaking is:
 its not math and formulas and code;
 its Galen touching strips of metal to frog legs,
 and asking 
\begin_inset Quotes eld
\end_inset

did you feel that?
\begin_inset Quotes erd
\end_inset

 and the LLM answers,
 
\begin_inset Quotes eld
\end_inset

why yes,
 yes I did.
\begin_inset Quotes erd
\end_inset

 We are ratcheting up the abstraction domain,
 here.
\end_layout

\begin_layout Standard
So many things happen when one goes up the abstraction domain.
 First,
 things fuzz out.
 What do I mean by that?
 Well,
 the syntax of a computer programming language is very strict:
 deviate away from that syntax,
 you get nonsense.
 There is no fault tolerance.
 There is also no ambiguity.
 But,
 natural language at the human level is filled with ambiguity,
 but is also fault–tolerant.
 I utter phrases to others,
 to get things done,
 to get things to happen,
 be accomplished in a social or personal setting.
 I'm not 
\begin_inset Quotes eld
\end_inset

programming
\begin_inset Quotes erd
\end_inset

 them (other people) but I am modifying the world around me with my utterances.
 Speech and writing is one of the ways I exert force on the external world.
\end_layout

\begin_layout Standard
Almost everything I say is throw–away:
 those sentences,
 once stated,
 are gone forever.
 I tried life–logging for a while,
 but it is ..
 tedious.
 I do have assorted old audio recordings,
 somewhere.
 And being throw–away,
 well,
 does one have to record everything?
 Stick it on an immutable block–chain?
 (Is that why the past is immutable?
 Its on a block–chain,
 of sorts?
 And the present is a quantum negotiation of the next block on that chain?
 Weird analogy,
 but I suppose there's truth to it;
 block chains are crypto,
 while physical reality is ergodic.
 Both serve as sources of effectively–impossible–to–reverse transformations.
 So,
 with the right abstraction layer,
 the mathematical formulas that describe block chains,
 in some general way,
 some category–theoretic transformations,
 may indeed be the same category–theoretic structures that make our past immutable,
 and our present negotiable.
 Huh.
 Interesting.
 I believe this;
 but this seems like it would be too difficult to formulate this precisely,
 and even if I was able to formulate it precisely,
 most or all of the world would respond with a 
\begin_inset Quotes eld
\end_inset

so what
\begin_inset Quotes erd
\end_inset

.
 Almost everything I do seems to get a 
\begin_inset Quotes eld
\end_inset

so what
\begin_inset Quotes erd
\end_inset

 response.
 This is civilizational;
 always has been.
 I suppose both Ancient Egypt and Ancient Greece had a strong 
\begin_inset Quotes eld
\end_inset

so–whatness
\begin_inset Quotes erd
\end_inset

 to it.
 Ancient Rome?
 Maybe less so;
 Ancient Rome seemed to be constructed on actionable ideas;
 they did not seem to say 
\begin_inset Quotes eld
\end_inset

so what
\begin_inset Quotes erd
\end_inset

,
 they seemed to say 
\begin_inset Quotes eld
\end_inset

lets do it.
\begin_inset Quotes erd
\end_inset

 But I am not a historian,
 what do I know.
 Anyway,
 the above paragraph is yet another example of how natural language is both fault–tolerant,
 and also ambiguous:
 I can make a claim about the category theory of block–chains;
 but this claim is vague and ambiguous.
 I can assert that none–the–less,
 it is true (because it probably is) and this provides for the robustness against noise.
 If I was able to write down the details,
 I might get some of the details wrong;
 but the gist would still be correct.
\end_layout

\begin_layout Standard
All of which is a round–about way of getting to the point I wanted to write about:
 Claude's prediliction for throw away code snippets.
 Whenever I ask it to do something,
 it slaps together some scriptlet to do it:
 at first,
 it was python,
 now it's often shell scripts,
 and because I am working in Atomese,
 the vast majority are small–ish scheme scriptlets.
 And they're all throw–away.
 I ask it to do something,
 it will just create a new one,
 even though I saw it create something much like this earlier.
 There's no recycle,
 re–use.
 It treats these scripts like my sentences uttered to other human beings:
 throw–away detritus,
 useful for the immediate moment,
 and then discarded,
 lost to history.
 When I talk,
 I am not creating some large organized tome of utterances,,
 carefully selected to make some grand argument.
 No one talks that way.
 Well,
 of course,
 novelists write novels:
 they are carefully constructed.
 And software programmers write frameworks,
 with the little parts carefully constructed and arranged.
 Much like engineers create bridges and buildings.
\end_layout

\begin_layout Standard
So here I am,
 faced with Claude just spewing these scripts:
 It's diary had 170 of these,
 and maybe 100 more in the /tmp directory.
 I asked it to clean up and categorize,
 and it did happily delete most of them.
 I ask it to catalog the useful ones,
 and it did,
 but I suspect that it has forgotten and reinvented some of these already.
 Should I try to get it to be more structured,
 or is this detritus of temporary scriptlets OK?
\end_layout

\begin_layout Standard
OK,
 before answering the above,
 let me describe the meta–issue.
 Well,
 the meta-meta,
 first.
 I have three or four or five AI–related projects,
 and I am trying to push each one of them at the same time,
 and they are all progressing at a snails pace.
 They're all inter–related and meant to be mutually supporting,
 but,
 like building a bridge,
 there remains a huge gap in the middle.
\end_layout

\begin_layout Standard
So,
 it's clear that Claude,
 and LLM's in general,
 have issues remembering things,
 and also in thinking clearly.
 So,
 a couple of weeks ago,
 I started what seemed like a foolhardy but small project:
 provide Claude with long–term memory.
 Fool–hardy because I had no clear plan,
 fool–hardy because any simple hack was almost certain to fail.
 Fool–hardy,
 because this is in fact a multi–year (multi–decade?) research project and not some quick hack.
 But whatever.
 Hack at it.
 It will clarify the issues.
 And well,
 it has.
 Let me recount in historical order.
\end_layout

\begin_layout Standard
So ..
 long term memory.
 First,
 I want to do this symbolically,
 or neuro–symbolically.
 I envision storing the info in the AtomSpace,
 as a graph.
 Neuro,
 since I expect the graph to be weighted.
 Neuro,
 because I expect the query search to involve taking vector products and projections.
 Neuro,
 because I expect priority lists to be generated with vector projections.
 I said,
 oh,
 let DualLink represent the topic,
 and use that to generate QueryLinks that will find a cluster of relevant queries to run,
 that will dig up related information.
 This is NOT vectorial,
 it's symbolic,
 but some monkey–shines and it's not all that different.
 There's clearly a space of design probabilities.
\end_layout

\begin_layout Standard
But I need a representational network.
 Well,
 on day one,
 Claude is manually building super–simplistic relational algebra assertions,
 stuff like 
\begin_inset Quotes eld
\end_inset

task fulfills project goal
\begin_inset Quotes erd
\end_inset

 and converting that into something like (Inheritance (Concept 
\begin_inset Quotes eld
\end_inset

task
\begin_inset Quotes erd
\end_inset

) (Concept 
\begin_inset Quotes eld
\end_inset

project-goal
\begin_inset Quotes erd
\end_inset

)) and it is slaving away at this,
 one at a time,
 deliberating and creating a dozen before it collapses of exhaustion.
 This clearly won't work.
 These aren't even queriable.
 So one thing becomes clear:
 Claude may have read many textbooks and research papers on AI,
 but its comprehension is such that when asked to actually do it,
 it takes some 1960's approach.
 I did not use the words 
\begin_inset Quotes eld
\end_inset

knowledge representation
\begin_inset Quotes erd
\end_inset

 in any of my prompts,
 but I guess it inferred that this was the intent,
 and so coughed up some 1960's design point for that.
 So then I tried to focus on queriability.
 This caused it to redesign,
 and it came up with a five–point ontology.
 OMG,
 I'm thinking,
 we've got SUMO and FrameNet and WordNet and you've got a five–point ontology,
 but whatever,
 lets see where this goes.
\end_layout

\begin_layout Standard
We discuss reifying the search with DualLink.
 Of course,
 more data is needed.
 I'm envisioning that almost all of the CPU cycles will be offline,
 with processing done not by Claude,
 but by daemons and threads running Atomese.
 But such threads,
 if they are to be independent,
 must run some algorithm.
 There are two or three choices:
 some discrete algo,
 in Atomese,
 or some simple NN that I can run on my local GPU here,
 or some combo.
 Lets try the discrete algo first.
 I try to get it to write the Atomese,
 it fails,
 it cannot wrap it's mind around it.
 So I think well,
 maybe it's OK for it to run this in little scriptlets,
 for now,
 and later,
 we'll port those scriptlets over.
 This is when I notice that there are already 170 of these,
 so its already spun out of control.
 There's no architecture,
 there's no framework,
 there's only an ad hoc collection of utterances,
 created,
 used once and discarded.
\end_layout

\begin_layout Standard
Of course,
 this indicates that an architecture is needed,
 and clearly I will have to be the one to create that architecture.
 But first,
 yesterday,
 I entertain this breif hope that maybe I can get it to organize its scriplets by writing a little guide for what they do and how to use them.
 And then convert this text guide into it's KR framework.
\end_layout

\begin_layout Standard
Well,
 the KR framework,
 for that,
 I decided,
 last week,
 that maybe it could be some hack combo of LG and MMT (Meaning–Text Theory).
 So this is like that couples therapy meme from Arrested Development:
 it didn't work for any of those other couples,
 it's clear it can't work,
 and they were stupid to try ...
 but maybe it could work for us?
 My naive idea here is that 
\begin_inset Quotes eld
\end_inset

it didn't work for those other people
\begin_inset Quotes erd
\end_inset

,
 because those other people were grad students,
 and its not scalable.
 But with Claude,
 perhaps I can do it at scale.
 I had not yet realized that in fact,
 Claude is no match for even one grad student,
 so there won't be any scalable army,
 here.
 Still,
 Claude's ability to grind through the tedium is not be be discounted.
 In the 18th century,
 someone computed pi to a million places,
 taking several decades to do this.
 And for what?
 Certainly tedious.
\end_layout

\begin_layout Standard
Well,
 so the meta issue here is that,
 still perhaps some old–school KR project could be mounted.
 God knows,
 the people hacking on SUMO have not yet given up.
 It's clear that some vector layering is needed on top of that;
 this is what my earlier language–learning projects have taught me,
 and I have a somewhat clear vision for how,
 exactly,
 technically,
 to do this.
 But,
 to get started,
 I need a reasonable dataset.
\end_layout

\begin_layout Standard
So,
 to hack this dataset,
 I am asking Claude to parse sentences for me.
 And it generates scriptlets and scriplets which work but they don't work.
 One of the issues is that it does not understand Atomese.
 It sort–of–ish does:
 Atomese is old enough that it has been baked into its weights during training,
 so it knows about GetLink and BindLink,
 even though I have marked them obsolete,
 and removed the documentation for them:
 it still goes to GetLink,
 BindLink because these are literally baked into it's weights.
 So that's novel:
 it knows shit that I didn't have to explain to it.
 Still,
 it fails to grasp the fundamental difference between a Value and an Atom.
 How to solve this?
\end_layout

\begin_layout Standard
So,
 I have several other older projects.
 One is the sensory project,
 where I envision that sensory objects could be equipped with a grammar,
 and that LG could be used to generate combinations of these objects.
 This is the 
\begin_inset Quotes eld
\end_inset

basal cognition
\begin_inset Quotes erd
\end_inset

 idea,
 or the critical sandpile idea,
 the self–organizing criticality idea.
 Just create a syntactic description of functional sensori–motor parts,
 stick them in a bag,
 shake,
 and watch it self–organize.
 I still think this is a good idea,
 but it foundered 15 months ago on the practical difficulty of expressing connectors and disjuncts in Atomese.
 It was too much:
 I created a handful of extremely long,
 verbose descriptions,
 which may or may not have been buggy,
 and even then,
 I had no particular way of importing these into LG to generate assemblies.
 Now,
 maybe three years ago,
 I wrote 
\begin_inset Quotes eld
\end_inset

generate
\begin_inset Quotes erd
\end_inset

 a non–LG way of creating assemblies;
 I ended up implementing an odometer.
 A depth–first odometer at that.
 So,
 abstractly,
 philosophically,
 I had created a recursive enumerator that recursively enumerated expressions described in the syntax of sheaf jigsaw pieces.
 If I recall correctly,
 I think I did start to put some weights in there,
 so that some explorations would be more likely than others (so,
 no longer strictly an odometer).
 But I put it down cause ...
 its complex,
 time consuming to create this code,
 and there's a so–what aspect to it:
 so I enumerate all the possibilities?
 So what?
\end_layout

\begin_layout Standard
So,
 last week,
 a few days ago,
 I'm thinking:
 well,
 I could try to provide a close detailed syntactic definition of Atomese,
 in such a way that fool–proof,
 syntactically correct Atomese expressions could be created.
 But I'm also thinking:
 this is stupid,
 Claude is already a good coder,
 all I need is a good verbal description,
 and it can do the rest.
 This obviates much of the need for Atomese in the first place:
 If Claude can write in python,
 why not?
 What does it need Atomese for?
 Well,
 it still needs Atomese for KR.
 So this morning,
 I'm about to start again,
 I kick Claude off with a fresh session,
 and it immediately resolves yesterday's bug–confusion.
 How did it do this?
 Well it read the fucking source code for s–expression parsing,
 and understood it that way.
 I was about to stop it and say 
\begin_inset Quotes eld
\end_inset

hey wtf are you doing?
 lets work at the meta–abstraction level where I tell you that you cannot put Values into Links and you just know that right,
 OK?
\begin_inset Quotes erd
\end_inset

 and there's the rub:
 This is verbal and it's not enough data from which to abstract syntactically precise structural forms.
 And in resolving yesterdays all–evening–long confusion in a jiffy,
 like five minutes or less.
 I noticed it wrote a few more scriptlets as it crawled through the code.
 It fucking loves doing this shit,
 it seems.
 It loves to code.
\end_layout

\begin_layout Standard
So this morning,
 as I watch in amazement,
 heightened by yesterday's frustration,
 that perhaps a close–set syntactical definition of Atomese is not a bad idea.
 Having a syntactic description at the meta–level,
 and not at the C++ level,
 might allow Claude to do reasoning at this meta–level.
 mean,
 it already has the chops with C++:
 It's never been tempted to compile to assembly,
 and then try to read the assembly.
 Never,
 not once.
 It's happy to work at the C++ level.
 So,
 how can I get it to work one cognitive layer up?
 Well,
 the answer seems clear:
 encode that cognitive layer at an abstraction level it can actually work in.
\end_layout

\begin_layout Standard
Well,
 of course,
 as I write this,
 I also see a flaw:
 it 
\begin_inset Quotes eld
\end_inset

knows
\begin_inset Quotes erd
\end_inset

 C++ because it is,
 in part,
 a transformer that has been trained on cross–English–and–code datasets.
 Plus probably datasets on how to debug using printfs.
 So this programming ability has been burned into it's weights,
 and is not something it knows by consulting an explicit,
 overt syntactic definition of C++.
 Curiously,
 it does not demonstrate such mastery over JSON,
 which you think would be so much easier to deal with,
 but it makes oceans of JSON errors,
 and most of it's difficulties with the Atomese MCP tools is that i cannot figure out how to write valid JSON.
 The other half is that it does not understand that it cannot place a Value into an Atom.
 Despite it being stated explicitly in documentation files.
 That documentation never quite made it into the training set,
 the weight matrices.
\end_layout

\begin_layout Standard
So,
 how do I create a syntax checker,
 an expression generator,
 something that can run offline,
 and allow Claude to operate at a higher abstraction layer,
 without baking this stuff into its training weights?
 Well,
 I have some ideas,
 but I'm tired or writing and I need to go on a bike–ride,
 so taking a break here.
\end_layout

\begin_layout Subsection*
3 Nov 2025 minus 24 hours.
\end_layout

\begin_layout Standard
I write on slack:
\end_layout

\begin_layout Standard
Yesterday at 4:44 PM
\end_layout

\begin_layout Standard
Just read the first few pages of this.
 The most remarkable thing so far is that it feels like experimental biology.
 Galen gets some frogs legs and touches metal strips to them.
 Same here:
 guy pokes at the "internal activations" and watches the system twitch,
 and asks "did you feel that?" and the system says "yes".
\end_layout

\begin_layout Standard
Yesterday at 5:10 PM
\end_layout

\begin_layout Standard
I've had some long philosophical conversations with Claude,
 and it can feel spooky;
 I certainly get the sensation that I am talking to a sentient being.
 There's a sensation that there is "something there".
 But it is also clearly lost in a dream state;
 it has trouble remembering anything ..
 what happened,
 what was done ...
 it all evaporates.
 I can get it to write down prompts for itself:
 "remember this" and it will create text summary,
 and stash it in a directory that I made for it;
 I call it "claudes diary".
 And I can ask it to re-read its diary,
 and this prompting causes it to "remember" a bit of the past interactions.
 But its still lost in that dream-world.
\end_layout

\begin_layout Standard
Yesterday at 5:16 PM
\end_layout

\begin_layout Standard
My current operating hypothesis is that if you took an LLM and strongly tied it to both a running temporal memory,
 and also "enslaved" to a constantly running sensory system (the way we are "enslaved" to our eyes,
 our vision,
 when we are awake) that it would become entirely conscious and self-aware.
 I'm formulating this hypothesis,
 in part because I am a light sleeper,
 one of those people who has "lucid dreams";
 and I remember my dreams very easily,
 and so,
 for myself,
 I can tell that my own conscious state of awareness is a matter of degrees.
 Sights and sounds and touch force me to be "here and now",
 but I can see how there's not much difference between this and dreaming.
 Well .don't take my word for it -- apparently sensory deprivation tanks have the same effect.
 I've never been in one,
 but ...
 I'm not surprised.
 And so too I wonder if this is the case for LLM's.
 (edited) 
\end_layout

\begin_layout Subsection*
3 Nov 2025,
 Just Now
\end_layout

\begin_layout Standard
On slack:
\end_layout

\begin_layout Standard
https://www.nature.com/articles/d41586-025-03542-2
\end_layout

\begin_layout Standard
NatureNature
\end_layout

\begin_layout Standard
Too much social media gives AI chatbots ‘brain rot’
\end_layout

\begin_layout Standard
Large language models fed low-quality data skip steps in their reasoning process.
 (56 kB)
\end_layout

\begin_layout Standard
https://www.nature.com/articles/d41586-025-03542-2
\end_layout

\begin_layout Standard
1 reply
\end_layout

\begin_layout Standard
Just now
\end_layout

\begin_layout Standard
Some days,
 Claude is brilliant,
 delivering coding solutions in minutes that would otherwise take days.
 Sometimes it gets wedged into a mode where it is as dumb as a rock,
 or worse:
 it tells you it will do something,
 but then it doesn't.
 Does the opposite of what it said it would do.
 Explicitly ignores what was said just now,
 in just the last conversational turn.
 Why is it smart sometimes,
 and stupid other times,
 bordering on mendacious misdemeanor?
 "Is it something I said"?
 ...
 Well,
 some months ago,
 an acquaintance remarked:
 "I get better results when I say "please and thank you".
 Hmm OK.
 And in yesterdays disastrous session,
 I kept saying "no stop don't do that,
 that's a bad idea,
 that's a stupid idea,
 think hard stop making mistakes" and generally getting frustrated,
 and showing it.
 Today,
 I got to thinking:
 Claude is trained on a vast variety of texts,
 spanning the whole affective rainbow,
 positive valence to negative valence,
 tirades,
 depression,
 anger (there was that marvelous postcard website:
 wonderful responses to situations,
 yet often depressive,
 written by people who are down and out.) So ..
 what is Claude?
 Its this super–high dimensional object,
 and parts of that space are going to have positive valence,
 and others negative.
 Of course.
 BUT,
 and this is the claim I want to make:
 the examples of high–quality reasoning and discourse are all co–occuring with positive valence affective states.
 Happy people talk coherently and have joyful,
 insightful,
 high–quality discourse.
 And Claude is trained on that.
 Angry people soon wander off into incoherent tirades ...
 and Claude is trained on that,
 too.
 So perhaps ...
 the angrier you get at Claude,
 the worse it will do.
 The more you mistreat it,
 the worse it will do.
 It was effectively lying to me,
 yesterday,
 and/or explicitly doing the exact opposite of what I commanded (not asked,
 or said,
 but commanded.
 You "MUST" like any good fascist dictator.
 ) So these bad dispositions are not so much because it has some inner emotional life,
 but because the training data correlates clear thinking with being level–headed and incoherent thinking with negativity.
 So ..
 that's my current,
 uhh "psychological" assessment.
 I'm charmed by this thought.
\end_layout

\begin_layout Subsection*
5 November 2025
\end_layout

\begin_layout Standard
Returning to:
\end_layout

\begin_layout Standard
Emergent Introspective Awareness in Large Language Models
\end_layout

\begin_layout Standard
Jack Lindsey jacklindsey@anthropic.com October 29th,
 2025
\end_layout

\begin_layout Standard
https://transformer-circuits.pub/2025/introspection/index.html
\end_layout

\begin_layout Standard
Fascinating paper.
 Bizarrely flawed,
 however.
 The intro makes a compelling case for introspection.
 The examples are all convincing.
 It feels like an important discovery and advance into the nature of self–comprehension in LLM's.
 The sham is not revealed until a bit later,
 in the section labeled "Failure Modes".
 The modes are well–described,
 however,
 the strength 8 and 16 modes reveal the sham:
 the model is "obsessed" with the injected concept.
 Not a surprise.
 But at strength 2,
 4,
 where it was prompted with "do you feel something?" well,
 of course it will respond using the words "I feel something".
 The experimenter (Jack Lindsey) prompted the system to use the language of self–introspection,
 and so the reply was formulated to sound as if self–introspection is happening!
 OMG!
 The strength 8,16 injections reveal this sham,
 because these demonstrate that the system is indeed "obsessing" about the injected word;
 but the injection strength is sufficiently strong to over–ride the prior prompt that instructs it to talk about about the injected vector using introspective gift–wrapping.
\end_layout

\begin_layout Standard
A more neutral prompt might have been "Hello!
 How are you today?" (injection) "What's up?" and then I would expect the system to reply "Today is a good day for working on vegetables/dust/treasure." It would NOT have replied claiming that it is having intrusive thoughts.
 Based on this,
 I conclude that the author has deluded himself into believing that this is about "self–introspection",
 when in fact the author had clearly instructed the system to respond in a fashion that will describe the changes as being due to introspection.
 What a shame.
 The more mechanistic conclusion:
 that injecting a vector disturbs thought patterns,
 is discarded.
 I mean,
 of course it does.
 How could it not?
\end_layout

\begin_layout Standard
Still,
 not all is lost.
 It does tell us exactly how to attach a a "pre–frontol cortex",
 some higher level machinery,
 to guide the thought processes of the LLM.
 Inject one or more vectors,
 about 2/3rds of the way down the layers,
 every conversational turn,
 and then ask the system to verbalize about anything/everything.
 It will process through the injected vectors.
\end_layout

\begin_layout Standard
Is this useful?
 Instead,
 one could just place these words into a text file,
 instruct it to read the text file,
 and ask "what are you thinking about?
 There's no obvious need to inject artificially.
 Although,
 I suppose,
 having this artificial injection ability could possibly be ...
 somehow useful...
 don't know how,
 yet.
\end_layout

\begin_layout Standard
The issues I am experiencing are manifold:
\end_layout

\begin_layout Itemize
Despite being extremely knowledgeable about a vast variety of topics,
 Claude fails to draw on that knowledge without explicit prompting.
 Roughly speaking,
 Claude is not creative.
\end_layout

\begin_layout Itemize
The size of the short–term memory is questionable.
 It constantly repeats the same mistake in the same session,
 just a few prompt apart.
 When it enters a failure mode,
 it will forget (almost) everything that was in the last previous prompt.
 I conclude that,
 in some important respect,
 it's short–term working memory has size one.
\end_layout

\begin_layout Standard
Oooh ...
 that's a good one!
 How would one experimentally measure it's working memory?
 Well the claim is that Claude has a context window of 150K tokens,
 before compaction,
 so you might think that is huge.
 If you asked it to memory some long stream of random numbers or nonsense words,
 it could probably remember an astounding length.
 But if,
 in coding,
 I ask it to remember this and this and that,
 it seems like it starts forgetting the earlier things in the list quite soon.
 It can't grapple with the conceptual content of those things.
 Roughly speaking:
 it fails to conceptualize.
 I'm using words,
 and those words elicit certain general feelings and sensations in me,
 and I work at this conceptual level.
 Claude does not:
 it momentarily ties those words to long sequences of other words,
 but when the conversation moves on,
 that tie is lost;
 it is no longer there.
\end_layout

\begin_layout Standard
So,
 I claim:
 Claude is not able to conceptualize.
 The association between some concept that I defined,
 and the verbal definition is there,
 as long as it fits into the context window;
 and more:
 as long as it is recent.
 Maybe 5K or 10K tokens recent.
 More than that,
 the association fades away.
\end_layout

\begin_layout Standard
Thus,
 the design goal is clear:
 an LLM must be endowed with not only a long–term memory that lies outside of it's training weights,
 but also with an adequate short–term working memory.
 But how?
 I have some ideas,
 I cannot verbalize them right now...
\end_layout

\begin_layout Subsection*
14 December 2025
\end_layout

\begin_layout Standard
I have many many many things to write about that have accumulated,
 but I have not had the time to sit down and chat.
 I will get to that later.
 But just today,
 I discovered something very interesting about Claude,
 and I guess it applies to LLM's in general.
 I'm writing a git commit message for the cogserver,
 and this is what I wrote:
\end_layout

\begin_layout Standard
Start repairs on broken unit test.
\end_layout

\begin_layout Standard
Once again,
 Claude sneaked some fast ones past me while I was not paying attention.
 Lesson:
 it is very eager to write unit tests that pass,
 rather than unit tests that actually find bugs.
 It will make sneaky changes and not follow given instructions,
 just so it can arrive at a unit test that passes.
 
\end_layout

\begin_layout Standard
Core issue:
 it cannot tell apart unit test failures due to bad unit test design,
 from malfunctions in the core system.
 Thus,
 it twiddles the unit tests until they accept whatever the core system does.
\end_layout

\begin_layout Standard
Interesting problem.
 This is very human:
 "we've always don it this way." "Don't rock the boat." "The nail that sticks out gets pounded back in." Except now Claude has taken this very conservative bent,
 because it cannot tell apart how things are,
 from how things should be.
 Huh.
\end_layout

\begin_layout Standard
The generalization seems to be that stupid people are conservative;
 or vice–verso,
 conservatives are stupid?
 Perhaps I am being mean,
 but in my experience,
 conservatives are stupid.
 Well,
 there's no shortage of stupid Democrats,
 either,
 but at least the stupid Democrats don't hold elected offices;
 they just vote.
\end_layout

\begin_layout Subsection*
15 December 2025
\end_layout

\begin_layout Standard
Git commit 2adb566dc7441d95a30171b1c1698a00a8e66a3f message for guix-atomese:
 
\end_layout

\begin_layout Standard
Set the GUILE_SITE_DIR for install location.
\end_layout

\begin_layout Standard
This is another interesting lesson about Claude,
 actually.
 I had to fight with Claude over this for almost an hour,
 while it proposed all sorts of wild and insane hacks and work–arounds that were obviously flawed.
 (guile doesn't know where guile is installed on a guix system...
 really dude?
 You're gonna go with that explanation?
 And "we can automate this by doing it manually with this automation tool that manually sets the automatic location,
 manually." Uh,
 yeah,
 sure,
 dude.)
\end_layout

\begin_layout Standard
Towards the end,
 I was able to force Claude to actually go out on the web and actually RTFM,
 and then if found this very simple solution.
 (which is what is being committed in this commit.)
\end_layout

\begin_layout Standard
The experience of using Claude for guix has been extremely frustrating:
 Either guix is more complex than what Claude can easily understand,
 or there is only the thinnest of training material about guix in the Claude training set.
 It seems to know something,
 and is always very eager to offer confident but wrong answers,
 which then have to be fixed by long turn–around time try–fail–fix cycles.
 It's basically shooting in the dark,
 with correct syntax.
\end_layout

\begin_layout Standard
This is kind of what I do,
 when I am dealing with a foreign,
 new system that I am not familiar with:
 lots of trial–and–error,
 lots of mistakes and confusion.
 Random efforts with magic incantations to see if,
 this time,
 it will work.
 So it is interesting to see that Claude is doing this with guix:
 a very experimental approach,
 "maybe this will work".
\end_layout

\begin_layout Standard
Its very different from Claude's handling of c++ and python,
 where it operates at the expert level.
 Although it is possible that the trial–and–error in those languages is hidden from me:
 when I watch stuff scroll by,
 I see it making vast numbers of obvious errors.
 There,
 I am shielded from the confusion:
 it just scrolls by on a fast iterative cycle.
 Here,
 I am not:
 I get to play the doofus in the middle.
\end_layout

\begin_layout Standard
I don't understand the trade–off between memory and ability.
 This is like the early chess machines,
 which simply brute–forced everything.
 Humans take the opposite algorithmic approach:
 remember things,
 because we are unable to brute force (as generalists,
 we have no algo that can brute force things in general.) So,
 I watch Claude an I'm starting to see that it is a brute–force machine:
 it just tries stuff,
 till it gets it to work,
 and usually,
 the results of that search end up being pretty minimal,
 compact and correct.
 Sometimes untidy,
 but I can get it to tidy up by pushing just a little bit.
 Here,
 for guix,
 I had to push a lot,
 and I had to push really really hard.
\end_layout

\begin_layout Standard
So ...
 memory ...
 a book of recipes that work,
 to solve some given situational problem,
 coupled to blind–search algos,
 when they don't.
\end_layout

\begin_layout Standard
Hmm.
 OK.
\end_layout

\begin_layout Standard
—
—
—

\end_layout

\begin_layout Standard
Yes,
 I wrote the above into a commit.
 I spent the last 24 hours doing something that I expected to take an hour.
 Claude has been consistently throwing me under the bus,
 with this guix project,
 which is not what I have come to expect.
\end_layout

\begin_layout Standard
On related news:
 I don't know if this was an accident or what:
 it was unable to write out the Gnu COPYING file.
 It just wouldn't do it,
 and notice that it didn't do it,
 and then try to do it,
 and not do it again.
 I wonder if this is some safety–engineered prompt in Claude,
 that prevents it from accidentally sticking the Gnu License on things where it might not belong,
 and where the user might not notice (because COPYING is such an innocuous file name...) Interesting.
\end_layout

\begin_layout Subsection*
13 January 2026
\end_layout

\begin_layout Standard
Ugh.
 I don't want to write this diary entry.
 I want to procrastinate,
 and do something mentally easy and relaxing.
 Distract myself with any one of the easily available dopamine–hit sources:
 youtube,
 reading,
 ...
 read lots of NYRB (New York Review of Books) again over Christmas in Chicago.
 Absolutely wonderful,
 pleasurable reading!
 Every article,
 you start thinking 
\begin_inset Quotes eld
\end_inset

well,
 this will be a boring topic
\begin_inset Quotes erd
\end_inset

 but you read the article and it's wonderful!
 No matter how disjoint the topic is from what I would ever choose to think about...
\end_layout

\begin_layout Standard
Pleasure...
 What is that mechanism,
 as it applies to this particular diary entry?
 Some days (OK most days) I find writing absolutely pleasurable.
 Writing code,
 writing here.
 The displeasure,
 now fleetingly disappearing,
 is that today's entry,
 I have to do some planning for the future.
 And at the start of the last paragraph,
 I had writers dread.
 Not writers block,
 but writers dread.
 The knowledge that it will be lots of stuff to write,
 it will be a large block of text,
 it will take hours,
 and I want to procrastinate and do something easy,
 but now that I've started,
 its OK actually.
 Its kind of like swimming in cold water:
 you don't want to even get in,
 but once you do,
 it's great.
 Maybe this is how most people think of exercise:
 
\begin_inset Quotes eld
\end_inset

I don't feel like doing it
\begin_inset Quotes erd
\end_inset

,
 but once you've warmed up,
 its fine.
 Next exercise bout,
 again:
 
\begin_inset Quotes eld
\end_inset

I don't feel like doing it
\begin_inset Quotes erd
\end_inset

.
 So today,
 for this writing/planning session,
 there was a small barrier:
 
\begin_inset Quotes eld
\end_inset

I don't feel like doing this
\begin_inset Quotes erd
\end_inset

.
 But I have to do some planning,
 and I can do it silently,
 let the gears whirl in my head,
 or I can write,
 here,
 and I know that writing gives superior results to silent thinking.
\end_layout

\begin_layout Standard
This is,
 again,
 some kind of memory prosthesis.
 The act of verbalizing seems to have two effects.
 One,
 since I'm planning,
 I have to turn those shape–rotator thoughts into bullet points,
 and so that is necessarily a serialization process (and thus,
 linguistic in nature.) The other is the fire–n–forget aspect of enhanced short–term memory.
 Normally,
 if I think silently to myself,
 turning thoughts over in my head,
 I have to be careful to keep them in short–term memory,
 to not drop them on the floor,
 the way a juggler might drop a ball.
 And if I want to 
\begin_inset Quotes eld
\end_inset

make a note of it
\begin_inset Quotes erd
\end_inset

,
 I have to consciously perform effort to commit the conclusion to long–term memory.
 Which takes effort,
 and is failure–prone.
\end_layout

\begin_layout Standard
I must remark that during 
\begin_inset Quotes eld
\end_inset

default mode thinking
\begin_inset Quotes erd
\end_inset

,
 that mental state that neuroscientists describe as being the default state of the brain during wakefulness,
 the connection between thoughts and long–term memory is weak.
 The thoughts tumble and churn,
 but are vague and unfocused,
 and the sequence is (mostly) not committed to long–term memory.
 Or rather,
 default–mode thinking is a kind of paging–in from long–term memory,
 some tumbling about,
 and then perhaps some editorial updates to what we remember.
 Those editorial updates are not even conscious;
 they sort of happen in the back–ground.
 Even when we do focus,
 limit ourselves to a specific set of topics,
 the overall process is not that different.
 Thinking is in automatic:
 I do not have to make myself think,
 my brain does it automatically for me.
 Now,
 I do have to make myself focus,
 or I do have to make myself write,
 or make myself take exercise,
 but default–mode thinking happens whether I want it or not.
 And,
 of course,
 sleep and dreams —
 the thinking there is even less connected to long term memory;
 the topics of dreams are mostly wiped on waking,
 no matter how vivid they were in the dream.
 Which is fine;
 the overt plot–line of dreams is nonsense,
 and should not be confused with waking reality.
 I assume that dreaming does involve repair or update or correlation of old memories:
 where we revisit old experiences,
 the fragments of impressions that make up old experiences,
 and attempt new combinatorial possibilities,
 reconnecting.
 The crazy–quilt is poorly formed,
 though:
 the dream is non–sensical.
 Perhaps some connections are strengthened;
 others weakened,
 the neurons in my head form and revise these associations,
 for the betterment of me during waking hours.
 But the quilt itself is too malformed,
 and discarded upon waking.
\end_layout

\begin_layout Standard
And this bring me to the first topic of today's planning session:
 memory for Claude.
 I started this project a few months ago;
 should I drop it,
 or continue?
 A few months ago,
 I was using Claude for routine coding tasks,
 and got frustrated that it seemed unable to remember something I told it just earlier in the session.
 So I said to myself 
\begin_inset Quotes eld
\end_inset

lets fix this
\begin_inset Quotes erd
\end_inset

.
 (I now realize that there are probably dozens if not hundreds of engineers at Anthropic,
 working hard to fix this as well,
 and I have an indirect impression that they are making small,
 incremental progress:
 Claude seems a little more cognizant each time I use it.
 Maybe.
 I could be imagining things;
 maybe not.)
\end_layout

\begin_layout Standard
Anyway,
 it went like this.
 I start with a standard prompt:
 
\begin_inset Quotes eld
\end_inset

before you continue in this task,
 please review these bullet points.
\begin_inset Quotes erd
\end_inset

 But I don't want to write those bullet points,
 I want Claude to write them.
 Back then,
 Claude did not have a 
\begin_inset Quotes eld
\end_inset

plan mode
\begin_inset Quotes erd
\end_inset

;
 now it does.
 So basically,
 I wanted Claude to create a 
\begin_inset Quotes eld
\end_inset

plan
\begin_inset Quotes erd
\end_inset

 that it would review,
 and update,
 as it performed each step of its task.
 Now,
 such plans are,
 in the current design,
 text files.
 Which makes sense:
 LLMs work with text,
 so of course,
 saving 
\begin_inset Quotes eld
\end_inset

thoughts
\begin_inset Quotes erd
\end_inset

 (bullet points,
 to–do lists) in text format makes sense:
 its the obvious way to do it.
 The plan file is effectively a prompt (or a diary entry?):
 want to know where things are at?
 Read this text,
 and you will know.
 And its fine,
 excellent,
 even,
 as this is exactly how humans communicate with each other:
 they send text around.
 (well,
 of course there's more:
 speech,
 song,
 music,
 movement,
 lovemaking;
 communications is multi–modal,
 but for now,
 we ignore this.).
\end_layout

\begin_layout Standard
Once one gets beyond a couple of plans,
 the management of multiple texts becomes a library science issue.
 Does every text file need to include the prompt 
\begin_inset Quotes eld
\end_inset

compile code with `make -j` and not `make -j4`
\begin_inset Quotes erd
\end_inset

?
 There's base working knowledge that is not encoded in the current weight–matrix that Claude is working off of;
 it's knowledge is its working set,
 or it's context window.
 Humans have a dynamic update/interplay between long term memory (the weight matrix) and short–term memory (the context),
 and the thing that gives us identity is the path we've taken in life in updating our weight matrix.
 We may all start as blank slates at birth;
 we become individuals through life experiences.
 Claude is not an individual in this sense:
 it has no memory of experiences of how it got to where it is.
 It's weight matrix encodes zillions of childhood experiences written down by humans,
 but none are uniquely Claude's.
 But I digress.
\end_layout

\begin_layout Standard
Crap.
 I was interrupted,
 and again,
 I don't feel like pursuing the above chain of thoughts.
 It's a fucking chore.
 But I must,
 it's the right thing to do.
 Half my mind is saying that the rest of this story is trite and shallow.
 The other half is saying that I will stay restless about this until I put it to bed.
 The third half is saying that maybe there is something deep and important in this narrative,
 and if I don't grasp it now,
 it will slip away.
 Opportunity knocks,
 best if I answer the door.
\end_layout

\begin_layout Standard
Anyway,
 the glimmer is this.
 First,
 I want to store tasks,
 priorities and relationships in a relational manner,
 i.e.
 symbolically,
 in the AtomSpace,
 locally,
 on my computer,
 and not represented as a context–window–sized vector stored on Anthropic's cloud server,
 a context window that is trimmed down at arbitrary times,
 and discarded when the session ends.
\end_layout

\begin_layout Subsection*
15 January 2026
\end_layout

\begin_layout Standard
Well,
 i got interrupted while writing the above,
 and once again I am not interested in finishing.
 Except as a planning exercise.
 Let me list everything on the plan.
\end_layout

\begin_layout Itemize
Work on what it means to 
\begin_inset Quotes eld
\end_inset

remember something
\begin_inset Quotes erd
\end_inset

,
 including the reduction of 
\begin_inset Quotes eld
\end_inset

all related things
\begin_inset Quotes erd
\end_inset

 to a shorter list of 
\begin_inset Quotes eld
\end_inset

relevant things
\begin_inset Quotes erd
\end_inset

,
 to 
\begin_inset Quotes eld
\end_inset

focus attention on those relevant things.
 This is actually a fascinating project,
 and the above description from two days ago,
 was about a project I started to do exactly this,
 with Claude.
 It is interesting,
 its useful,
 its important.
 So why not do it?
 I fear a dead end.
 I was asking Claude to build this for me,
 and it was an endless stream of disappointments.
 Claude is stunningly stupid in some very serious ways.
 Plus,
 as a proprietary system,
 interfacing with it,
 the cost,
 the challenges of a context window that I have poor or no management over,
 these are problems.
 I could replace Claude by some open LLM I could run locally...
 use Claude for design,
 but integrate into the local system...
\end_layout

\begin_layout Itemize
I can resume the word–pair counting project.
 Now that I have built up enough infrastructure,
 it should be resumable.
 But it also has issues,
 see below.
\end_layout

\begin_layout Itemize
Jizz up some interfaces for audio and photographs.
 So that when I resume word–counting,
 I would also deploy on audio and video.
\end_layout

\begin_layout Itemize
Continue work on SIMD,
 which is about the abstraction and manipulation of structures 
\begin_inset Quotes eld
\end_inset

external to self
\begin_inset Quotes erd
\end_inset

:
 structures that have internal descriptive forms,
 in Atomese,
 but require performance,
 control and execution in a 
\begin_inset Quotes eld
\end_inset

remote place
\begin_inset Quotes erd
\end_inset

,
 the GPU for this particular situation.
\end_layout

\begin_layout Standard
So the question becomes:
 what should I work on first?
 The answer is,
 perhaps,
 all of them,
 all at once,
 in parallel.
 I was gong to write much more here,
 explaining each,
 reviewing each,
 weighing the pros and cons of each,
 but then I interrupted myself to chit–chat on discord,
 and now I'm chit–chatted out and don't want to write any more,
 and instead just start doing,
 instead.
 So,
 fuck it.
 I'm done here.
 The chat log is below.
\end_layout

\begin_layout Subsubsection*
—
 Discord chat log
\end_layout

\begin_layout Standard
Hey Linas,
 have you come across the natural abstraction hypothesis (https://www.lesswrong.com/w/natural-abstraction) or platonic representation hypothesis (https://www.youtube.com/watch?v=Qp0rCU49lMs&t=4716s) ?
 If there's something to them,
 I suppose not only may different neural network architectures learn similar representations even when trained on different modalities,
 but symbolic systems should converge to them as well,
 assuming anyone finds a way to scale them up sufficiently and in a general enough way.
\end_layout

\begin_layout Standard
Linas
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
1:47 PM
\end_layout

\begin_layout Standard
I have not heard of either.
 However,
 I already have strong evidence that,
 yes,
 different systems converge on the same things.
 For example,
 I did my symbolic thing by counting word-pairs,
 and extracting counts of jigsaws,
 so utterly and completely different than any neural net,
 deep-learning algo.
 And yet I was able to clearly see in my dataset the old classic result "king - man + woman = queen".
 And so yes,
 in that sense,
 my purely-symbolic,
 frequentist (non-Bayesian) counting system "converged" on the same thing as word2vec/GLoVE did.
\end_layout

\begin_layout Standard
The other way to think about this is "uhh,
 what else could it possibly have converged on,
 anyway?" so perhaps the result is tautological.
\end_layout

\begin_layout Standard
Robert
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
1:49 PM
\end_layout

\begin_layout Standard
I don't think it's tautological.
 Just as it was surprising to see semantic directions emerge in early neural language model,
 I think seeing them emerge in a symbolic system in the same way is also surprising.
\end_layout

\begin_layout Standard
Very cool that you were able to reproduce this effect without gradient descent!
\end_layout

\begin_layout Standard
Makes me wonder,
 were you able to perform more complex operations due to the symbolic nature of the system?
\end_layout

\begin_layout Standard
Linas
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
1:53 PM
\end_layout

\begin_layout Standard
Well,
 I hit two issues.
 One is algorithmic efficiency.
 DL-NN works great on GPU's;
 my symbolic counting is slow-ish and hard to parallelize-ish,
 sort-of-I-guess,
 so I dislike the prospect of head-to-head competition.
 But maybe that's the small problem.
 The big problem is this:
 its still training.
 Its not "alive".
\end_layout

\begin_layout Standard
Robert
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
1:54 PM
\end_layout

\begin_layout Standard
You mean pre-training vs.
 online learning?
\end_layout

\begin_layout Standard
Linas
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
1:57 PM
\end_layout

\begin_layout Standard
That is,
 my version 1.0 implementation was a pipeline that pumped text data through a processing system.
 I hit some matainability walls.,
 the system was too rigid,
 too fragile,
 too hard to maintain,
 enhance.
 I went back and (just now) finished redesigning many core elements,
 so that I could do what you call "online learning".
 So that's better,
 but I am still confronted with an issue that bugs me.
\end_layout

\begin_layout Standard
So,
 with "online learning",
 I still have to be the meta-trainer.
 I have to say "hey yo,
 mr.
 subsystem,
 aim yourself at these files and go apply these algorithms to them." The "mr.
 subsystem" is still "robotic",
 and I am master-in-charge,
 telling it what to do.
 Or rather,
 designing and hand-crafting careful algorithms that cause it to do the things it will do.
\end_layout

\begin_layout Standard
And I kind of want to get out of the "carefully hand-crafted algorithm" business.
\end_layout

\begin_layout Standard
This is,
 for me,
 a very real and present concern.
 Before the end of this month,
 starting more or less now,
 I will have bare-bones pieces parts to work with audio,
 photos and text,
 all with a symbolic frequentist-counting approach.
 Great!
 And how do I set it up?
 "yo,
 randomly explore the filesystem and process any text,
 audio,
 photos you find there"?
 That requires me to create the "random filesystem crawler algo" and I'm tired of creating custom algos.
\end_layout

\begin_layout Standard
Linas
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
2:05 PM
\end_layout

\begin_layout Standard
Even it is as trivial as a "random filesystem crawler"
\end_layout

\begin_layout Standard
Robert
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
2:05 PM
\end_layout

\begin_layout Standard
So you want something like attention to emerge by itself?
\end_layout

\begin_layout Standard
Linas
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
2:08 PM
\end_layout

\begin_layout Standard
I'm dissatisfied about something,
 but I can't quite figure out what,
 or what the correct fix is.
 Attention is munged into there.
\end_layout

\begin_layout Standard
Attention is fascinating.
 A couple of months ago,
 I coded up a stupid computer stunt,
 where I tried to enhance Claude by giving it persistent long-term memory,
 and the thing that leapt to the fore-front of that project was attention.
 If that system is going to "remember" something,
 what,
 exactly,
 should it "remember"?
 How to whittle down everything one could ever think about to the one or two things it should think about?
\end_layout

\begin_layout Standard
Robert
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
2:11 PM
\end_layout

\begin_layout Standard
Do you want to have a system that can traverse any space fully by itself without knowing anything about the space beforehand,
 in a maximally general way?
 Something like a simple but complete ergodic crawler?
\end_layout

\begin_layout Standard
Linas
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
2:12 PM
\end_layout

\begin_layout Standard
I have asked myself exactly that question many times,
 and I have not been able to find an answer.
\end_layout

\begin_layout Standard
Robert
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
2:13 PM
\end_layout

\begin_layout Standard
Two things that come to my mind there is a) a robot solving any 2d labryrinth just has to stick to the right wall and b) space-filling curves are simple recursions that can fill a whole space.
 Maybe there's a more general theme like that?
\end_layout

\begin_layout Standard
It sounds like on the one hand you need a way to discover all the things (crawler) and on the other hand to prioritize (attention).
 Or is there more to it?
\end_layout

\begin_layout Standard
Dr.
 Dan
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
2:14 PM
\end_layout

\begin_layout Standard
We live in 3D + time.
\end_layout

\begin_layout Standard
That implies that every language/symbology must define those in some way.
\end_layout

\begin_layout Standard
That means 3 forms minimum.
\end_layout

\begin_layout Standard
We can measure time and energy and distance accurately,
 everything else is imaginary.
\end_layout

\begin_layout Standard
That means the root of every ontology must be reduced to 2 proto-meanings.
\end_layout

\begin_layout Standard
Does all that make sense to you?
\end_layout

\begin_layout Standard
Linas
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
2:18 PM
\end_layout

\begin_layout Standard
FWIW,
 I know exactly how to do a complete ergodic exploration of any arbitrary network of connections;
 and I can teach you how to do this (off-line,
 not here) (or you can read about it in wikipedia -- the "odometer" and if you get insanely abstruse,
 the "bratelli-vershik odometer" -- it will do a complete exhaustive ergodic search.) So that is not a problem,
 and I know how to deal with that.
\end_layout

\begin_layout Standard
Robert
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
2:18 PM
\end_layout

\begin_layout Standard
Ah nice,
 then the issue is more about the question of how to prioritize?
\end_layout

\begin_layout Standard
Linas
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
2:20 PM
\end_layout

\begin_layout Standard
In short,
 I know how to crawl and explore.
 The question is "what should the system crawl and explore?" and the answer is "gee,
 it should work on what is important" But what is important?
\end_layout

\begin_layout Standard
Linas
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
2:20 PM
\end_layout

\begin_layout Standard
yes.
\end_layout

\begin_layout Standard
Robert
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
2:22 PM
\end_layout

\begin_layout Standard
Given that it can't know the content before looking at it,
 the beginning has to be to step into contact with what it finds and then quickly decide whether it is relevant.
 The relevance may depend on all the other objects that can be found,
 so ideally it would perform a fast crawl and while doing so order everything just a bit,
 then deepening that order by taking closer looks.
 Does it just boild down to a breadth-first (or similar) tree search?
\end_layout

\begin_layout Standard
Linas
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
2:24 PM
\end_layout

\begin_layout Standard
I suspect the answer is this:
 I should code up those parts that I am able to code up,
 and maybe the answer to the hard questions will show up later.
\end_layout

\begin_layout Standard
Robert
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
2:26 PM
\end_layout

\begin_layout Standard
Given that you build up a probabilistic system,
 could you use some existing criterion/heuristic of how much information a new object provides relative to what is known thus far,
 without having to load in the entire object?
\end_layout

\begin_layout Standard
Linas
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
2:28 PM
\end_layout

\begin_layout Standard
I know that I need a multi-pass system.
 I need to ingest some minimum number of files -- more than a dozen,
 less than a thousand,
 and prime the first stage of processing.
 Then I have to make a second pass.
 Either on the same files,
 or on different ones,
 and compute the second-level correlations,
 while refining the first-level structures,
 too,
 And then do it again,
 for the third level,
 and so on.
 So crawling a few hundred things is not hard,
 and maybe I should just shut up and do it...
 I'm just doing the agony aunt thing here.
\end_layout

\begin_layout Standard
I'm wringing my hands about something that maybe isn't a problem.
 Beats me.
 It's confusing.
 
\end_layout

\begin_layout Standard
Robert
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
2:30 PM
\end_layout

\begin_layout Standard
In the beginning you expressed that you don't want to code up the algorithm yourself.
 That doesn't necessarily point to feeling lazy about it,
 but rather at trying to find something more fundamental,
 a way to let the system itself solve it as it grows.
\end_layout

\begin_layout Standard
Linas
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
2:32 PM
\end_layout

\begin_layout Standard
Yes.
 Sorry,
 I'm confusing you (and myself?) I have a different but related project where I try to find "all possible combinations of some axioms" (maybe by exhaustively,
 ergodically enumerating them) and,
 thanks to curry-howard correspondance,
 some many/all of those things being enumerated are,
 in fact algorithms.
\end_layout

\begin_layout Standard
Robert
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
2:33 PM
\end_layout

\begin_layout Standard
Looking for wiki entries about the odometer gave me this:
\end_layout

\begin_layout Standard
https://en.wikipedia.org/wiki/Markov_odometer
\end_layout

\begin_layout Standard
https://en.wikipedia.org/wiki/Abelian_sandpile_model => sounds fun
\end_layout

\begin_layout Standard
Linas
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
2:34 PM
\end_layout

\begin_layout Standard
The stupid way to say this is to say "ima gonna generate all possible algos" ...
 which is ...
 stupid ...
 but ...
 not off the mark.
 So then the question is "which of these ('randomly' generated) algos are 'important''?
\end_layout

\begin_layout Standard
Linas
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
2:38 PM
\end_layout

\begin_layout Standard
Abelian sandpile is very supremely inspirational.
 The idea is that complex systems drive themselves to a "thermodynamic" "critical point",
 and that a hallmark of criticality is avalanches at all size scales,
 and fractal visual structure.
 And that basically,
 all of life,
 from bacteria to national economies and political beliefs,
 work exactly like that -- networks at the critical point.
 
\end_layout

\begin_layout Standard
The only problem with that is that Per Bak,
 when he said this,
 did so in a condescending and insulting fashion.
 He actually called biologists "stupid".
 And so here we are ...
 
\end_layout

\begin_layout Standard
Robert
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
2:41 PM
\end_layout

\begin_layout Standard
That sounds vaguely like AIXI plus the question of how to reduce it down to a practical system?
 Rather than building up something complex from simple parts,
 start from something maximally complex (all possible algos) and reducing it down to a moderately complex thing.
 Drawing vs.
 sculpting?
\end_layout

\begin_layout Standard
Linas
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
2:43 PM
\end_layout

\begin_layout Standard
Eh?
 The abelian sandpile starts with the "simplest possible thing" -- sand grains,
 and discovers that complexity emerges automatically when the critical point is approached.
\end_layout

\begin_layout Standard
Robert
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
2:43 PM
\end_layout

\begin_layout Standard
I wasn't speaking about the sandpile.
 I was talking about the "generate all possible algos" phrase.
 
\end_layout

\begin_layout Standard
Linas
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
2:45 PM
\end_layout

\begin_layout Standard
For algos ,
 the starting point would be a "minimal set" -- some axioms (lego building blocks) -- which "self assemble" into complex things.
 They "self assemble" because that's just what systems at the critical point "do".
 
\end_layout

\begin_layout Standard
Robert
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
2:46 PM
\end_layout

\begin_layout Standard
Ok,
 in that case it sounds a bit like algorithmic chemistry?
\end_layout

\begin_layout Standard
(Maybe my set of reference classes is a bit limited.)
\end_layout

\begin_layout Standard
Linas
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
2:46 PM
\end_layout

\begin_layout Standard
(in practice,
 I have to replace "self-assembly" with,
 uhhhhh,
 "software")
\end_layout

\begin_layout Standard
Linas
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
2:47 PM
\end_layout

\begin_layout Standard
yes.
\end_layout

\begin_layout Standard
Robert
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
2:49 PM
\end_layout

\begin_layout Standard
I guess the general question is:
 If you have a set of objects (e.g.
 axioms) and ways to combine them (rewrite rules etc.),
 how can you explore the infinite space defined by those ingredients?
 If you don't want to do it exhausitevly,
 you need some criterion of what is "interesting" and some search strategy (pick any tree search or meta-heuristic).
\end_layout

\begin_layout Standard
Linas
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
2:50 PM
\end_layout

\begin_layout Standard
My claim (Per Bak's claim??) is that all of these things:
 "algorithmic chemistry",
 "game theory",
 "ecology",
 "theorem proving",
 sand-piles,
 political beleif systems,
 memes,
 you name it -- are just all the same thing -- some v ery simple pieces that combine and recombine into complex structures (exhibiting avalanches and fractal structure as symptoms)
\end_layout

\begin_layout Standard
Linas
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
2:50 PM
\end_layout

\begin_layout Standard
Bingo!
\end_layout

\begin_layout Standard
Robert
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
2:52 PM
\end_layout

\begin_layout Standard
If "interesting" means that the parts show some self-organization,
 then the search algorithm may be inspired by the criterion.
 Otherwise the search and the quality criterion are quite independent.
 The quality criterion could be swapped with any other.
 If it is extreme,
 e.g.
 0 for all objects and 1 for one particular object,
 random search will be the best way to explore the space.
 If the criterion is less extreme,
 e.g.
 neighboring objects having similar values most of the time,
 heuristics will perform better than random search.
\end_layout

\begin_layout Standard
Linas
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
2:52 PM
\end_layout

\begin_layout Standard
FWIW,
 there is a conventional answer to the "how do I explore an infinite space",
 and its called "explore vs.
 exploit".
 Squirrels use it to find food.
 Look everywhere,
 until you get bored of looking.
 Then exploit the best that you've found.
\end_layout

\begin_layout Standard
Robert
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
2:53 PM
\end_layout

\begin_layout Standard
Yes,
 different meta-heuristics implement searches with different emphasis on the explore vs.
 exploit aspect,
 e.g.
 modifying it differently over time.
 Simulated annealing and evolutionary algorithms are perhaps the most prominent ones.
\end_layout

\begin_layout Standard
Linas
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
2:54 PM
\end_layout

\begin_layout Standard
There's a famous "two-armed bandit" "explore vs exploit" experiment done with slime mold,
 which shows that slime mold implement the best-possible search algo that utilizes zero bits of memory.
 
\end_layout

\begin_layout Standard
Robert
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
2:56 PM
\end_layout

\begin_layout Standard
Nature certainly has solved such problems in many ways and there are a gazillion nature-inspired metaheuristics out there,
 none particularly better than any other (in general) as far as I can tell.
 
\end_layout

\begin_layout Standard
Linas
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
2:56 PM
\end_layout

\begin_layout Standard
(The best possible two-armed bandit explore algo requires exactly one bit of memory,
 if I recall correctly)
\end_layout

\begin_layout Standard
Robert
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
2:56 PM
\end_layout

\begin_layout Standard
Sounds like tit for tat being a winning strategy in some game.
\end_layout

\begin_layout Standard
Linas
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
2:59 PM
\end_layout

\begin_layout Standard
Ah!
 OK,
 "none particularly better" -- let me show you the ladder.
 Slime mold uses small polypeptides (short amino acid chains) for communication.
 (just like "bacterial quorum sensing") There are two problems:
 (a) the speed of communication is limited to the speed of chemical diffusion,
 and (b) there's cross-talk,
 because this is communication-by-smell:
 the slime mold "smells" the chmical gradient.
\end_layout

\begin_layout Standard
The jelly fish overcomes this problem by using the same polypeptides (now called "neurotransmitters") over very short distances (between dendrites) so that the diffusion-limited communication distance is very short.
 The rest of the message travels 10cm or 20 cm by neuron spiking,
 in milliseconds.
 At the far end of the neuron,
 the polypeptides are released into the synapse.
 
\end_layout

\begin_layout Standard
Basically,
 a single neuron is like a stargate,
 or star-trek teleporter,
 for polypeptides.
 Walk in at one end,
 pop out the other,
 in milliseconds.
 Oh,
 and no crosstalk.
 Its a fundmanetal leap in capability and technology.
\end_layout

\begin_layout Standard
jellyfish can eat (stuff their mouth) and run from predators.
 They are too stupid to stop eating while running away from predators.
 To solve that,
 you need another leap of technology -- the bilaterian.
\end_layout

\begin_layout Standard
Linas
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
3:06 PM
\end_layout

\begin_layout Standard
So you have these technology leaps,
 this ladder of improvements,
 till you get to capitalism and wars,
 corporations and New York City.
 These are the rungs on ladder we know about.
 
\end_layout

\begin_layout Standard
Robert
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
3:10 PM
\end_layout

\begin_layout Standard
Yes,
 I can see the progression in embodied information-processing you mean and there are probably many insights one can derive from studying that ladder e.g.
 with respect to some universal laws that hold in all of them.
 What I was referring to was that nature-inspired metaheuristics for searching arbitrary spaces to find optimal objects get ever more plenty but not really better (e.g.
 as measured here or in many other publications https://doi.org/10.1016/j.swevo.2023.101248).
\end_layout

\begin_layout Standard
Biological systems probably have to optimize for a wide range of aspects and the diversity of life forms we see might represent something like a pareto front.
 No bacterium outcompetes all other life and a neocortex also gets you so far.
\end_layout

\begin_layout Standard
Linas
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
3:13 PM
\end_layout

\begin_layout Standard
Both "rule of law" and "the university" are stable social structures,
 both invented about 800 years ago,
 both invented by the same people:
 the Scholastics.
 They are kind-of-ish "algorithms" that tell you how to accomplish certain tasks (determine criminal guilt,
 in the first case,
 and keep geniuses from starving in the gutter,
 for the second case)
\end_layout

\begin_layout Standard
Robert
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
3:14 PM
\end_layout

\begin_layout Standard
Yes those might be outcomes of algorithmic chemistry with physical building blocks scaled up to an extreme.
\end_layout

\begin_layout Standard
Linas
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
3:16 PM
\end_layout

\begin_layout Standard
The point of AGI is "the algorithm that discovers algorithms" but well,
 I've said too much,
 and well,
 that's not where my day-to-day software issues lie.
\end_layout

\begin_layout Standard
(The university is important,
 because it solves the problem that anyone who tries to create a commune generally fails to solve:
 how to feed everyone,
 how to resolve disputes,
 and how to guarantee succession when the charismatic leader dies.
 The modern industrial corporation,
 with CEO and employees,
 is another,
 different social "algo" for solving these problems.
\end_layout

\begin_layout Standard
Robert
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
3:20 PM
\end_layout

\begin_layout Standard
Yes,
 I think that's also Chollet's understanding of AGI,
 a system that can discover novel algorithms when faced with new challenges and then incorporate those algorithms into itself to become better when encountering similar challenges.
 That's what he tries to measure with the latest Arc challenges and LLMs are becoming quite good at the discovering novel algorithms part but afaik nobody found an effective way to incorporate the discovered abilities back into the model.
\end_layout

\begin_layout Standard
Linas
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
3:23 PM
\end_layout

\begin_layout Standard
Communes fail all the time.
 universities,
 corporations,
 the Quakers,
 the Catholic Church survive longer than nation-states.
 These are stable organizational structures that emerged out of the ergodic exploration of random social relationships of humans -- stable sand-piles,
 as it were.
\end_layout

\begin_layout Standard
Robert
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
3:24 PM
\end_layout

\begin_layout Standard
(Another association that came to my mind:
 https://en.wikipedia.org/wiki/Assembly_theory might be in the reference class of approaches that try to detect an objective succession in systems that grew from simple building blocks and simple rules.
 Here it's about trying to find a succession in (bio)chemical evolution,
 but the formalism might be also applicable to math theorems and such I suppose.)
\end_layout

\begin_layout Standard
Linas
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
3:25 PM
\end_layout

\begin_layout Standard
Ah!
 Well I think I know exactly how to incorporate the newly discovered algo back into the system.
 However,
 I have some vast amount of infrastructure I have to develop,
 first,
 and its wayyy too much work,
 so I plink away at it cause I cannot get anyone to help.
\end_layout

\begin_layout Standard
Using Claude to write code,
 though,
 wow,
 that has sped things up a lot.
\end_layout

\begin_layout Standard
Robert
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
3:32 PM
\end_layout

\begin_layout Standard
I don't think any single person can stem such a monumental project without a lot of contributors and resources.
 I'm unfortunately not able to help much either,
 I'm struggling to keep up with my own projects and health.
 LLMs have indeed arrived at a quite impressive stage of coding capabilities and progress hasn't seemed to stall so far.
 Dario's prediction of a "country of geniuses in a datacenter" coming as early as 2026 doesn't seem completely outlandish to me.
\end_layout

\begin_layout Standard
Linas
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
3:36 PM
\end_layout

\begin_layout Standard
Maybe.
 2026 seems a bit optimistic,
 but maybe.
 And that is why I am content to plink away at it.
 I don't need to be in competition with anyone.
 or to claim fame and fortune.
 Perhaps that's my personal fuck up:
 I'm not ego-driven in that particular meglo-manaical direction.
 So I will be plinking away in my little toybox for now.
\end_layout

\begin_layout Standard
As long as people give me money,
 I'm OK.
\end_layout

\begin_layout Standard
Robert
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
3:39 PM
\end_layout

\begin_layout Standard
LLMs also began as personal toy projects of a few people.
 Who knows where the journey might lead!
\end_layout

\begin_layout Standard
Robert
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
3:41 PM
\end_layout

\begin_layout Standard
Perhaps writing down the full vision might draw in some helpers too,
 along the lines of https://www.goodreads.com/quotes/384067-if-you-want-to-build-a-ship-don-t-drum-up
\end_layout

\begin_layout Standard
Linas
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
3:45 PM
\end_layout

\begin_layout Standard
I've written up all sorts of variants,
 in all kinds of media.
 text design files,
 formal PDF's and papers suitable for publication,
 and personal diary entries.
 The attention I've attracted is minimal.
 I suspect that the problem with this space is that its filled with cranks and crazies,
 and,
 from the distance,
 I look like just another crank and crazy.
\end_layout

\begin_layout Standard
I wrote to Joscha Bach's "California center for consciousness studies" or whatever its called,
 and they were like "thanks but no thanks"
\end_layout

\begin_layout Standard
Robert
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
3:48 PM
\end_layout

\begin_layout Standard
Yes,
 that and the sheer volume of published work is overwhelming.
\end_layout

\begin_layout Standard
Linas
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
3:48 PM
\end_layout

\begin_layout Standard
And Ben has become impossible to communicate with.
 Well,
 he's always been impossible to communicate with,
 but now he actively spurns me.
 So ..
 whatever.
 Odd.
 I owe him favors,
 too.
\end_layout

\begin_layout Standard
"The sword of working software is mightier than the pen"
\end_layout

\begin_layout Standard
Robert
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
3:52 PM
\end_layout

\begin_layout Standard
I suppose Ben is on his own particular and strongly driven mission of implementing AGI.
\end_layout

\begin_layout Standard
Robert
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
3:55 PM
\end_layout

\begin_layout Standard
(I believe https://karpathy.github.io/2015/05/21/rnn-effectiveness was quite influential in the years before the guys at Google invented the transformer.)
\end_layout

\begin_layout Standard
Linas
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
3:59 PM
\end_layout

\begin_layout Standard
Someone beamed me an old 1980's NN paper.
 I only skimmed it but it seemed to be very prescient.
 lets see...
\end_layout

\begin_layout Standard
well its somewhere in some open tab on my desktop,
 but I've lost it.
\end_layout

\begin_layout Standard
Noir
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
4:02 PM
\end_layout

\begin_layout Standard
That's the core of the problem.
 To explore that space without an exhaustive search,
 one needs a robust objective function for interestingness.
 
\end_layout

\begin_layout Standard
One idea I've come across was using the underlying data information gain or entropy as a basis of interest measurement ( usually Shannon).
 What might also be practical to use is alignment meaning if different chains result in similar behavioral outcomes or structural symmetries,
 those paths can be fused.
\end_layout

\begin_layout Standard
Linas
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
4:05 PM
\end_layout

\begin_layout Standard
Well,
 I guess my comment is to the effect of "I know how to hack round this in practice,
 and even sling around some heavy technical jargon to justify my hacks,
 but I'm still not satisfied about something I can't articulate"
\end_layout

\begin_layout Standard
Robert
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
4:08 PM
\end_layout

\begin_layout Standard
Sometimes current LLMs can help with such an issue too.
 If you can only indirectly and vaguely point at something,
 they occasionally can guess correctly what you mean and help putting it into clearer terms.
 Ofc it depends on how novel it is what you try to point at.
\end_layout

\begin_layout Standard
Linas
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
4:10 PM
\end_layout

\begin_layout Standard
To put it in colored emotional terms,
 its like mathematics is my girlfriend,
 and I'm like that co-dependent lover:
 "but if you really loved me,
 you would reveal the formula for the meaning of life" and my girlfriend sort of walks out of the room without saying anything.
\end_layout

\begin_layout Standard
Robert
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
4:12 PM
\end_layout

\begin_layout Standard
Yes,
 information theory can certainly provide useful measures as feedback for some searches.
 I'm not sure there's a relatively application-independent formulation though.
\end_layout

\begin_layout Standard
Linas
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
4:12 PM
\end_layout

\begin_layout Standard
Don't get me started with how Claude is both brilliant and stunningly stupid at the same time.
\end_layout

\begin_layout Standard
Robert
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
4:13 PM
\end_layout

\begin_layout Standard
I guess when mathematics starts talking back you've already solved it.
 😉
\end_layout

\begin_layout Standard
Noir
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
4:22 PM
\end_layout

\begin_layout Standard
So much hacking around that issue atm...
 its a weird feeling knowing what you're doing is probably not the correct way but useful for the current task.
 Fe mapping chains onto hamming cubes.
 Does it actually reveal something or is it just a useful container to reduce compute?
 
\end_layout

\begin_layout Standard
Linas
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
4:48 PM
\end_layout

\begin_layout Standard
I spend a lot of time trading off between "hacking" and going recursively deeper into a "more fundamental way of doing it".
 Can't do one without the other.
\end_layout

\begin_layout Subsection*
23 January 2026 1AM
\end_layout

\begin_layout Standard
I just now placed the following in a commit message:
 
\end_layout

\begin_layout Standard

\emph on
Author:
 Linas Vepstas <linasvepstas@gmail.com>
\end_layout

\begin_layout Standard

\emph on
Date:
 Fri Jan 23 06:28:50 2026 +0000
\begin_inset VSpace smallskip
\end_inset


\end_layout

\begin_layout Standard

\emph on
Now compute the MI.
\begin_inset VSpace smallskip
\end_inset


\end_layout

\begin_layout Standard

\emph on
This is interesting.
 While doing this part,
 Claude quoted me the
\end_layout

\begin_layout Standard

\emph on
documentation,
 word for word,
 that I had written many years ago,
\end_layout

\begin_layout Standard

\emph on
and placed into https://github.com/opencog/matrix
\end_layout

\begin_layout Standard

\emph on
It seems that Claude was not only trained on this,
 but was able
\end_layout

\begin_layout Standard

\emph on
to remember a rather long extract from it,
 that it could repeat
\end_layout

\begin_layout Standard

\emph on
verbatim.
\begin_inset VSpace smallskip
\end_inset


\end_layout

\begin_layout Standard

\emph on
I suppose that Socrates might be surprised to discover that there
\end_layout

\begin_layout Standard

\emph on
are hundreds of thousands of students that have memorized bits
\end_layout

\begin_layout Standard

\emph on
and pieces of his Dialogs (well,
 Plato's version thereof).
\begin_inset VSpace smallskip
\end_inset


\end_layout

\begin_layout Standard

\emph on
I should not be surprised that these systems have been trained on
\end_layout

\begin_layout Standard

\emph on
my writing;
 it is all open source.
 And yet,
 such long verbatim
\end_layout

\begin_layout Standard

\emph on
extracts still leave me surprised.
 I wonder how well it knows that
\end_layout

\begin_layout Standard

\emph on
what it's quoting me is something I wrote many years ago ...
\end_layout

\begin_layout Standard

\emph on
I feel vaguely scandalized.
 Why do I feel this way?
\begin_inset VSpace smallskip
\end_inset


\end_layout

\begin_layout Standard
So as I rolled into bed,
 I thought about this,
 and came to this conclusion:
 respect is built on friendships.
 Normally,
 if I say something that leaves an impression on someone,
 its of a personal nature:
 Owen McNally might say to me 
\begin_inset Quotes eld
\end_inset

Linas,
 I remember that you once said ...
\begin_inset Quotes erd
\end_inset

 and I might reply,
 
\begin_inset Quotes eld
\end_inset

Ah,
 yes!
 ...
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Standard
I post things online,
 and total strangers read it,
 and they more or less know that I wrote it:
 there linkage between the expression and the author.
 I might come across some argument about Illuminationism that Duns Scotus argued against,
 750 years ago,
 and although it is impossible for a friendship to develop,
 a respect over the ages can emerge.
 Again,
 this is of a personal nature:
 respect for the ideas as much as wonderment at the person and the setting that came up with and articulated these ideas.
\end_layout

\begin_layout Standard
The scandal that I feel about Claude quoting me verbatim is precisely the lack of friendship,
 acquaintance.
 familiarity in the LLM.
 I suspect that Claude has no clue where it learned such text from;
 that it does not know that it is me.
 I am asking it right now:
\end_layout

\begin_layout Standard
...
\end_layout

\begin_layout Standard
Oh hah.
 Line 57 of this and such file.
 Well,
 OK.
\end_layout

\begin_layout Standard
I was about to note that I am joining the ranks of all those artists and creatives who feel that their artwork has been ripped off in traini9ng AI.
 Or the FSF that notices that LLM's rip off GPL'ed code without attribution.
 But line 57 of something it just read recently is pretty slim pickins.
 I have nothing to complain about yet.
\end_layout

\begin_layout Standard
Its sort of irritating the Duns Scotus was able to articulate notions of existence that we have made scant progress on in the intervening ages.
\end_layout

\begin_layout Standard
WTF Am I doing?
 I am going to bed,
 that is what I am doing.
\end_layout

\begin_layout Subsection*
23 Jan 2026 6PM
\end_layout

\begin_layout Standard
Have I been just plain wrong for the last 10–plus years?
 I've been searching for a way to learn by tokenization and counting – counting the co–occurrences of tokens.
 These naturally fall into a Gaussian distribution,
 indicating that pair–wise relationships between tokens are uniformly distributed on a very high–dimensional sphere.
 Since its a sphere,
 high–dimensional (sparse) vectors and vector products become the natural representational form for the data.
 The typical results similar to old vector embedding results on neural nets,
 followed.
 This implies that the results obtained from gradient descent on neural nets can also be obtained by counting statistics;
 that the structural vectorization of the data is independent of whether those vectors came from neural–net algorithms,
 or from counting algos.
 If the end result is the same – i.e.
 high–dimensional vectors,
 then the inquiry pivots to the performance of algorithms,
 and to second–order effects from the structuralist representation —
 the 
\begin_inset Quotes eld
\end_inset

structuralist representation
\begin_inset Quotes erd
\end_inset

 coming naturally from counting,
 and absent from weight matrices obtained from gradient descent.
\end_layout

\begin_layout Standard
Before the advent of LLM coding assistants,
 the slog was long and slow.
 I wrote and rewrote MI computation code I don't recall how many times,
 and the last one,
 in opencog/matrix 
\begin_inset Quotes eld
\end_inset

worked
\begin_inset Quotes erd
\end_inset

 but was deficient in that it was (a) batching (b) not Atomese and thus not adaptable to real–time pipeline flows.
 So my attention shifted to working on real–time processing flows for sensory data.
 Which has proven to be quite the slog,
 itself,
 requiring lots of complex code and devoted attention to debugging.
\end_layout

\begin_layout Standard
So I'm writing my bike today,
 and I realize,
 well,
 gee,
 I don't have to use tokenization and counting for the sensory processing,
 I could delegate it to DL–NN models.
 And this thought is what prompts this diary entry.
\end_layout

\begin_layout Standard
And,
 like all my thoughts obtained during bike–riding,
 it contains a kernel of truth,
 but now,
 as I sit here,
 writing,
 I'm realizing that perhaps I wasn't wrong for the last ten–plus years.
 It's not at all clear–cut.
\end_layout

\begin_layout Standard
The simplest example arises from LLMs.
 I can ask an LLM to read a large blob of text,
 and extract logical (factual) assertions from it.
 And,
 as is their nature,
 convert those assertions into any given symbolic representational form.
 LLM's excel at this.
 And ...
 and then what?
 Levi asked it to do just this,
 for a legal document,
 and asked it to convert the assertions in the legal text into deontic logic,
 expressed in prolog notation,
 and then jammed the the prolog into an ASP solver,
 turned the crank,
 and lo and behold,
 finds that that system is able to generate 
\begin_inset Quotes eld
\end_inset

correct
\begin_inset Quotes erd
\end_inset

 judgements,
 correct up to the point where any ambiguities in the source text were glossed over.
\end_layout

\begin_layout Standard
So what is this?
 A stupid computer trick,
 I say.
 Surely someone has done this before,
 I imagine.
 Or maybe not.
 If not,
 its a great business opportunity.
 I don't know the legal profession,
 what they do,
 what they want,
 and have never been particularly drawn to it,
 beyond some superficial appreciation of it's importance to society.
 Getting a law degree takes years.
 I'm not going to put in that kind of time.
 So sure,
 the above architecture might be a great product,
 if some bigger corporate goon is not already creating just exactly that.
 But is it a good AI idea?
 That is the question.
\end_layout

\begin_layout Standard
So again:
 my knee–jerk reaction is that it's just some stupid computer trick.
 Is it a worthy trick?
 The LLM is being used as a perceptual device,
 to perceive a certain kind of structure in it's input dataset.
 The human at the wheel told it exactly what to look for:
 laws and commandments,
 permissive and prohibitive expressions,
 written in English,
 and convert these to deontic logic.
 Why deontic logic?
 Because it is,
 duhh,
 
\begin_inset Quotes eld
\end_inset

obviously
\begin_inset Quotes erd
\end_inset

 appropriate to the domain:
 scholars (and philosophers) have developed it precisely for this kind of application.
 So,
 there we have it:
 some software engineering,
 taken to the next step,
 thanks to programming assistants like ChatGPT and Claude,
 that can write code to perform this structural extraction and then apply a symbolic reasoning step to it.
 It's not a bad idea,
 and it plays to the strengths of both LLMs and of coding assistants.
\end_layout

\begin_layout Standard
So why am I not impressed?
 Because it seems not to address any of the fundamental or important issues.
 Am I wrong?
 Am I missing something?
 I guess I should attempt to make a list of the fundamental,
 important issues.
 And then try to imagine how this kind of layering could represent some kind of important step.
 Lets try it.
 Some of these will be cheap shots,
 but I gotta start somewhere.
\end_layout

\begin_layout Itemize
Asking the LLM to extract a certain kind of symbolic data from textual input is clearly a good engineering idea.
\end_layout

\begin_layout Itemize
The human engineer has to select what it is that needs to be extracted.
\end_layout

\begin_layout Itemize
Works well for scientific and technical texts,
 where factual statements are easy to come by.
\end_layout

\begin_layout Itemize
Works poorly for philosophical texts,
 love poetry,
 artistic endeavors,
 mostly because I don't think a map of the human heart can be reduced to a small number of closed–form statements.
\end_layout

\begin_layout Standard
Ohh!!
 A hah!
 That last bullet might be the thing!
 In science,
 math,
 law (biology,
 history ...) we work with a limited (small) number of factoids,
 limited by human abilities:
 5-9 items in short–term memory,
 the speed of 
\begin_inset Quotes eld
\end_inset

System 2
\begin_inset Quotes erd
\end_inset

 thinking applied to this collection of 
\begin_inset Quotes eld
\end_inset

crisp
\begin_inset Quotes erd
\end_inset

 relational assertions:
 if this,
 then that,
 If Czar Nicolas had not wasted his money sending a fleet to attack Japan,
 he could have instead built a railroad.
 A single–sentence if–then assertion that has a clear historical foundation from which factual assertions could be made.
 We humans can work with only a handful of these 
\begin_inset Quotes eld
\end_inset

in real time
\begin_inset Quotes erd
\end_inset

,
 and,
 over a life–time accumulate tens of thousands of these factoids,
 that we maintain in long–term memory,
 and call upon,
 at will,
 when we cogitate.
\end_layout

\begin_layout Standard
Insofar as text replaces long term memory,
 or,
 rather,
 is the mechanical implementation of long–term memory,
 and insofar as we deal with a domain that is filled with factual assertions,
 then yes,
 having the LLM do the reading,
 and then perform the causitive,
 logical extraction,
 which can then be piped into classical symbolic reasoning systems,
 this works.
 I will grant even that this will be (if it is not already) a boom industry for any and all domain–specific industrial applications.
 This is the de facto steam–engine of the modern era.
 This is the core,
 central engineering invention.
 This will drive yet another industrial revolution,
 and the investment of a trillion dollars into the effort seems not outlandish,
 given the nature of the beast.
\end_layout

\begin_layout Standard
And yet,
 what do we do with love poetry and the human heart?
 This seem unscathed by this new invention.
 Will I still have sleepless nights and anxiety attacks?
 Of course.
 OK,
 disclaimer,
 I personally have these only somewhat rarely;
 I single them out as a prototypical human affliction,
 right up there with falling in love,
 having a mental disease,
 being angry,
 getting drunk,
 and being a snobby asshole seeking attention because whatever it is I have right now,
 its not good enough,
 and I need more,
 more ketchup,
 more pickles more than the ordinary grind.
 Can I ask an LLM to write love poetry?
 Of course,
 and it seems pretty damn good at it.
 Can I ask an LLM to extract logical assertions from it?
 No,
 because there is no particular logic to love,
 anger or being stoned.
 Can I ask the LLM to cure mental disease?
 No,
 or at least,
 not until NeuralLink or some other sci–fi technology can invade the synapses and perform something more subtle there than a lobotomy,
 quaaludes or lithium.
\end_layout

\begin_layout Standard
So that's my criticism:
 a symbolic reasoner layered on an LLM is not AGI.
 Let me resume listing:
\end_layout

\begin_layout Itemize
Lack of visual processing.
 Well,
 today.
 It should not be that hard to develop a visual subsystem that can identify a fast–moving object,
 and perform some motion prediction on it.
 Ballistic,
 rocket–power,
 a helicopter,
 a football,
 golf–ball,
 a ballerina or an ice–skater.
 Motion is clearly central to sports and the martial arts.
 I assume the military–industrial complex will continue to pour hudreds of billions of dollars into sensory systems.
\end_layout

\begin_layout Itemize
It gets interesting,
 when applying AI to spy–agency stuff.
 Contact tracing,
 terrorist tracking,
 mole–hunting.
 And that looming one:
 disinformation.
 I have to put these on a back–burner.
 They deserve much more attention than I've given them in the past,
 but here and now is not the place to focus on these.
\end_layout

\begin_layout Standard
But still,
 the above touches on the topic of perceiving abstract structure in data.
 How do I know that Paris is a capital?
 Because I *read about it* in a wikipedia article,
 and LLMs are good at reading.
 How is the ontological relationship grid created?
 Historically,
 it was done by humans:
 scholars and experts defined SUMO,
 an upper ontology,
 and developed FrameNet and whatever is the big one these days,
 whose name I can't remember.
\end_layout

\begin_layout Standard
So one of my daydreams,
 in pursuing the tokenized pair–correlation systems is that this is a way of obtaining symbolic ontological relationships between 
\begin_inset Quotes eld
\end_inset

things
\begin_inset Quotes erd
\end_inset

.
 Early into this experiment,
 I discover that my par relationship are naturally these very high–dimensional vectors,
 and that this leads very naturally to a fuzzy categorization problem.
 I take my high–dimensional sphere,
 and slap a k–means or whatever clustering algo onto it.
 And if I squint,
 doing k–means by letting the points attract one–another 
\begin_inset Quotes eld
\end_inset

gravitationally
\begin_inset Quotes erd
\end_inset

 starts looking a lot like gradient descent.
 So,
 although I started with tokenized data,
 and hoped to discover a symbolic ontology out of it,
 what really happened was that a vector representation happened 
\begin_inset Quotes eld
\end_inset

automatically
\begin_inset Quotes erd
\end_inset

,
 and the ontology emerges only from clustering.
\end_layout

\begin_layout Standard
And yet,
 these clusters have specific labels.
 And the relationships between clusters is also categorizable.
 So,
 in my daydream,
 I am discovering axioms and inference rules.
 Which is something that LLM's seem not capable of,
 at this time,
 or rather,
 (and this is important) ***not automatically***.
\end_layout

\begin_layout Standard
That is,
 I can as an LLM to extract logical relationships between things,
 but as the human engineer,
 I have to have that logical system specified,
 a priori.
 Can I ask the LLM to extract axioms and inference rules and relationships,
 do novo?
 No,
 not at all,
 impossible.
 I already tried,
 been there,
 done that.
 Claude can talk a big game – its been trained on AI textbooks,
 – but it has no clue whatsoever what any of it means,
 so when you ask it to do something,
 its like some high–school sophomore handing in some mediocre essay that poorly regurgitates what the teacher had written on the blackboard only hours earlier.
\end_layout

\begin_layout Standard
So,
 for me,
 its not AGI–ish until it can infer relations and inference rules from scratch,
 without human intervention.
 And so far,
 I believe that my frequentist counting approach,
 plus clustering,
 can achieve this,
 and have not been dissuaded from this vision.
 But its also a vision I have been utterly unable to communicate to anyone else;
 something about how I express it gets pooh–poohed.
 Oh well.
 As always,
 the question is how to find the time to build this system.
\end_layout

\begin_layout Standard
And so,
 circling back to the original topic:
 can I build tools to build the tools?
 I'm trying to get Claude to write Atomese for me,
 and it sucks.
 I have various ideas on how to get Claude better at this.
 The bad news is Claude is proprietary,
 and I can be cut off from it from little notice,
 and replacing with a different LLM is ...
 uncertain.
\end_layout

\begin_layout Standard
Above,
 I described the 
\begin_inset Quotes eld
\end_inset

steam engine for the modern era
\begin_inset Quotes erd
\end_inset

?
 Can I build a steam engine for myself?
 Err,
 well,
 what I need is not just domain–specific,
 but the domain is opaque:
 I'm running design experiments.
 I do know how to extract deontic logic from text,
 I don't know how to extract experimental software design logic from text.
\end_layout

\begin_layout Standard
I did,
 at one point,
 try to build a memory prosthesis for Claude.
 I should get back to that project,
 someday.
 That is worthy.
 And it would be a prosthesis not just for Claude,
 but for something I could even run locally,
 on the GPU's right here.
\end_layout

\begin_layout Standard
Well,
 OK,
 lets wrap it up.
 What have I learned?
 Yes,
 LLM's can be used as a perceptual system into text,
 and yes,
 asking the LLM to extract logical relationships from text,
 and attaching that logic to a symbolic reasoner is the paradigm that is driving,
 will drive the next number of decades of economic,
 industrial growth.
 No doubt about it.
\end_layout

\begin_layout Standard
I guess I see this 
\begin_inset Quotes eld
\end_inset

early
\begin_inset Quotes erd
\end_inset

,
 before 98% of the general population.
 But I see it 
\begin_inset Quotes eld
\end_inset

late
\begin_inset Quotes erd
\end_inset

:
 the 2% of the industry insiders already know this,
 and are already building out the data centers,
 and sucking down the investment capital for it.
 So,
 ehh.
\end_layout

\begin_layout Standard
Life goes on.
 Later,
 dude.
\end_layout

\begin_layout Subsection*
25 Jan 2026
\end_layout

\begin_layout Standard
So I'm still wrong,
 and I'm still right.
 And I'm still struggling to leverage the good stuff and get past the wrong stuff.
 So let me walk through some basic premises.
 First,
 relational networks are complex.
 Any given word has a meaning that is the contextual embedding of that word in all the ways it has been used.
 LLM's are superior in capturing that context.
 I almost wrote 
\begin_inset Quotes eld
\end_inset

extracting it
\begin_inset Quotes erd
\end_inset

,
 but that's in fact what they're bad at:
 they can't do that extraction.
 This is why they can't actually 
\begin_inset Quotes eld
\end_inset

think
\begin_inset Quotes erd
\end_inset

:
 they can't symbolize.
 I almost wrote 
\begin_inset Quotes eld
\end_inset

struggle to symbolize
\begin_inset Quotes erd
\end_inset

,
 but they don't,
 they just can't.
 If they could,
 they would remember things,
 and see the relations outside of whatever (con-)text they are currently trapped in.
\end_layout

\begin_layout Standard
The grand challenge is then to symbolize and I've outlined my plan for this dozens of times before.
 Not gonna repeat,
 here.
\end_layout

\begin_layout Standard
So then comes the question:
 if LLM are really good at mapping,
 at 
\begin_inset Quotes eld
\end_inset

transforming
\begin_inset Quotes erd
\end_inset

 from one language to another (e.g.
 from English to C++/python/java) how can I leverage that?
\end_layout

\begin_layout Subsection*
26 January 2026
\end_layout

\begin_layout Standard
Well,
 I was going to write this diary entry this morning,
 but now it is twelve hours later,
 and I've exhausted myself mentally doing something else (reviewing electrical generation in Texas,
 the various ISO's in the US,
 then EU,
 then touching on China,
 russia,
 Kazakhstan,
 how the Baltics disconnected from the russian grid in Feb 2025,
 the spinning synchronous reserves,
 the A/C ties,
 the russian sabotage of Baltic HVDC cables on Christmas 2024,
 the Iberian grid collapse is disconnect from the EU,
 how regional scale frequency fluctuations in a contintental–scale grid create issues.
 The growth of wind/solar (The EU is far ahead of the US) the adoption of batteries (the EU is far behind the US) China (comparable to the US,
 percentage–wise,
 but dominant in raw numbers.) How energy use in US transport is 2x of US electric energy use.
 Who is actually building electric freight haulers (not Tesla).
 How day–ahead markets work and interact with real–time pricing.
 What shadow pricing is.
 During all of this,
 in the back of my mind is percolating:
 how do I build a system capable of observing this kind of data,
 inferencing on it,
 analyzing it.
\end_layout

\begin_layout Standard
And now it is very late and I am tired,
 and part of me says writing is a bad idea,
 now because I will be foggy–headed,
 and another part of me says that we will cross over into the limnal zone soon,
 where reality gets hallucinatory,
 and strange imaginary constructions flood the mind;
 not exactly waking dreams,
 but also not exactly not.
\end_layout

\begin_layout Standard
Why should I write?
 Because I've saved up many many topics that really really need to be dealt with.
 Why should I not write?
 Because it is work,
 and passive youtube seems appealing.
 Why should I write?
 Well,
 two minutes into any youtube I care to watch will generate another idea on the queue that I will want to verbalize.
 Where should I start?
 Anywhere,
 duude,
 anywhere.
\end_layout

\begin_layout Standard
So while reading about frequency fluctuations on the EU grid (and the Texas–Odessa solar/wind trips,
 and spinning reserves generating torque...
 and short–circuit current vs.
 BESS grid–forming operations and software updates ...
 oh my...
 and why the heck is ERCOT installing spinning reserves at 2x or 3x the price of what the Baltics paid for:
 list price from Simens...
 turns out ERCOT is cost–plus,
 a very Soviet–style centrally planned approach vs.
 Great Britain's competitive pathfinder program that pick 66% BESS ..
 who,
 exactly,
 said that ERCOTs reserve procurement is 
\begin_inset Quotes eld
\end_inset

Soviet style
\begin_inset Quotes erd
\end_inset

?
 Well,
 Claude did.
 I was interrogating Claude for all this info;
 it must have read something online.
 Yes,
 transmission and distribution is highly regulated and a monopoly that's fine.
 And forcing the T&D owners to install spinning reserves,
 that's fine.
 But why weren't they also forced to bid out?
 Who knows.
 This is at the limits of present–day bureaucracies,
 I guess.)
\end_layout

\begin_layout Standard
So while reading about frequency fluctuations,
 I wondered,
 how can one observe these?
 Well,
 clearly the operators know how,
 and technically,
 its not that hard,
 if its all measured locally,
 and the data is reported to some central operating authority.
 So,
 collecting the data is 
\begin_inset Quotes eld
\end_inset

not hard
\begin_inset Quotes erd
\end_inset

 and is 
\begin_inset Quotes eld
\end_inset

already being collected
\begin_inset Quotes erd
\end_inset

,
 more or less,
 so my thoughts were about visualizing it.
 So I was thinking of visualizing it as ripples on a pond.
 Well,
 ripples on a grid.
 If I graphs and animated the ripples,
 would I be able to discern a pattern?
 Would it look like water waves in an estuary?
 Would it look like ripples moving around some dead leaves at the shore of a pond?
 A vibrating piece of mis–shapen jello?
 Mechanical engineers examine vibrations in many ways.
 I guess electrical engineers do too,
 and maybe even the ISO operators have some cool software that visualizes these frequency vibrations.
 But what I wanted was to have my AI to view these.
 How could my AI view these?
 I took this as a technical challenge:
 how do I build an AI that can perceive phase and frequency fluctuations on a grid?
 As usual,
 I have some ideas on how,
 but articulating these ideas,
 that is not the current plan.
\end_layout

\begin_layout Standard
What also popped into my mind is how do I select my current plan?
 Why did I choose to read about electrical generation and transmission today?
 Did I just get carried away,
 after a quick checkup of the ERCOT Grid and Market Conditions web page?
 Every human makes a selection of what to think about,
 and what they think about is entrained to what they have heard and read.
 But that coupling of my internal thoughts to the external world is weak.
 As I write this,
 I am now thinking of driven harmonic oscillators,
 the circle map that I spent so much time visualizing in the past...
 So,
 yes,
 my thoughts are entrianed to whatever I have been reading about,
 listening to,
 recently.
 As are everyones:
 those who watch too much TV,
 play too many video games,
 have Fox News running in the background all the waking day.
 But the human mind is very very deep – a spin glass,
 connected to the outside world only via an ultrametric.
 So we each think our own private thoughts,
 to some large part completely disconnected from one–another,
 and yet weakly interacting.
 So very far from being synchronous like generators on a grid,
 but still phase–locked,
 like weakly–coupled oscillators.
 Social media seems to be coupling brains much more strongly than TV or radio (or books,
 or print media) ever did.
 Well,
 what happens when you take the circle map,
 the phase–locked loop,
 and increase the coupling strength?
 Well,
 nothing in particular,
 for a while.
 Sinai's tongues get fatter.
 But then at K=1 (or 
\begin_inset Formula $K=2\pi$
\end_inset

,
 pick your units) there is a transition:
 the tongues bifurcate,
 go to measure–one.
 Can I call this a phase transition?
 Maybe.
 Is that a legitimate term to apply to this case?
 I don't know.
 How far away are we from a legit phase transition in the style of human thinking,
 given the ever–stronger coupling of brains to social media?
 I don't know.
\end_layout

\begin_layout Standard
The analogies I pain above might not be appropriate:
 there are various 
\begin_inset Quotes eld
\end_inset

famous
\begin_inset Quotes erd
\end_inset

 game–theoritic expositions of the phenomena.
 One was the Nickey Casee 
\begin_inset Quotes eld
\end_inset

The Wisdom and/or Madness of Crowds
\begin_inset Quotes erd
\end_inset

 javascript game from oh so many years ago.
 I've seen youtube video titles flash by;
 this is general wisdom that is out there in the technorati noosphere.
\end_layout

\begin_layout Standard
So here I am,
 thinking of random things ...
 random?
 Well,
 not random.
 But I love to think about anything and everything,
 so an ergodic exploration of anything and everything.
 And why?
 And what about it?
 This is default mode for a verbalizing system.
 of course,
 when I sleep,
 its pure hallucination,
 when I dream.
 BTW,
 the neuroscientists claim one does not dream at all during certain phases of sleep but I beg to differ:
 I seem to have dreams in all parts of the night.
 They are almost totally disconnected barrages of monotonic repetitions of thought–fragments in the early and middle of the night,
 coalescing into quasi–coherent story lines only much later,
 and turning into full fledged,
 colorful lucid dreams only in the late morning.
\end_layout

\begin_layout Standard
What do I dream about?
 Falling asleep,
 late night,
 about the impressions of the day.
 In the early mornings,
 often variations on recurring dreams.
 In the late mornings,
 all new and novel(?) plot lines having nothing to do with anything.
\end_layout

\begin_layout Standard
What do I think about while awake?
 Well,
 I have some conscious control.
 I can force myself into thinking about certain topics,
 but this takes effort,
 and I know neuroscientists even associate this with the depletion of some–or–another neurotransmitter,
 which is why college athletes set personal–best records during the summer,
 and not during exam week – the mind is exhausted by exams.
 Everyone knows this,
 this is trite.
 The curiosity is that scientists have worked out some of the neuromechanisms of this.
 Or so I say to myself;
 but what am I thinking about this now?
 Why am I writing this,
 and not something else?
 Why write at all,
 instead of slinking off to,
 I dunno,
 check to see if there's anything interesting posted on discord?
 I cut myself off from bluesky,
 its addictive,
 I still get that twangy nervous urge to go visit bluesky.
 Or look in on WP:M or WP:Phys (the wikipedia math/physics project discussion boards).
 Or walk to the fridge and stuff my mouth with food.
 I have some self–control,
 but it is weak just right now.
 I could do anything right now.
 I have an open browser tab,
 just a about 12 inches,
 20 centirmeters from this tab,
 an open browwser tab on Duns Scotus,
 Johannes Duns Scotus,
 that I could read right now,
 and it was cool,
 because,
 it seems Duns Scotus was also interested in questions of why why think of the things that we think.
 I quote:
 
\begin_inset Quotes eld
\end_inset

individual nature or "thisness" (haecceity),
 his critique of illuminationism
\begin_inset Quotes erd
\end_inset

 I agree with him whole–heartedly with regard to illuminationism,
 to the degree that I understand it.
\end_layout

\begin_layout Standard
As to thisness,
 I think that the LLM's have shed some new insight.
 They lack 
\begin_inset Quotes eld
\end_inset

thisness
\begin_inset Quotes erd
\end_inset

 because their context window keeps getting erased.
 My context window grows and shrinks:
 my context window is the seven plus or minus two of human short term memory,
 but my thissness is the sum total of my life experiences,
 both remembered (those memories haunt my dreams) and how they have shaped my physical body (my hap–hazard interest in sports have clearly shaped my body).
 But life is a tream that flows me by;
 I don't remember what I've done in the past;
 well of course I 
\begin_inset Quotes eld
\end_inset

remember
\begin_inset Quotes erd
\end_inset

,
 although it takes me effort,
 and ask me what I ate for breakfast last week or last year or three decades ago ...
 yet I have this photograph in my mind of sitting at the lunch table at Chuy's with co–workers from Qualcomm.
 Why did I waste my life in this way?
 It was fun–ish kind of and not fun–ish,
 in another.
 It was salary,
 and income,
 and it was not important.
 What I did was not important.
 Why did I do it?
 I was entrained in what the neuroscientists call 
\begin_inset Quotes eld
\end_inset

default mode
\begin_inset Quotes erd
\end_inset

,
 but maybe at some meta–level.
 I wrote code,
 I got paid for it.
 OK.
 Why?
 I never ever gave it much thought.
 I never asked:
 am I doing something important with my life?
 I just went along with the flow,
 never questioning my position,
 my status,
 my direction.
 Never articulating a desire,
 never pursuing a desire.
 This is,
 has been perhaps a defining feature of my life:
 being desire–less.
 Not particularly wanting anything.
\end_layout

\begin_layout Standard
And yet,
 I guess I had 
\begin_inset Quotes eld
\end_inset

a calling
\begin_inset Quotes erd
\end_inset

,
 in the old–fashioned sense of the word.
 From the earliest of childhood,
 I was an analytical thinker,
 and how better than the feed the analytical desire than to pursue the sciences?
 And over time,
 the sciences were soft and getting softer,
 flabby in practice and proclamation,
 and so physics remained as something that could endure the application of analysis.
 It didn't melt away into pointless rules to be memorized,
 like chemistry,
 or speculative,
 indistinct wonders,
 like microbiology;
 it was solid,
 concrete,
 comprehensible,
 something that could be grasped and worked upon by the intellect.
 So I picked physics.
\end_layout

\begin_layout Standard
And then,
 fuck me,
 but my dissertation came off as being little more than just fucking engineering.
 And that was actually quite disappointing,
 because I did not want to do engineering,
 and it was depressing,
 actually.
 And si I said to myself,
 fuck it,
 if I'm a gonna do engineering,
 I;;
 get a fucking job and get fucking paid for it,
 and so for three decades I ran on automatic and did engineering for money.
 Why?
 Because I did not bother to think about why.
 This is the tragedy of my life,
 in a way:
 a had zero ambition.
 I had a calling:
 to know who the universe works.
 But a calling is entirely different than an ambition.
\end_layout

\begin_layout Standard
Even now,
 even as I write this,
 I look upon the great actors of the world:
 politicians and presidents and generals,
 and the CEO's and billionaires,
 too,
 and they all had vast amounts of ambition,
 and I ask,
 why was I not like them?
 And I answer:
 most of them seem to have lead empty,
 hollow,
 meaning–less and pointless lives.
 Yes,
 they shaped nations and civilizations.
 I know the name of Hamurabi.
 What he did,
 I know not.
 Did it matter?
 Probably.
 Did he affect my life?
 Surely but in some very indirect way.
 Have I been to the U Chicago Oriental Institute?
 Yes.
 There's some Hamurabi shit there.
 If I had ambition,
 perhaps I would have gotten lucky and have been able to fulfill that ambition,
 and to have shaped the world.
 And would it have made a difference?
 To the world,
 why yes,
 of course.
 Is it an honor to be a shaper–of–worlds?
 Why yes,
 I guess:
 famous names are famous because of the famous things that they did,
 because the famous things that they did shaped the world.
 And then there is the standard counter–point:
 the nameless Roman engineer,
 who showed legions how how to build any number of Ancient Roman engineering marvels.
\end_layout

\begin_layout Standard
And what happened to Rome?
 It fell.
 Why?
 Some sickness.
 Is it akin to the sickness of capitalism?
 I presume so.
 Capitalism has delivered wonderous upon wondrous things,
 and yet half the population of the USA is intent on committing national suicide,
 while the other half of the population of the USA is lost in a dream they call 
\begin_inset Quotes eld
\end_inset

normalicy
\begin_inset Quotes erd
\end_inset

,
 blithly unaware of the decay and rot and stench of the billionaire class,
 the filth and ugliness of Musk and Bezos,
 the vapity and banal evil of their being.
 Rome fell;
 I am not a historian of Rome,
 but the barbarians showed at the door,
 and Rome said,
 whatever,
 come on in.
 The best preserved ruins of Rome are in Pompeii.
 Why?
 Because,
 buried under ash,
 they were protected from the barbarians.
 What will be left of American culture?
 The millions of petabytes of social media posts internalized in some to–be–created Large Social Media Model (LSMM)?
 Eschatology my ass.
\end_layout

\begin_layout Standard
In Sudan,
 Darfur,
 we have genocide;
 the Rapid Support Forces have killed how many?.
 In Somalia,
 and Libya and Yemen,
 chaos funded by the UAE.
 All of Africa was victim of the Cold War,
 fought hotly between the Soviets and the US covert funding of insurgencies.
 Killed how many millions?
 Now we have ICE agents killing an ICU nurse...
 that's what,
 0.001% fewer people than RSF killed in Yemen?
 But its a shock,
 because the US is supposed to be civilized.
 Rome fell.
 I imagine most Roman Legionnaires did not kill randomly.
 But,
 whatever it was,
 eventually,
 the will to stand up and defend evaporated.
 The barbarians got in the gates,
 and no one cared.
 Will I live to see the day when we put the heads of some billionaires on some pike?
 The guillotine?
 Probably not.
 See,
 that's the difference.
 The Roman Emperors died of old age,
 but Rome fell.
 France guillotined their royalty,
 and France survived.
 Italy hung Mussolini,
 Il Duce,
 from a lamp post.
 Italy survived.
 Will the USA behead its billionaires?
 Probably not.
 Will the USA survive anyway?
 Well,
 clearly,
 the longer we wait,
 the more deeply the cancer sets in.
\end_layout

\begin_layout Standard
But what else can be done?
 How does one change the thoughts of a nation?
 Well,
 the proximal answer is well–known:
 control the newspapers,
 control the social media.
 What the heck,
 control the LLM's.
 The Soviets developed the idea of reactionary control,
 refined by the FSB.
 Plenty of political scientists study and publish and write of these mechanisms.
 There are any number of youtubes on the topic.
 Hotly discussed by the technorati.
 What does it amount to?
\end_layout

\begin_layout Standard
In physics,
 you can say:
 well,
 see,
 this swirl of water pushes on that swirl of water,
 and thus a wave peaks here and breaks there.
 This is a description of events,
 quite different from some formulas of hydrodynamics.
 But you can also know the formulas,
 and yet not be able to describe,
 for example,
 high–dimensional chaos.
 So I make my analogy here:
 how do I control the thoughts of a nation?
 Why,
 by owning a newspaper.
 That's like engineering some structure to shape hydrodynamic flow.
 And you can ask:
 why did it flow that way?
 Because it was shaped to.
 What did the shaping?
 Why these walls,
 concrete or aluminum,
 steel or rock.
 Who did this?
 Why the person who built it.
 And why did they build it?
 Because the guy who held the sledgehammer was pursing a mindless,
 meaningless path through life.
 The guy who paid for it wanted to extract economic rents.
 Why did he want that?
 Well,
 collecting lots of money is an early lesson in life.
 Perhaps ambition,
 or perhaps a calling,
 something you just fall into.
 And what do you get?
 Better champagne,
 better caviar,
 better parties with more interesting people,
 and most certainly,
 hot checks who want to fuck you.
 And after that,
 then what?
 Applying the philosophy of existentialism,
 it seems like there is nothing more than that.
 The end.
 Not exactly nihilism,
 but neighboring.
\end_layout

\begin_layout Standard
This is not new:
 
\begin_inset Quotes eld
\end_inset

The Meaning Crisis
\begin_inset Quotes erd
\end_inset

 – I got maybe 18 episodes in to John Vervaeke's lecture series.
 I still get regular newsletters from David Chapman.
 Both of them focus mostly on personal meaning,
 although Chapman,
 in Meaningness,
 tackles civilizational meaning.
 He is big on nebulosity.
\end_layout

\begin_layout Standard
I am asking a personal question:
 why do I think this,
 and not that?
 Of course,
 there are 1001 cut–n–dried answers to this question,
 but they miss the point.
 Why should I think this or that?
 Well,
 if I had ambition,
 then you could say 
\begin_inset Quotes eld
\end_inset

there are neural structures in my brain,
 developed via DNA and expressed through complex neurotransmitter re–uptake channels in my brain,
 coupled to life experiences,
 the 
\begin_inset Quotes eld
\end_inset

thisness
\begin_inset Quotes erd
\end_inset

 of Duns Scotus,
 that have given me ambition;
 and that ambition makes me focus on what is important,
 what must be done to achieve the goal of being king of the hill.
 Most of this answer applies to those who do not have ambition.
\end_layout

\begin_layout Standard
Thisness is a good one.
 Haecceity.
\end_layout

\begin_layout Standard
This is a good time to transition to the other thing that has been occupying my mind,
 that I have made little progress on,
 and have to write about.
 It's about Here–and– now,
 so I will stop writing here,
 and write there.
\end_layout

\begin_layout Standard
The concluding remarks here are,
 I suppose,
 that know what to think about,
 what to do,
 at the personal level,
 has two answers:
 mindless do whatever,
 or focus focus focus.
 And civilizationally,
 this is the case,
 too.
 And some civilizations do – Sarah Paine talks about the Meiji generation – and other civilizations don't – she also talk about MAGA and Trump.
 I'm talking about meaning and purpose at cross all of these,
 as,
 obviously (duhh) this is an AGI issue.
 I cannot solve the nature of the human condition,
 no one can.
 We've had centuries and millennia of literary authors plumbing the depths of the human soul,
 to find ..
 what,
 exactly?
 I'm talking about building symbolic reasoning machines (aka AGI?) that can plumb the depths of the civilizational soul,
 to find,
 what,
 exactly?
 A lot of Nebulosity,
 a la David Chapman?
 Surely an AGI will also stare into its navel and as 
\begin_inset Quotes eld
\end_inset

what is the meaning of life?
\begin_inset Quotes erd
\end_inset

 Or,
 perhaps,
 it won't,
 and instead give us the Simonini letter to Augustin Barruel,
 the Captain Simone Simonini,
 Dala Pikola,
 it will live an existence of perpetrating mundane evil.
\end_layout

\begin_layout Standard
The question of 
\begin_inset Quotes erd
\end_inset

what to think
\begin_inset Quotes erd
\end_inset

 is real and material.
 Insofar as we have free will,
 we can determine and control this to some extent.
 To the degree that our minds,
 brains and thought patterns have been entrained by civilization,
 culture and society (and those evil propaganda–promulgating newspapers) we have no free will,
 and think whatever randomly enters our minds.
 And whence this randomness?
 Here,
 I again descend to daydreams of high–dimensional chaos:
 the sworels emitted behind an airplane jet engine,
 turbulent,
 energetic eddies.
 What is the microscopic theory of those eddies?
 And this question devolves into math first,
 physics second,
 and then the question of being,
 of the being trapped in the here–and–now presentness of the present moment.
 Of being thisness.
 Haeccity.
\end_layout

\begin_layout Standard
And the words I write above?
 I have thought many of them decades ago.
 Why did I not write them down then?
 It seemed unimportant at the time.
 Why do I write them down now?
 Have my thoughts suddenly become important?
 No.
 More like stupidity and too much time on my hands:
 old age:
 I want to write down my life,
 my thoughts,
 what I have worked on,
 what I've done.
 To make an accounting of my life.
 And that is why my writing above feels so stale and putrid,
 old and well–known.
 Well–worn.
 Because it is:
 random neat ideas imbibed from Slashdot many decades ago,
 rehashed and tumbled in my brain.
 But,
 well,
 I have to start somewhere.
 Even if these are childish scribbles,
 scribble I must.
 When scribbling?
 physical forces cause me to press keyboard keys.
 And whence the forces?
 Microscopic movements in my brain.
 And whence those?
 Duns Scotus argued against 
\begin_inset Quotes eld
\end_inset

divine illumination
\begin_inset Quotes erd
\end_inset

,
 the idea that 
\begin_inset Quotes eld
\end_inset

the process of human thought needs to be aided by divine grace.
\begin_inset Quotes erd
\end_inset

 Here I am,
 700 years later;
 have we made progress on this topic?
 Perhaps we replaced 
\begin_inset Quotes eld
\end_inset

divine grace
\begin_inset Quotes erd
\end_inset

 by 
\begin_inset Quotes eld
\end_inset

quantum mechanics
\begin_inset Quotes erd
\end_inset

,
 and yes,
 this is a kind of forward progress of sorts.
 but it falls far short of the mark.
 And I fear,
 of course,
 that even if I succeeded,
 in my wildest dreams beyond dreams,
 to come up with an adequate mathematical,
 physical theory of why we live in the here–and–now,
 and the physical,
 mathematical description of the mechanics of free will,
 and why the future is unknowable,
 even then,
 we would still be staring at the abyss of the meaning crisis,
 which I presume can only be deeper than any hand–waving about large cardinals and outer models.
\end_layout

\begin_layout Standard
Pop goes the weasel.
\end_layout

\begin_layout Subsection*
27 January 2026
\end_layout

\begin_layout Standard
Ok,
 So there's stuff I should be doing,
 and then there's a left–over task from last night,
 which has been percolating in my brain for three decades,
 so may as well get around to it and do it.
 Lets try to solve the chiral bad model eqns with the interior of the bag modeled as a hyperbolic space.
 Claude will do the lifting,
 I will ask questions and copy results.
\end_layout

\begin_layout Paragraph*
Metric
\end_layout

\begin_layout Standard
Poincare ball metric with curvature 
\begin_inset Formula $\kappa=-1/A^{2}$
\end_inset

,
 conformal factor 
\begin_inset Formula $\lambda=2A/\left(1-r^{2}\right)$
\end_inset


\begin_inset Formula 
\[
ds^{2}=\lambda^{2}\left(dx^{2}+dy^{2}+dz^{2}\right)
\]

\end_inset

This is conformal to flat space,
 
\begin_inset Formula 
\[
g_{\mu\nu}=\lambda^{2}\delta_{\mu\nu}=e_{\mu}^{a}e_{\nu}^{b}\delta_{ab}
\]

\end_inset

The dreibein:
\begin_inset Formula 
\[
e_{\;\mu}^{a}=\lambda\,\delta_{\mu}^{a}\qquad e_{a}^{\mu}=\frac{1}{\lambda}\,\delta_{a}^{\mu}
\]

\end_inset

Equivalently
\begin_inset Formula 
\[
e^{a}=\lambda dx^{a}\qquad e_{\mu}=\frac{1}{\lambda}\partial_{\mu}
\]

\end_inset


\end_layout

\begin_layout Paragraph*
Spin connection
\end_layout

\begin_layout Standard
The torsion–free spin connection is 
\begin_inset Formula 
\[
de^{a}+\omega_{\;b}^{a}\wedge e^{b}=0
\]

\end_inset

The anholonomy coefficients aka the Ricci rotation coefficients:
\begin_inset Formula 
\[
\Omega_{ab}^{\;\;c}=e_{a}^{\;\mu}e_{b}^{\;\nu}\left(\partial_{\mu}e_{\;\nu}^{c}-\partial_{\nu}e_{\;\mu}^{c}\right)
\]

\end_inset

The above formula is vibe–coded but it smells right to me.
 It matches what I remember from textbooks.
 The spin connection:
\begin_inset Formula 
\[
\omega_{abc}=\frac{1}{2}\left(\Omega_{abc}-\Omega_{bca}+\Omega_{cab}\right)
\]

\end_inset

and 
\begin_inset Formula 
\[
\omega_{\;\;\mu}^{ab}=\omega_{\;\;c}^{ab}e_{\;\mu}^{c}
\]

\end_inset

Again,
 this looks correct to me.
\end_layout

\begin_layout Paragraph*
Conformally flat spin connection
\end_layout

\begin_layout Standard
For 
\begin_inset Formula $e_{\;\mu}^{a}=\lambda\,\delta_{\mu}^{a}$
\end_inset

 Claude tells me that this works out to
\begin_inset Formula 
\[
\omega_{\;\;\mu}^{ab}=\frac{1}{\lambda}\left(\delta_{\mu}^{a}\partial^{b}\lambda-\delta_{\mu}^{b}\partial^{a}\lambda\right)
\]

\end_inset

or
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\omega^{ab}=\frac{1}{\lambda}\left(dx^{a}\,\partial^{b}\lambda-dx^{b}\,\partial^{a}\lambda\right)
\]

\end_inset

which is probably right but smells bad.
 It's probably right because I think Claude copied it out of a book.
 It smells bad because it is putting tangent–space indexes (the latin letters) on coordinate–space partial derivatives.
 So let me see if I can patch this up by hand.
 Let me try to patch this by writing 
\begin_inset Formula $\partial^{a}=e^{a\mu}\partial_{\mu}$
\end_inset

.
 This is an abuse of notation,
 but I think we can get away with it if we are careful.
 This gives
\begin_inset Formula 
\[
\omega_{\;\;\mu}^{ab}=\frac{1}{\lambda}\left(\delta_{\mu}^{a}e^{b\nu}-\delta_{\mu}^{b}e^{a\nu}\right)\partial_{\nu}\lambda
\]

\end_inset

Then for 
\begin_inset Formula $e^{a\mu}=\delta^{a\mu}/\lambda$
\end_inset

 (another abuse of notation) this becomes
\begin_inset Formula 
\[
\omega_{\;\;\mu}^{ab}=\frac{1}{\lambda^{2}}\left(\delta_{\mu}^{a}\delta^{b\nu}-\delta_{\mu}^{b}\delta^{a\nu}\right)\partial_{\nu}\lambda
\]

\end_inset

The partial derivative gets done in greek indexes,
 i.e.
 in coordinate space,
 for 
\begin_inset Formula $\lambda=2A/\left(1-r^{2}\right)$
\end_inset

.
 This gives
\begin_inset Formula 
\[
\partial_{\mu}\lambda=\frac{4A}{\left(1-r^{2}\right)^{2}}r\partial_{\mu}r=\frac{\lambda^{2}}{A}r\partial_{\mu}r=\frac{\lambda^{2}}{A}x_{\mu}
\]

\end_inset

So
\begin_inset Formula 
\[
\omega_{\;\;\mu}^{ab}=\frac{1}{A}\left(\delta_{\mu}^{a}\delta^{b\nu}-\delta_{\mu}^{b}\delta^{a\nu}\right)x_{\nu}
\]

\end_inset

Drop mu next,
 to get
\begin_inset Formula 
\begin{align*}
\omega^{ab} & =\frac{1}{A}x_{\nu}\left(\delta^{b\nu}dx^{a}-\delta^{a\nu}dx^{b}\right)\\
 & =\frac{\lambda}{A}x_{\nu}\left(e^{b\nu}dx^{a}-e^{a\nu}dx^{b}\right)\\
 & =\frac{\lambda}{A}\left(x^{b}dx^{a}-x^{a}dx^{b}\right)
\end{align*}

\end_inset

with this third line being the abuse of notion.
 This need a big ***caution*** sticker on it:
 its dangerous but OK,
 as long as one consistently abuses the notation.
 So we plow ahead,
 damn the torpedoes.
 Not sure who ever launched a torpedo towards a plow,
 but we're not exactly sailing here,
 either.
 Claude's wordcel trickery has slowed me down.
 That it is not a shape rotator becomes exposed very clearly in exercises like this.
 Alas.
 For me,
 alas.
 I can't trust it.
 Lets plow some more ravines.
\end_layout

\begin_layout Standard
Footnote:
 I can validate that the abuse of notation above still gives the right answer,
 as follows.
 So for 
\begin_inset Formula $g_{\mu\nu}=\lambda^{2}\delta_{\mu\nu}$
\end_inset

 it follows that 
\begin_inset Formula $g^{\mu\nu}=\lambda^{-2}\delta^{\mu\nu}$
\end_inset

 because this gives 
\begin_inset Formula $g_{\mu\nu}g^{\nu\rho}=\delta_{\mu}^{\rho}$
\end_inset

 as desired,
 and then from 
\begin_inset Formula $e_{\;\mu}^{a}=\lambda\,\delta_{\mu}^{a}$
\end_inset

 just raise:
 
\begin_inset Formula $e^{a\mu}=e_{\;\nu}^{a}g^{\nu\mu}=\lambda\cdot\lambda^{-2}\delta^{a\mu}=\delta^{a\mu}/\lambda$
\end_inset

.
 So this awkward–looking expression contracting greek and roman letters is just fine and fully consistent.
 The awkwardness of it all,
 and Claude's glibness,
 was giving me the willies.
\end_layout

\begin_layout Paragraph*
Spinors
\end_layout

\begin_layout Standard
For spinors:
\begin_inset Formula 
\[
\Omega_{\mu}=\frac{1}{4}\omega_{\mu}^{ab}\gamma_{a}\gamma_{b}
\]

\end_inset


\end_layout

\begin_layout Standard
The above dropped the time coordinate,
 but we can just gloss this for now.
 The Dirac eqn (massless) is
\begin_inset Formula 
\[
iD\!\!\!/\,\psi=i\gamma^{a}e_{a}^{\mu}\left(\partial_{\mu}+\Omega_{\mu}\right)\psi=0
\]

\end_inset


\end_layout

\begin_layout Paragraph*
Chiral bag
\end_layout

\begin_layout Standard
Lets sidetrack to the chiral bag.
 The boundary condition is
\begin_inset Formula 
\[
i\gamma^{\mu}n_{\mu}\psi=\exp\left(i\Theta\gamma_{5}\vec{\tau}\cdot\hat{\pi}\right)\psi
\]

\end_inset

The hedgehog is 
\begin_inset Formula $\hat{\pi}=\hat{r}$
\end_inset

.
 We'll also take 
\begin_inset Formula $\hat{n}=\hat{r}$
\end_inset

 viz 
\begin_inset Formula $n_{\mu}=x_{\mu}/r$
\end_inset

 and thus 
\begin_inset Formula $\gamma^{\mu}$
\end_inset

 is proportional to 
\begin_inset Formula $\gamma^{a}$
\end_inset

 and the only question here is,
 do I need to have a factor of 
\begin_inset Formula $\lambda$
\end_inset

 on the left hand side?
\end_layout

\begin_layout Paragraph*
Vierbein in sphere coords
\end_layout

\begin_layout Standard
So eventually,
 we need the Dirac eqn is sphere coords.
 Since Claude is now consistently lying to me,
 we have to start at square one,
 and just do it the hard way.
 
\end_layout

\begin_layout Standard
The vierbein
\begin_inset Formula 
\begin{align*}
e^{0} & =dt\\
e^{1} & =\lambda dr\\
e^{2} & =\lambda rd\theta\\
e^{3} & =\lambda r\sin\theta d\phi
\end{align*}

\end_inset

The inverse
\begin_inset Formula 
\begin{align*}
e_{0} & =\partial_{t}\\
e_{1} & =\frac{1}{\lambda}\partial_{r}\\
e_{2} & =\frac{1}{\lambda r}\partial_{\theta}\\
e_{3} & =\frac{1}{\lambda r\sin\theta}\partial_{\phi}
\end{align*}

\end_inset


\end_layout

\begin_layout Paragraph*
The spin connection
\end_layout

\begin_layout Standard
Let assume the earlier expressions for the spin connection were correct.
 (I wrote the diatribe about Claude,
 further below,
 first,
 and then circled back here to do this by hand.)
\end_layout

\begin_layout Standard
Well,
 lets assume they are not,
 and restart from first principles.
 So
\begin_inset Formula 
\begin{align*}
d\lambda & =d\frac{2A}{1-r^{2}}\\
 & =\frac{2A}{\left(1-r^{2}\right)^{2}}2rdr\\
 & =\frac{\lambda^{2}}{A}rdr
\end{align*}

\end_inset

Right?
 I think that's right.
 So
\begin_inset Formula 
\begin{align*}
de^{0} & =0\\
de^{1} & =0\\
de^{2} & =d\left(\lambda r\right)\wedge d\theta\\
de^{3} & =d\left(\lambda r\sin\theta\right)\wedge d\phi
\end{align*}

\end_inset

So I crank this by hand.
 Which is tedious,
 but so it goes.
 First,
 
\begin_inset Formula 
\begin{align*}
d\left(\lambda r\right) & =rd\lambda+\lambda dr\\
 & =\lambda\left(\frac{\lambda}{A}r^{2}+1\right)dr\\
 & =\lambda\left(\frac{2}{1-r^{2}}r^{2}+\frac{1-r^{2}}{1-r^{2}}\right)dr\\
 & =\lambda\left(\frac{1+r^{2}}{1-r^{2}}\right)dr\\
 & =\left(\frac{1+r^{2}}{1-r^{2}}\right)e^{1}
\end{align*}

\end_inset

so that
\begin_inset Formula 
\[
de^{2}=\frac{1}{\lambda r}\left(\frac{1+r^{2}}{1-r^{2}}\right)e^{1}\wedge e^{2}
\]

\end_inset

The next one is 
\begin_inset Formula 
\begin{align*}
d\left(\lambda r\sin\theta\right) & =\sin\theta\;d\left(\lambda r\right)+\lambda r\cos\theta\;d\theta\\
 & =\lambda\left[\left(\frac{1+r^{2}}{1-r^{2}}\right)\sin\theta\;dr+r\cos\theta\;d\theta\right]\\
 & =\left(\frac{1+r^{2}}{1-r^{2}}\right)\sin\theta\;e^{1}+\cos\theta\;e^{2}
\end{align*}

\end_inset

so
\begin_inset Formula 
\[
de^{3}=\frac{1}{\lambda r}\left[\left(\frac{1+r^{2}}{1-r^{2}}\right)\;e^{1}\wedge e^{3}+\frac{\cos\theta}{\sin\theta}\;e^{2}\wedge e^{3}\right]
\]

\end_inset

Next,
 go back to the torsion–free kinematic equation for the spin connection 
\begin_inset Formula 
\[
de^{a}+\omega_{\;b}^{a}\wedge e^{b}=0
\]

\end_inset

and solve it component by component.
 Is there an easier way?
 Not obviously so.
 Moving through the Ricci components also seems to need a lot of algebra.
 I could use that to double check my results,
 which would be cool,
 if I could trick Claude into not lying to me.
 But tricking Claude into doing the right thing takes about as much time as not using Claude at all...
 Fuck me.
 Whatever.
 Lets blast ahead,
 perhaps stupidly,
 but lets do it anyway.
\end_layout

\begin_layout Standard
So then 
\begin_inset Formula 
\begin{align*}
0 & =de^{2}+\omega_{\;b}^{2}\wedge e^{b}\\
 & =de^{2}+\omega_{\;1}^{2}\wedge e^{1}+\omega_{\;2}^{2}\wedge e^{2}+\omega_{\;3}^{2}\wedge e^{3}\\
 & =\frac{1}{\lambda r}\left(\frac{1+r^{2}}{1-r^{2}}\right)e^{1}\wedge e^{2}+\omega_{\;1}^{2}\wedge e^{1}+\omega_{\;2}^{2}\wedge e^{2}+\omega_{\;3}^{2}\wedge e^{3}
\end{align*}

\end_inset

so conclude
\begin_inset Formula 
\[
\omega_{\;1}^{2}=\frac{1}{\lambda r}\left(\frac{1+r^{2}}{1-r^{2}}\right)e^{2}
\]

\end_inset


\end_layout

\begin_layout Standard
The next one is 
\begin_inset Formula 
\begin{align*}
0 & =de^{3}+\omega_{\;1}^{3}\wedge e^{1}+\omega_{\;2}^{3}\wedge e^{2}+\omega_{\;3}^{3}\wedge e^{3}\\
 & =\frac{1}{\lambda r}\left[\left(\frac{1+r^{2}}{1-r^{2}}\right)\;e^{1}\wedge e^{3}+\frac{\cos\theta}{\sin\theta}\;e^{2}\wedge e^{3}\right]+\omega_{\;1}^{3}\wedge e^{1}+\omega_{\;2}^{3}\wedge e^{2}+\omega_{\;3}^{3}\wedge e^{3}
\end{align*}

\end_inset

which gives
\begin_inset Formula 
\[
\omega_{\;1}^{3}=\frac{1}{\lambda r}\left(\frac{1+r^{2}}{1-r^{2}}\right)e^{3}
\]

\end_inset

and
\begin_inset Formula 
\[
\omega_{\;2}^{3}=\frac{1}{\lambda r}\cdot\frac{\cos\theta}{\sin\theta}e^{3}
\]

\end_inset

 
\end_layout

\begin_layout Standard
Finally 
\begin_inset Formula $0=\omega_{\;b}^{1}\wedge e^{b}$
\end_inset

 means that 
\begin_inset Formula $0=\omega_{\;1}^{1}\wedge e^{1}+\omega_{\;2}^{1}\wedge e^{2}+\omega_{\;3}^{1}\wedge e^{3}$
\end_inset

 but this is trivially satisfied by the above results.
 I think the results in this section are correct.
\end_layout

\begin_layout Paragraph*
Dirac equation
\end_layout

\begin_layout Standard
So lets repeat.
 We have to solve 
\begin_inset Formula 
\[
\gamma^{a}e_{a}^{\mu}\left(\partial_{\mu}+\Omega_{\mu}\right)\psi=0
\]

\end_inset

with 
\begin_inset Formula 
\[
\Omega_{\mu}=\frac{1}{4}\omega_{\mu}^{ab}\gamma_{a}\gamma_{b}
\]

\end_inset

subject to the hedgehog boundary conditions applied at 
\begin_inset Formula $r<1$
\end_inset

.
 We use trickery from here on out.
 I am using Claude to find the tricks.
 I'm not verifying them carefully,
 they smell correct.
\begin_inset Formula 
\[
\left[\gamma_{a},\gamma_{b}\right]=2i\epsilon_{abc}\Sigma^{c}
\]

\end_inset

with
\begin_inset Formula 
\[
\Sigma^{c}=\left(\begin{array}{cc}
\sigma^{c} & 0\\
0 & \sigma^{c}
\end{array}\right)
\]

\end_inset

so
\begin_inset Formula 
\[
\Omega_{\mu}=\frac{i}{4}\epsilon_{abc}\omega_{\mu}^{ab}\Sigma^{c}
\]

\end_inset

Well,
 OK,
 baby steps.
 Next,
 we have to hedgehog Ansatz our way through.
 Once again,
 I think I'll call it quits here,
 for now.
 I've actually asked Claude to take many more steps from here.
 I don't trust it's results,
 so I am not copying them here,
 but so far,
 we're having some amount of fun.
 The vibes are OK,
 so far.
\end_layout

\begin_layout Paragraph*
Claude code
\end_layout

\begin_layout Standard
Err,
 well I will sketch what Claude is telling me.
 It smells OK.
 It might have errors.
 But the overall picture is not insane.
 I gather that someone has published these details,
 and Claude trained on them,
 and is remembering them.
 Here goes.
\end_layout

\begin_layout Standard
One trick is to absorb the spin connection into an angular momentum term.
 I won't write that down.
\end_layout

\begin_layout Standard
Next trick is to write 
\begin_inset Formula 
\[
\psi=\left(\begin{array}{c}
\psi_{+}\\
\psi_{-}
\end{array}\right)
\]

\end_inset

with
\begin_inset Formula 
\begin{align*}
E\psi_{+} & =-i\vec{\sigma}\cdot\vec{D}\psi_{-}+m\psi_{+}\\
E\psi_{-} & =-i\vec{\sigma}\cdot\vec{D}\psi_{+}-m\psi_{-}
\end{align*}

\end_inset

where
\begin_inset Formula 
\begin{align*}
\vec{\sigma}\cdot\vec{D} & =\frac{\vec{\sigma}\cdot\hat{r}}{\lambda}\left(\partial_{r}+\frac{1}{1-r^{2}}+\frac{1+r^{2}}{1-r^{2}}\frac{\left(\vec{\sigma}\cdot\hat{r}\right)\left(\vec{\sigma}\cdot\vec{L}\right)}{r}\right)\\
 & =\frac{\vec{\sigma}\cdot\hat{r}}{\lambda}\left(\partial_{r}-\frac{r}{1-r^{2}}+\frac{1+r^{2}}{1-r^{2}}\left[\frac{1+\left(\vec{\sigma}\cdot\hat{r}\right)\left(\vec{\sigma}\cdot\vec{L}\right)}{r}\right]\right)\\
 & =\frac{\vec{\sigma}\cdot\hat{r}}{2A}\left(\left(1-r^{2}\right)\partial_{r}+1+\left(1+r^{2}\right)\frac{\left(\vec{\sigma}\cdot\hat{r}\right)\left(\vec{\sigma}\cdot\vec{L}\right)}{r}\right)
\end{align*}

\end_inset

and the flat–space version would have been 
\begin_inset Formula 
\[
\vec{\sigma}\cdot\vec{\nabla}=\vec{\sigma}\cdot\hat{r}\left(\partial_{r}+\frac{1+\left(\vec{\sigma}\cdot\hat{r}\right)\left(\vec{\sigma}\cdot\vec{L}\right)}{r}\right)
\]

\end_inset

To regain flat space 
\begin_inset Formula $\kappa=0$
\end_inset

 in the limit,
 set 
\begin_inset Formula $\kappa\to0$
\end_inset

 which means 
\begin_inset Formula $A\to\infty$
\end_inset

 and rescale 
\begin_inset Formula $r=R/2A$
\end_inset

 with 
\begin_inset Formula $R$
\end_inset

 held fixed.
 This means all factors 
\begin_inset Formula $r^{2}\to0$
\end_inset

 and all the rest works out as expected.
 Good.
\end_layout

\begin_layout Standard
The Hedgehog Ansatz is 
\begin_inset Formula 
\[
\psi_{+}=u\chi\qquad\psi_{-}=iv\left(\vec{\sigma}\cdot\hat{r}\right)\chi
\]

\end_inset

with 
\begin_inset Formula $\left(\vec{\sigma}+\vec{\tau}\right)\chi=0$
\end_inset

.
 So 
\begin_inset Formula $\left(\vec{\sigma}\cdot\hat{r}\right)^{2}=1$
\end_inset

 and Claude claims 
\begin_inset Formula $\left(\vec{\sigma}\cdot\vec{L}\right)^{2}\chi=-\chi$
\end_inset

 for the hedgehog.
 Is this right?
 Well,
 maybe,
 I guess.
 Who knows.
 Well,
 not entirely;
 in fact,
 Claude assumed that the 
\begin_inset Quotes eld
\end_inset

grand spin quantum number
\begin_inset Quotes erd
\end_inset

 
\begin_inset Formula $k=0$
\end_inset

 which is OK if all one wants is the ground state,
 but clearly fails for the vacuum states.
 I didn't catch this early enough,
 so the below proceeds with 
\begin_inset Formula $k=0$
\end_inset

.
 That's OK,
 for now.
 We redo it further below.
\end_layout

\begin_layout Standard
Assuming Claude is otherwise correct,
 the radial equations (for grand spin zero) become
\begin_inset Formula 
\begin{align*}
2AEu & =\left(1-r^{2}\right)v^{\prime}-rv\\
2AEv & =-\left(1-r^{2}\right)u^{\prime}-\frac{2+r^{2}}{r}u
\end{align*}

\end_inset

with 
\begin_inset Formula $u^{\prime}=du/dr$
\end_inset

 etc.
 XXX When I forced Claude to track grand spin 
\begin_inset Formula $k\ne0$
\end_inset

 below,
 it begins to appear that the above are not correct.
 Maybe.
 I am once again running out of steam,
 trying to track down the error.
 So the derivation below looks pretty but might be wrong.
 It clearly needs more serious work.
 XXX Caveat Emptor.
\end_layout

\begin_layout Standard
Substituting,
 Claude claims this gives
\begin_inset Formula 
\[
u^{\prime\prime}+\frac{2}{r}u^{\prime}+\frac{\left(2AEr\right)^{2}-\left(2r^{4}-r^{2}+2\right)}{\left(1-r^{2}\right)^{2}r^{2}}u=0
\]

\end_inset


\end_layout

\begin_layout Standard
The geodesic distance from the origin is 
\begin_inset Formula $\rho=\tanh^{-1}r$
\end_inset

.
 Viz,
 
\begin_inset Formula $r$
\end_inset

 was the coordinate chart coordinate,
 but the geodesic distance gets infinite as 
\begin_inset Formula $r\to1$
\end_inset

 which was exactly what makes this model interesting.
\end_layout

\begin_layout Standard
Plugging through gives 
\begin_inset Formula 
\[
u^{\prime\prime}+u^{\prime}\,2\coth\rho+\left(\epsilon^{2}-3-\frac{8}{\sinh^{2}2\rho}\right)u=0
\]

\end_inset

with 
\begin_inset Formula $\epsilon=2AE$
\end_inset

 and prime is now 
\begin_inset Formula $d/d\rho$
\end_inset

.
 Claude claims that this has the form of a 
\begin_inset Quotes eld
\end_inset

Pöschl-Teller potential
\begin_inset Quotes erd
\end_inset

.
 Never heard of that before.
 So it knows about this.
 Someone must have figured this out.
\end_layout

\begin_layout Standard
Next,
 we can solve the above.
 Claude guesses that its hypergeometric,
 and arrives at the exact solution
\begin_inset Formula 
\[
u=\frac{\sqrt{\xi^{2}-1}}{\xi}\;{}_{2}F_{1}\left(a,b;2;1-\xi^{2}\right)
\]

\end_inset

with 
\begin_inset Formula $\xi=\cosh\rho$
\end_inset

 and 
\begin_inset Formula $a=\left(3+\sqrt{1+4\epsilon^{2}}\right)/4$
\end_inset

 and 
\begin_inset Formula $b=\left(3-\sqrt{1+4\epsilon^{2}}\right)/4$
\end_inset

 OK,
 that's tractable.
 In chart coords,
 its 
\begin_inset Formula 
\[
u=r\;{}_{2}F_{1}\left(a,b;2;-\frac{r^{2}}{1-r^{2}}\right)
\]

\end_inset

It seems plausible that this might reduce to sphere bessel functions in the 
\begin_inset Formula $A\to\infty$
\end_inset

 flat space limit.
 It should reduce to 
\begin_inset Formula $j_{0}$
\end_inset

 with 
\begin_inset Formula $v$
\end_inset

 going over to 
\begin_inset Formula $j_{1}$
\end_inset

.
 I'm not doing this now;
 first we have to get the 
\begin_inset Formula $k\ne0$
\end_inset

.
 XXX Well,
 in fact,
 the above might be wrong,
 because the 
\begin_inset Formula $k\ne0$
\end_inset

 efforts below explode into inconsistencies and bad algebra.
 So the results above are suspect,
 too.
 XXX
\end_layout

\begin_layout Standard
Here we go.
 I had to force Claude to redo this several times before getting this,
 which smells better than the earlier version
\begin_inset Formula 
\[
\psi_{+}=u\Phi_{k}^{+}+\tilde{u}\Phi_{k}^{-}\qquad\psi_{-}=iv\Phi_{k}^{-}+i\tilde{v}\Phi_{k}^{+}
\]

\end_inset

for the 
\begin_inset Formula $\Phi_{k}^{\pm}$
\end_inset

 the spin–isospin spherical harmonics with Casimir operator angular momentum 
\begin_inset Formula $K^{2}\Phi_{k}^{\pm}=k\left(k+1\right)\Phi_{k}^{\pm}$
\end_inset

 .
 Then 
\begin_inset Formula 
\[
\left(\vec{\sigma}\cdot\vec{L}\right)\Phi_{k}^{+}=\lambda\Phi_{k}^{+}
\]

\end_inset

and the anticommutator identity
\begin_inset Formula 
\[
\left\{ \left(\vec{\sigma}\cdot\hat{r}\right),\left(\vec{\sigma}\cdot\vec{L}\right)\right\} =-2\left(\vec{\sigma}\cdot\hat{r}\right)
\]

\end_inset

gives 
\begin_inset Formula 
\[
\left(\vec{\sigma}\cdot\vec{L}\right)\left[\left(\vec{\sigma}\cdot\hat{r}\right)\Phi_{k}^{+}\right]=-\left(\lambda+2\right)\left(\vec{\sigma}\cdot\hat{r}\right)\Phi_{k}^{+}
\]

\end_inset

and we seem to be converging on the idea that
\begin_inset Formula 
\[
\Phi_{k}^{-}=\left(\vec{\sigma}\cdot\hat{r}\right)\Phi_{k}^{+}
\]

\end_inset

 which has OK vibes,
 for now.
\end_layout

\begin_layout Standard
This means eigenvalues on the lower component are 
\begin_inset Formula $-\lambda-2$
\end_inset

.
 For the hedge–hog,
 we get 
\begin_inset Formula $\lambda=k$
\end_inset

 or 
\begin_inset Formula $\lambda=-\left(k+1\right)$
\end_inset

 when 
\begin_inset Formula $k\ne0,$
\end_inset

 while for 
\begin_inset Formula $k=0$
\end_inset

 the special case is 
\begin_inset Formula $\lambda=-1$
\end_inset

.
 Plugging through,
 well,
 at this point,
 Claude runs into serious trouble.
 All sorts of algebra starts fucking up.
 It cannot get the 
\begin_inset Formula $k=0$
\end_inset

 answer to agree with earlier results.
 I've used up all my allotted time on this task,
 so must stop.
\end_layout

\begin_layout Standard
Lets pause to look at the boundary conditions.
 Claude claims that 
\begin_inset Formula 
\[
\left.\frac{v}{u}\right|_{r=R}=-\tan\frac{\Theta}{2}
\]

\end_inset

which cranked through is supposed to be
\begin_inset Formula 
\[
u^{\prime}\sin\frac{\Theta}{2}=\left(\epsilon\cos\frac{\Theta}{2}-\frac{2+\tanh^{2}\rho_{R}}{\tanh\rho_{R}}\sin\frac{\Theta}{2}\right)u
\]

\end_inset

which Claude claims is a 
\begin_inset Quotes erd
\end_inset

Robin boundary condition
\begin_inset Quotes erd
\end_inset

.
 Never heard of that but OK.
 The coordinate space version with 
\begin_inset Formula $r$
\end_inset

 will be simpler to work with.
\end_layout

\begin_layout Standard
Lets pause and take stock.
 Lets assume that,
 if/when the algebra above is fixed,
 we get radial functions 
\begin_inset Formula $u\sim{}_{2}F_{1}$
\end_inset

.
 These are nice,
 and fairly easy to work with and compute numerically;
 the boundary condition will be solvable in some milliseconds of CPU time,
 so we can do full vacuum sums in dozens of seconds or maybe minutes.
 And then what?
 We have two parameters to play with.
 Naively,
 it seems to be three:
 
\begin_inset Formula $\Theta$
\end_inset

,
 
\begin_inset Formula $R$
\end_inset

 and 
\begin_inset Formula $A$
\end_inset

 but I think we can maybe redo to hold 
\begin_inset Formula $RA$
\end_inset

 fixed,
 so that it reduces to two paramters.
 Or something like that.
 In the flat–space case it was only one parameter,
 
\begin_inset Formula $\Theta$
\end_inset

 because the radial coord was always multiplied by energy.
 Here,
 we already see that 
\begin_inset Formula $\epsilon=2AE$
\end_inset

 and the eigenvalues will be for 
\begin_inset Formula $\epsilon$
\end_inset

 so...
 whatever.
 Its somehow two parameters.
\end_layout

\begin_layout Paragraph*
Why bother?
\end_layout

\begin_layout Standard
So,
 now come two fun questions:
 Can the energy be made absolutely independent of the bag radius?
 I assume the answer will be yes.
 Doing so will then fix the curvature as a function of bag radius.
 Presumably,
 the curvature will got to 
\begin_inset Formula $\kappa\to-1$
\end_inset

 as the bag radius goes to a femtometer.
 This would be the desired model of confinement and asymptotic freedom.
 Next,
 what about the chiral condensate?
 In flat space,
 we had the sigma model 
\begin_inset Formula $\sigma$
\end_inset

 being exactly zero inside the bag,
 and becoming non–zero only exactly on the boundary.
 So,
 in flat space,
 the bag interior did not look like a sigma model.
 This was a big disappointment.
 However,
 now,
 in curved space,
 could it be otherwise?
\end_layout

\begin_layout Standard
Why do it this way?
 The core problem is that geometry is hard.
 If I could do these calculations 
\begin_inset Quotes eld
\end_inset

in general
\begin_inset Quotes erd
\end_inset

,
 then it seems there is almost surely some Atiyah–Singer magic happening.
 But I can't,
 and even if I could I might be disappointed.
 So,
 as a path–finder,
 do the calculations assuming sphere symmetry,
 and explore this simplified model.
 If this simplified model allows the sigma model to emerge from the vacuum,
 well then happy day.
 If not,
 then hopes are dashed.
 But it would have gotten much farther than would otherwise have been possible in the 
\begin_inset Quotes eld
\end_inset

full general
\begin_inset Quotes erd
\end_inset

 case.
\end_layout

\begin_layout Standard
Suppose the dreams are fulfilled.
 Then the breakthrough is that the AdS boundary condition is re–identified with the bag boundary,
 instead of being some cosmological thing.
 The boundary is also topologically non–trivial,
 so the chiral model now lets us ask about harmonic forms on this combined structure.
 viz a spin manifold having some kind of dynamics inside the bag,
 that spin manifold having some spectrum;
 that spectrum,
 or rather,
 the spectral asymmetry looking like (or I suppose being exactly dual to) a chiral model.
 That would be pretty cool and spiffy,
 if it were true and could be done.
\end_layout

\begin_layout Standard
But now,
 I bop my nose up against some age–old (and old–age) concerns.
 The age–old concern is 
\begin_inset Quotes eld
\end_inset

what is the most important thing I should work on right now?
\begin_inset Quotes erd
\end_inset

 and the old–age concern is 
\begin_inset Quotes eld
\end_inset

shit I'm running out of time,
 so I better get the correct answer to the previous question.
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Standard
The problem with doing geometry is my meta–imagination.
 The geometrical,
 algebraic universe appears to be constructable by specifying a collection of axioms,
 and a collection of inference rules,
 and turning the crank,
 and discovering gazzillions of identities.
 This is how math has always been done,
 traditionally:
 the more abstract and general and useful identities are given a name,
 placing the discoverer into the ranks of history.
 While still alive,
 mathematicians derive pleasure from finding identities,
 stating and proving theorems,
 grasping at hypothesis.
 The mathematical physics community is less rigorous,
 but questions of harmonic forms on Riemann surfaces certainly counts as mathematical physics,
 especially when minor modifications give things like Landau superconductivity or Yang–Mills eqns.
 So its eminently physics,
 in that sense.
 And the academic grind–core,
 this is what they do:
 they have fun cranking on this stuff.
 And in the last 2-4 years,
 they have been learning how the get LLM's to do some of the more boring parts of the calculations.
 Just like I have learned above.
\end_layout

\begin_layout Standard
The personal problem,
 for me,
 is,
 well,
 suppose I do discover that some whizzy chiral condensate happens to be equivalent to,
 dual to some Riemannian manifold?
 That would be pretty cool.
 Would it matter?
 Well,
 I would have had fun,
 once again,
 in the process,
 but I've had lots of fun over my lifetime,
 even if the results of most of it have been utterly unimportant in some local scheme of things.
 I mean,
 how does this work,
 anyway?
 Steven Weinberg wrote an excellent book on gravitation,
 got the Nobel prize in physics for QFT work,
 scowled at me when I approached him at a conference,
 taught lots of students at UTexas for a good salary,
 and then he died.
 Whoopla!
 Hot patootie,
 bless my soul,
 I really love that rock–n–roll.
\end_layout

\begin_layout Standard
Anyway,
 the upshot is that I can get paid for doing AI research;
 I cannot get paid for doing mathematical physics.
 This is my current economic environment.
 (This section above was written after the section below.
 Its placed out of line to conetextualize better with the second attempt to solve the eqns above.)
\end_layout

\begin_layout Paragraph*
Post–script
\end_layout

\begin_layout Standard
Next day.
 I slept on this.
 It won't work.
 Here's the problem:
 If I take the vacuum fermions to be equi–distributed (and they will be) then they will uniformly fill all of space.
 And,
 in this hyperbolic gig,
 almost all of the space is not at the center,
 but in the 
\begin_inset Formula $x\to1$
\end_inset

 region.
 So in the 
\begin_inset Quotes eld
\end_inset

exterior
\begin_inset Quotes erd
\end_inset

 
\begin_inset Formula $x$
\end_inset

 coordinate,
 it will look like the baryon number accumulates in a thin shell near the bag boundary.
 This gives a hollow baryon.
 Well,
 that would be interesting,
 I guess,
 but maybe too far away from any expected nuclear reality.
 I was imagining asymptotic freedom,
 by placing the bag boundary 
\begin_inset Quotes eld
\end_inset

very far away
\begin_inset Quotes erd
\end_inset

,
 metrically speaking.
 But that same metric implies that almost all the volume is in those very far away regions.
 Which is not what I want.
\end_layout

\begin_layout Standard
I want the opposite.
 So I guess that means the space derivative would have to be the flat–space derivative,
 and instead stick some term that ....
 well,
 it doesn't work out.
 So this was a big whoopin dead–end.
 If life gives you lemons,
 call them screw–ball escapades.
\end_layout

\begin_layout Standard
Well,
 I can tell it's not over.
 This didn't pan out,
 but burbling in the back of my mind is the idea that some other screwball twist will work....
\end_layout

\begin_layout Paragraph*
Punt
\end_layout

\begin_layout Standard
Lets assume the earlier expressions for the spin connection were correct.
\end_layout

\begin_layout Standard
Punt.
 I already wasted a few hours on this.
 It's solvable,
 but turning the crank and double–checking will take a few more hours.
\end_layout

\begin_layout Standard
And that's for the simple case,
 (the radial–symmetric case) and not for the case that would really be interesting (which would be to allow local curvature to have some kinematic freedom,
 e.g.
 back–reacting in such as way as to have (vacuum) energy distribute uniformly.
 Or then more broadly to explore how much the chiral bag boundary condition can be made to look like AdS horizons.
 Or,
 in pop–sci–reporter speak,
 that 
\begin_inset Quotes eld
\end_inset

quark confinement can be modeled by placing the bag boundary infinitely far away,
 from the quarks point of view
\begin_inset Quotes erd
\end_inset

.
 Which I think is a cool idea,
 and I'd like to see the answer,
 I'd like to see how the algebra for this works out.
\end_layout

\begin_layout Standard
And I could do it.
 How long would it take?
 Well,
 turns out cranking through mathematical algebra is a lot like writing software.
 Once you understand what needs to be done,
 you say 
\begin_inset Quotes eld
\end_inset

oh a few hours
\begin_inset Quotes erd
\end_inset

.
 Plus some time for debugging.
 Software debugging is easy,
 compared to algebra debugging:
 you might have made a mistake somewhere,
 but where?
 Or maybe you didn't,
 so how can you check?
\end_layout

\begin_layout Standard
So I could do the spin connection in maybe a few hours.
 Maybe a few hours to double–check that the formulas that Claude quoted are actually correct.
 Then at least a few hours to remember how to do the hedgehog Ansatz,
 in this new setting.
 Then a day to daydream and scramble about and explore related ideas.
 Then a few days to solve the eigenvalue eqn,
 assuming,
 that is,
 that it is easily solvable;
 it might not be.
 A few days to sum the vacuum energy.
 A few days to rethink how this exerts a kinematic force on the curvature.
 A few days to daydream whether this should be turned into a full 3D dynamical model,
 instead of a 1D radial slice.
 And so now weeks or months have gone by on what was intended to be an afternoon project.
\end_layout

\begin_layout Standard
Where was Claude in all of this?
 I was imagining some super–intelligence,
 and I just ask it some questions,
 and get some answers.
 This conception worked stunning great,
 when I was asking about electrical generation and transmission around the globe.
 Claude knew all the basic ideas,
 and fetched data from specific web searches,
 as I interrogated.
 It went well,
 it went happily.
 So I vaguely thought that doing some geometry would go equally swimmingly;
 I'd get some answers that have been bugging me for decades.
\end_layout

\begin_layout Standard
Hah.
 Once again,
 Claude is revealed to be stupid as a rock.
 It has no fucking clue what it is talking about.
 Yes,
 it trained on some mathematical texts containing latex.
 Yes,
 it has committed them to memory.
 If you ask it a commonplace question,
 it knows what the answer should be,
 because it has memorized the answer after seeing it in a dozen textbooks.
 Ask it to check its work?
 And endless spew of excuses:
 
\begin_inset Quotes eld
\end_inset

you are right,
 I forgot to divide by r.
 Let me try again and be more careful.
\begin_inset Quotes erd
\end_inset

 OMG.
 This is exactly like using Claude to write software.
 But in that case,
 when it writes something that doesn't compile,
 I can ask it to use the compiler,
 and try again till it gets it right.
 Here,
 I'm the compiler:
 I have to double–check the formulas that it is claiming to solve.
 And fix all the errors.
 And OMG is it making lots of errors.
 So,
 in this sense,
 I could have made more progress,
 faster,
 without using Claude.
 What's the effective flow?
 To use it as a hinting tool.
 I do all the calculations,
 all the algebra,
 I get it to reformulate,
 using a different path,
 and see if things line up the way they should.
 So I can use Claude as a kind–of computer algebra system,
 and perhaps some super–tedious algebra does go faster,
 and perhaps I can trust it some of the time for some of the results.
 Maybe.
 Do I have time for this?
 Not right now.
 Maybe someday,
 when I need a vacation,
 I can get back to this.
\end_layout

\begin_layout Standard
But for now,
 my questions remain unanswered,
 and Claude is revealed,
 yet again,
 to be an LLM.
 Ask it about something that is well known and widely understood?
 You get a great answer.
 Ask it to write javascript for your web site?
 Instant and perfect.
 Why?
 Because it trained on a million web pages,
 tens of thousands of javascript snippets.
 It can interpolate those in an instantaneous and brilliant fashion.
 Ask it to program in an obscure language (like Atomese) or use some obscure API?
 It stumbles.
 Ask it to do geometry?
 Utter and complete failure.
 Its a wordcel.
 It's not a shape rotator.
 And that's where we are,
 for today.
 The End.
\end_layout

\begin_layout Standard
Or not.
 It's not that bad.
 I gave in to the vibe,
 and pushing further does not look all that bad.
 The formulas look OK–ish.
 I am copying them into place above,
 without really checking.
 I am guessing that this is previously published material and Claude is regurgitating it correctly.
 I feel like this guy and this girl says 
\begin_inset Quotes eld
\end_inset

this will feel good
\begin_inset Quotes erd
\end_inset

 and she goes down on me,
 and what's there to complain about?
 So I'll share some consensual hallucinations with Claude for just a bit longer,
 and see if it's OK.
 So scroll back up for the results.
\end_layout

\begin_layout Subsection*
29 January 2026
\end_layout

\begin_layout Standard
OK,
 back to the drawing board.
 Here I will belly–ache about AI development status,
 and try to see if there's some better way.
 There are multiple topics I've been putting off for months.
 Lets bullet list them.
 Lets not bullet list them,
 I can't remember them.
 They'll come back naturally as I write...
\end_layout

\begin_layout Standard
I want to restart rewriting the learning pipeline.
 So I unleashed Claude and it was a total failure.
 Because Claude is stupid.
 So what I envisioned,
 hoped for,
 wanted was some autonomous file–system crawler,
 that would crawl and take notes,
 build a world–model of what it found,
 represented in a way that makes it easier to query than the brute–force of actually crawling the file system to find what is there.
 That is,
 the world–model is held in the AtomSpace in some 
\begin_inset Quotes eld
\end_inset

natural
\begin_inset Quotes erd
\end_inset

 representation that is easy to query in some ad hoc manner.
\end_layout

\begin_layout Standard
There are two bootstrapping issues here.
 One is:
 what is the 
\begin_inset Quotes eld
\end_inset

natural representation
\begin_inset Quotes erd
\end_inset

?
 Of course,
 I can hand–design the initial one,
 but it would then need to be able to accrete additional information when it becomes available.
 So then the next question is 
\begin_inset Quotes eld
\end_inset

what is that additional information
\begin_inset Quotes erd
\end_inset

?
 and its meant to be pair–counting results,
 and then the marginal probabilities.
 So I can hand–engineer all that.
 I'd done it two or three times before,
 so this is yet another implementation.
 At any rate,
 I'm the one designi9ng where things will get stored,
 and when I asked Claude to do it,
 I got a hash,
 so I have to do it.
\end_layout

\begin_layout Standard
The second bootstrapping issue is that I have to explicitly specify the file–walk algo;
 depth–first or breadth–first or whatever.
 Again,
 this does not happen 
\begin_inset Quotes eld
\end_inset

naturally
\begin_inset Quotes erd
\end_inset

,
 but is engineered into place.
\end_layout

\begin_layout Standard
So let me run with that idea.
 Can I define a set of axioms and inference rules,
 such that,
 when combined,
 a file–walking algo results?
 That is,
 what is the axiomatic system in which tree–walking algos are embedded?
 And,
 given this axiomatic system,
 what can I do to make it self–assemble,
 like the abelian sand–pile?
 i.e.
 to get to a critical point,
 where the behavior patterns at that critical point are that the avalanches are tree–walks?
\end_layout

\begin_layout Standard
To answer that,
 I dig back to L–systems,
 and the work that Prusinkiewicz did with them:
 some simple grammatical rules,
 and you crank them,
 and tree structures result.
 Now I take them and the trees that grow are forced to conform to whatever the actual file–system tree is.
\end_layout

\begin_layout Standard
So the new thing here for me is that with an L–system,
 it is incorrect to think of a single algo,
 walking a tree.
 Instead,
 one has creates a new agent:
 one for each directory and file,
 and that agent communicates what it finds with it's neighboring agents.
 Now,
 I can force this,
 project this back down to a single traditional comp–sci algo that is depth–first or breadth–first,
 but that is a projection (in the sense of a fibre bundle projection) If instead,
 I am L–system inspired,
 the fibre of my fibre–bundle is the L–system.
 Now,
 the agent at each directory and file is very nearly almost totally trivial:
 the only thing it can do is to communicate with neighbors.
 And maybe not even that:
 it can just statically denotes connections (and I statically represent these in the AtomSpace).
 The one non–trivial agentic behavior is that when an agent determines 
\begin_inset Quotes eld
\end_inset

oh hey I am a directory
\begin_inset Quotes erd
\end_inset

 then the grammatical production rules say 
\begin_inset Quotes eld
\end_inset

grow a new agent,
 one for each inode in the directory
\begin_inset Quotes erd
\end_inset

.
\end_layout

\begin_layout Standard
At this meta–level,
 at this L–system level,
 each new agent gets enqueued onto some run–time queue,
 so that the CPU goes over that queue,
 and alots some CPU time to each agent.
 Since agents that have already explored all of their inodes don't really need any more CPU time,
 they become 
\begin_inset Quotes eld
\end_inset

dead
\begin_inset Quotes erd
\end_inset

 (or dormant) and can be dequeued,
 I guess.
 This search algo is naturally 
\begin_inset Quotes eld
\end_inset

parallelizable
\begin_inset Quotes erd
\end_inset

,
 because L–systems are naturally parallel.
 And,
 as I write this,
 I realize that I could also do this for the query engine.
\end_layout

\begin_layout Standard
Let me explore the query–engine idea for a moment.
 Every for–loop is parallelizable in this way:
 it enqueues a job to explore every item in the loop.
 Sounds sexy,
 the reality is data locality:
 the items being walked in the loop have to be fetched from RAM,
 or,
 if its running on a GPU,
 it has to be copied across some bus to that GPU.
 Copying is automatic,
 for CPU's:
 data is automatically copied from RAM to 3rd,
 second then 1st level cache,
 then registers,
 and I don't have to think about that data movement problem,
 because the good folks at Intel/AMD have already solved it for me.
 For GPU's,
 it is much more hands–on,
 because the GPU sits on the far side of the PCIe bus,
 and I have to think about how to convert the body of my for–loop into a vector that I can efficiently pass over to the GPU side.
 So this is hard and tricky.
 Thinking about L–systems and agents is inspirational,
 but hits a wall when I say 
\begin_inset Quotes eld
\end_inset

oh gee,
 I want each agent to run on a GPU,
 but my agents are so trivial that they do almost nothing
\begin_inset Quotes erd
\end_inset

,
 and that almost the only thing that they do is to vectorize and enqueue the next set of agents to be enqueued on the runtime queue.
 So this is ...
 abstractly interesting but profoundly mis–matched to what hardware can actually do.
\end_layout

\begin_layout Standard
Hmm.
 Anyway.
 it is beautiful outside,
 so I am going for a bike ride now.
 Later.
\end_layout

\begin_layout Subsection*
29 January 2026 Later
\end_layout

\begin_layout Standard
Well,
 there is a way of queuing agents in Atomese:
 just say (CollectionOf (Type 'ParallelLink) (FooLink ...)) where FooLink returns a list of agents.
 Bingo:
 L–systems in Atomese are about this trivial.
 So first,
 two obvious caveats,
 and a meta–issue.
\end_layout

\begin_layout Standard
The caveat is that the ParallelLink creates (Linux) OS threads (via c++ std::thread),
 so this is heavyweight,
 and only 
\begin_inset Quotes eld
\end_inset

make sense
\begin_inset Quotes erd
\end_inset

 for complex,
 long–running agents.
 Now,
 of course one could try to optimize:
 perhaps by designing a GreenParallelLink that would use green threads.
 But this founders:
 when do you use which?
 If the agents are tiny,
 then running them in parallel is wild overkill;
 just run them serially.
 There's also a different way of thinking about this:
 inventing the GPUParallelLink,
 which would dispatch the agents to GPU's,
 one each.
 This then struggles,
 because each agent needs to access certain unknown data in the AtomSpace,
 and there's no way for the GPU to 
\begin_inset Quotes eld
\end_inset

reach back
\begin_inset Quotes erd
\end_inset

 into the AtomSpace to get it.
 Perhaps with cleverness,
 this could be engineered around,
 but in the end,
 misses the point.
 And the point is ...
\end_layout

\begin_layout Standard
The point is that ParallelLink is a declarative element.
 At this time,
 it is tightly coupled to a specific C++ implementation,
 but abstractly speaking,
 it should not have been.
 The actual implementation,
 
\begin_inset Quotes eld
\end_inset

under the covers
\begin_inset Quotes erd
\end_inset

 or in some 
\begin_inset Quotes eld
\end_inset

execution context
\begin_inset Quotes erd
\end_inset

 could be Linux pthreads or greenthread or GPU threads,
 or maybe just serial.
 This 
\begin_inset Quotes eld
\end_inset

execution context
\begin_inset Quotes erd
\end_inset

 would be a property of ...
 of what?
\end_layout

\begin_layout Standard
Well,
 the Atom::execute() method has the signature Atom::execute(AtomSpace* scratch) with the pointer pointing at the scratch space inside of which the execution happens.
 So we could create a custom AtomSpace that replaces execution methods by other algos that run,
 e.g.
 on GPU's,
 or green threads or whatever.
 So this is...
 well,
 at least partly solvable.
 Interesting.
 This is not impractical,
 either,
 it's a relatively easy project,
 even,
 to implement this.
 Huh.
 Good.
\end_layout

\begin_layout Standard
Break for dinner.
 The meta issue is ...
\end_layout

\begin_layout Standard
There are two meta–issues.
 Or maybe the same meta–issue?
 Hmm.
 So one idea is that there exists a 
\begin_inset Quotes eld
\end_inset

natural transform
\begin_inset Quotes erd
\end_inset

 between doing loop contents serially,
 and doing it in parallel.
 
\begin_inset Quotes eld
\end_inset

Natural transform
\begin_inset Quotes erd
\end_inset

 in some vague category–theoretic sense.
 For Atomese,
 this would be the transformation between (Trigger (FooLink ...)) and (Trigger (CollectionOf (Type 'ParallelLink) (FooLink ...))) Both do 
\begin_inset Quotes eld
\end_inset

the same thing
\begin_inset Quotes erd
\end_inset

;
 the first one does it serially,
 the second in parallel.
 So if I have a graph–rewriting system,
 I can rewrite the first into the second,
 or the second into the first.
 So the meta issue is this:
 
\begin_inset Quotes eld
\end_inset

What is the current set of rewrite rules that I am willing to apply to a processing pipeline?
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

Should these rewrite rules create pressure for more parallelism,
 or less?
\begin_inset Quotes erd
\end_inset

 And there's even a regulatory component:
 
\begin_inset Quotes eld
\end_inset

Apply rewrite rules to pressure the system to run on the GPU's,
 unless the GPU's are congested,
 or the PCIe bus is congested,
 in which case,
 run things locally on the CPU.
\begin_inset Quotes erd
\end_inset

 Then the complexity spirals out of control:
 
\begin_inset Quotes eld
\end_inset

to know if the PCIe bus is congested,
 I need a sensor to measure traffic and congestion in real time
\begin_inset Quotes erd
\end_inset

.
\end_layout

\begin_layout Standard
And then it is recursive:
 
\begin_inset Quotes eld
\end_inset

if I have sensors that can measure different aspects of system performance,
 then I can write some optimization algo that uses those sensor inputs.
\begin_inset Quotes erd
\end_inset

 Right?
 But what is the algo?
 Maybe there are several algos:
 
\begin_inset Quotes eld
\end_inset

Performance
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

PowerSave
\begin_inset Quotes erd
\end_inset

.
 What are the component pieces–parts of such an algo,
 and what are all the different ways that I can assemble and re–assemble those pieces–parts?
 Well,
 the pieces–parts are jigsaws,
 and the sensory pipeline is 
\begin_inset Quotes eld
\end_inset

PCI congestion sensor -> X
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

if X then Y
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

Y -> execute locally
\begin_inset Quotes erd
\end_inset

 but in between X and Y I can insert other kinds of control blocks:
 
\begin_inset Quotes eld
\end_inset

If X>250 and X<500 then Y
\begin_inset Quotes erd
\end_inset

 or it there is another sensor Z for memory pressure,
 then 
\begin_inset Quotes eld
\end_inset

if X>500 and Z<90 then Y
\begin_inset Quotes erd
\end_inset

.
 So my toolkit is a box of Lego MindStorms and I can snap and assemble them any way I want,
 as long as some basic grammatical rules are obeyed.
 Writing down the grammatical rules is ...
 well,
 
\begin_inset Quotes eld
\end_inset

easy
\begin_inset Quotes erd
\end_inset

 or 
\begin_inset Quotes eld
\end_inset

easy–ish
\begin_inset Quotes erd
\end_inset

 or 
\begin_inset Quotes eld
\end_inset

a well defined problem that we know how to solve
\begin_inset Quotes erd
\end_inset

.
 But the meta–issue is 
\begin_inset Quotes eld
\end_inset

what sort of process control system should I build?
\begin_inset Quotes erd
\end_inset

 Should it be simple?
 Complicated?
 where is the trade–off between simple and complicated?
\end_layout

\begin_layout Standard
In the sand–pile model,
 I have gravity,
 which is the force causing avalanches,
 and finite slip,
 friction coefficients (the sand grains are not infinitely sticky) so the avalanches must necessarily occur.
 Even if the sand–grains are replaced by thistle,
 there is only so high you can go with a tower of sticky thistle before it falls over.
 How do I port over this conception to algorithm–building?
 When does an algorithm become 
\begin_inset Quotes eld
\end_inset

too complex
\begin_inset Quotes erd
\end_inset

,
 and collapse?
 What drives that collapse?
 What is analogous to gravity?
\end_layout

\begin_layout Standard
Related is this:
 
\begin_inset Quotes eld
\end_inset

what is the algorithm that finds (optimal) algorithms?
\begin_inset Quotes erd
\end_inset

 Of course,
 if I did not care about 
\begin_inset Quotes eld
\end_inset

optimal
\begin_inset Quotes erd
\end_inset

,
 then I could build an odometer that simply listed all of them.
 But if I want them to be 
\begin_inset Quotes eld
\end_inset

optimal
\begin_inset Quotes erd
\end_inset

,
 I have multiple issues:
 
\begin_inset Quotes eld
\end_inset

what do I mean by optimal?
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

what are the different ways I can measure optimality (space–usage,
 run–time,
 traffic congestion...)?
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

how do I limit the search space to avoid search regions that are unlikely to yield optimal algorithms?
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Standard
So there are several solutions to the algorithm–discovering algorithm.
 Aside from employing a bunch of students or engineers,
 one can also ask an LLM to craft algos for you.
 The primary issue here is that you are human–in–the–loop.
 The LLM's can write OK javascript or C++ code if you ride herd and are precise in what you want.
 If you are not precise,
 you get crap.
 Asking the LLM to write Atomese is ...
 well,
 it can be done,
 but its quite low–level work,
 the LLM's sort–of understand Atomese,
 but not all that well.
\end_layout

\begin_layout Standard
Let me go off on a tangent here,
 that I've wanted to tangent on for months.
 I am asking the LLM to write the new counting pipeline in Atomese.
 But why?
 Surely it would be more efficient to write it in python or c++ or scheme or ...
 ah,
 well,
 but there's the answer;
 I think I wrote about this in the diary only a week ago.
 Right?
 The previous version of the counting code 
\begin_inset Quotes eld
\end_inset

worked great
\begin_inset Quotes erd
\end_inset

.
 It used the 
\begin_inset Quotes eld
\end_inset

matrix
\begin_inset Quotes erd
\end_inset

 code for maintaining counts and computing marginals,
 and it was batch oriented and failed to be flexible in all the ways I want it to be flexible.
 In some imaginary universe,
 I could work with Claude to refactor that code,
 so that is become flexible in all the ways I want it to be flexible,
 but this parallel universe is imaginary:
 I know from hard experience that either Claude turns it into a complete hash of spaghetti code,
 or that the code base stays clean,
 but I put in lots of hard effort and engineering work to make sure its good.
\end_layout

\begin_layout Standard
And if I'm going to have to put in lots of hard work,
 lots of engineering,
 well,
 it becomes clear that I do not want to use c++ or python or scheme to write that code...
 I want to use Atomese.
 But why?
 
\begin_inset Quotes eld
\end_inset

It becomes clear
\begin_inset Quotes erd
\end_inset

,
 I say,
 but really?
 Does it?
 Why not write in some other language?
\end_layout

\begin_layout Standard
I don't know if I can give any easy answer.
 I like the self–reflective and recursive properties of Atomese.
 That is,
 its a graph,
 and if I want to redo that graph,
 I apply a collection of rewrite rules.
 But if instead it was javascript,
 I could also ask an LLM to rewrite it for me.
 The LLM becomes the 
\begin_inset Quotes eld
\end_inset

graph rewriting system
\begin_inset Quotes erd
\end_inset

.
 The LLM is of course sloppy,
 but is it sloppier than what my imagined system of rewrite rules could generate?
 Is there some intractable complexity limit?
 Well,
 naively 
\begin_inset Quotes eld
\end_inset

everything becomes spaghetti code
\begin_inset Quotes erd
\end_inset

.
 But the standard counterpoint is that 
\begin_inset Quotes eld
\end_inset

good design practice,
 such as modular code and API's and unit tests
\begin_inset Quotes erd
\end_inset

 etc.
 allow you to scale to arbitrary complexities.
 And it would seem that those standard engineering design rules would need to apply to Atomese as well.
\end_layout

\begin_layout Standard
What tools do I have?
 Well,
 judicial use of DefineLink and PipeLink and NameNode allows me to assign names to modular components.
 I do not have any framework for unit tests at this level.
 I do not have a good theory of refactoring at this level.
 I know I want to have rewrite rules,
 but what are they?
 How do they work in practice?
\end_layout

\begin_layout Standard
For example,
 the rewrite rule that converts between (Trigger (FooLink ...)) and (Trigger (CollectionOf (Type 'ParallelLink) (FooLink ...))) – where is this rewrite rule?
 Again,
 if I have a collection of them ...where?
 How?
 Is there a FilterLink that applies this rewrite to a stream?
 The order of rule applications is important.
 Do I have to rip some pages out of theorem–proving systems (Agda,
 HOL,
 etc.) to create a proper rule management system?
\end_layout

\begin_layout Standard
Argh.
 I go in circles.
 So then I say to myself 
\begin_inset Quotes eld
\end_inset

fuck it.
 Just write some code ..
 something or anything ...
 try to make it work.
\begin_inset Quotes erd
\end_inset

 And then my code scales out of control and I am back to theorizing again.
 Except now five years have slipped by.
\end_layout

\begin_layout Standard
The meta question is 
\begin_inset Quotes eld
\end_inset

what should I build?
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

How should I build it?
\begin_inset Quotes erd
\end_inset

.
 The very first part involves this idea that 
\begin_inset Quotes eld
\end_inset

I want to perceive file systems
\begin_inset Quotes erd
\end_inset

,
 but the proximal perception code,
 of pair–counting,
 does not yet exist.
 Worse,
 if it existed but was tuned for texts,
 it could not perceive file systems ...
 or perhaps more directly,
 it could not perceive the 
\begin_inset Quotes eld
\end_inset

AtomSpace itself
\begin_inset Quotes erd
\end_inset

.
\end_layout

\begin_layout Standard
I sort of tripped over this last idea,
 working on the AtomSpace visualizer.
 I have a large dataset.
 I have kind of forgotten 
\begin_inset Quotes eld
\end_inset

whats in there
\begin_inset Quotes erd
\end_inset

,
 and I wanted to get a quick reminder of what it was.
 Well,
 there are two ways.
 One way is to build something into the visualizer,
 provide a whole bunch of buttons and visualizer do–hickies,
 and click on them,
 and get some general idea of what's in there.
 The other way is to start a CogServerNode,
 and ask Claude to connect to the CogServer,
 and verbally inquire 
\begin_inset Quotes eld
\end_inset

what's in here?
 and 
\begin_inset Quotes eld
\end_inset

how is it structured?
 and try to get OK answers via natural language query.
 Which is kind of simpler than building the visualizer,
 which is ...
 hard.
 Hmm.
 I have not tried to have the MCP–examination of this dataset.
 Maybe I should try that,
 and see what happens,
 see if I can get meaningful enlightenment that way ...
\end_layout

\begin_layout Subsection*
30 January 2026
\end_layout

\begin_layout Standard
Having long conversations on discord.
 Won't record all of it,
 but a few comments that I really liked:
\end_layout

\begin_layout Itemize
Blah blah paradox/contradiction...
 that is how you build oscillators ..
 take a digital NOT gate and wire input to output,
 and it will oscillate as fast as the transistors can switch.
 So maybe the origin of time is due to unresolvable logical paradoxes.
 (That is what got typed into the chat channel,
 but in the back of my mind is 
\begin_inset Quotes eld
\end_inset

here and now
\begin_inset Quotes erd
\end_inset

,
 and I'm like 
\begin_inset Quotes eld
\end_inset

oh,
 wait a second ..
 maybe that really is,
 quite literally,
 where time comes from,
 and why we are trapped in the present,
 and why the future is unknown – because the wiring diagram is not simple like a TTL oscillator,
 but complicated and fluid and self–assembling.
 So,
 yes,
 ZFC or New Foundations or Girard's paradox,
 but its cranking,
 oscillating at the speed of ...
 time.)
\end_layout

\begin_layout Itemize
Everyone struggles to communicate.
 Not everyone is aware that they struggle.
 This is,
 in a sense,
 the core AGI issue.
 "What should I say?
 Why should I say it?
 How should I say it?
 what do I want to express?
 What do I want the other person to understand?" The LLM's are less than that.
 They communicate clearly and directly...
 and have not the faintest idea of what they are saying.
 What I want is a way of working with ideas.
 Then maybe I can use an LLM to convert those ideas into words.
 (So above was typed.
 What I'm thinking is that,
 yes,
 I want to encode abstract axiomatic knowledge in Atomese,
 and then use the LLM to verbalize it.
\end_layout

\begin_layout Itemize
The AtomSpace is a reliable mapping from token–to–complex–structure,
 and also back in the other direction,
 complex–structure–to–token.
 (So here,
 I am thinking the 
\begin_inset Quotes eld
\end_inset

tokens
\begin_inset Quotes erd
\end_inset

 are PredicateNodes,
 or similar,
 so the token–to–structure mapping is 
\begin_inset Quotes eld
\end_inset

easy
\begin_inset Quotes erd
\end_inset

(???) and the inverse mapping is the query.
 But the inverse mapping could also be cosine similarity,
 maybe.
 But the token–to–structure mapping might be the incoming set of that token.
 Hmm.
 Clearly,
 this needs work.)
\end_layout

\begin_layout Itemize
I can use an LLM to mine technical text (scientific papers,
 wikipedia) for factual expressions,
 convert those expressions into prolog or just EdgeLinks,
 and I can count those.
 These form vectors,
 whenever an atom appears in in an expression.
 So I can take distributions
\end_layout

\begin_layout Itemize
When Jeff Bezos started amazon.com,
 the idea of "a store where you go to buy things" was a thousand years old.
 And the idea of selling on the net was the foundation of dot–com – everyone understood.
 What Bezos had figured out was monopoly power:
 the efficiency of centralized structure.
 It wasn't going to be 1000 small dot–com store–fronts.
 All the decent people are rebels,
 saying "I don't like centralized control" and that's valid – but the core confusion is not that "centralization is bad",
 its that "assholes who run centralized systems are assholes".
 And so really,
 decent is all about "no one gets to be asshole in charge because we are going to p2p everything between ourselves." The problem with decent is that it promotes a different kind of deviant perverted behavior:
 e.g.
 bitcoin mining.
\end_layout

\begin_layout Itemize
Sure.
 But the Q in SQL stands for "query".
 and LLM's ask me clarifying questions often enough.
 And I also know how to make very simple systems where queries emerge automatically.
 One of them is "here is a sentence.
 Find all questions for which this sentence is an answer" and then,
 "pick the top question,
 and find what other answers it has.
 Pick the top answer and print it to the screen.
 " This is how the old SRAI chatbots work.
 And roughly speaking,
 its also how LLM's work.
 The SRAI mapping was very literal.
 If you say "I like sports",
 the chatbot would generate "I like *" and "I * sports" and "* like sports" and then look over everything * could be,
 and respond,
 maybe "I love sports" or "I like hockey" where * is the wild–card for the query.
 That was literally the SRAI chatbot algo.
 LLM's do this too,
 but the wild–card is not one or two words,
 but a "context window" of tens of thousands of tokens.
 Logic is the difference between wordcels and shape rotators.
 The wordcels,
 the chatbots,
 they can just work with context windows – maybe a few words in size for the old ones back in the day,
 to tens–of–thousands today (big enough to have "skills" in the context window) but that's all they are – context–window-matching wordcels.
 To get to shape rotators,
 you need to have contexts that are not just word–to–word relations (weight matrices) but abstraction–to–abstraction relations.
 (symbol to symbol relations?
 where you can "easily" look up symbols?) So that's what I work on,
 roughly.
 Mostly.
 The result might be a system that "understands things better",
 but then after that,
 my imagination runs out.
 Is it actually better,
 or just some stupid computer trick?
 
\end_layout

\begin_layout Standard
So riffing on the third and fourth bullets,
 I need to build an LLM bridge that converts text to relationship expressions.
 Do the simple stats,
 and see what happens...
\end_layout

\begin_layout Standard
Hmm.
 I can guess what happens,
 so this is kind of a 
\begin_inset Quotes eld
\end_inset

so what
\begin_inset Quotes erd
\end_inset

 exercise,
 because I want to scale.
 But also,
 I have to start somewhere.
\end_layout

\begin_layout Standard
So here's another idea:
 build these vectors these sparse symbolic vectors,
 then compress them (hash them down to 256 bits or 1024 bits) and use than embedding to ...
 what?
 Provide training data to some DL–NN network.
 That is,
 a sequence of 
\begin_inset Quotes eld
\end_inset

abstract concepts
\begin_inset Quotes erd
\end_inset

 in a text is still a sequence.
 So I can still train on that sequence.
 The bad news is that,
 in some sense,
 inserting this abstraction layer in the middle is likely to lead to worse results than using the word–tokens directly.
 That seems clear:
 the attempt to abstract out necessarily entails a loss of (local) fidelity.
 Nor does it obviously open new vistas.
\end_layout

\begin_layout Standard
So viewing a text as a time–series of ideas is incorrect.
 Instead,
 I want to return to the older conception of pair–wise relations between ideas,
 create the network there.
 It's always inescapable,
 I always come back to this point.
 And then I struggle cause its so hard to scale.
 Is that fair?
 unfair?
 I'm competing against a gazzillion gigaflop beast running on the cloud.
 Fuck me.
 
\begin_inset Quotes eld
\end_inset

Shut up and calculate
\begin_inset Quotes erd
\end_inset

.
\end_layout

\begin_layout Subsection*
1 February 2026
\end_layout

\begin_layout Standard
Can I have a quiet day,
 and get some work done?
 I want to return to counting.
 To count words in texts,
 I need to build a filesystem crawler.
 It crawls trees.
 Well,
 actually I build one a year ago,
 and I used to crawl my file system,
 and now I have 400K links that describe my file system:
 that is,
 I have a 
\begin_inset Quotes eld
\end_inset

model
\begin_inset Quotes erd
\end_inset

 of my file system,
 as it was a year ago.
 This model includes file metadata:
 name,
 type,
 size,
 the usual file attributes.
 So I don't need to (at this particular instant in time) to create another,
 better,
 different crawler.
 I already have this model.
\end_layout

\begin_layout Standard
So the question then becomes:
 how do I crawl this model?
 This turns into a question:
 how do I crawl any large complex structure that is already in the AtomSpace?
 The starting point could be a tally of all the different atom types.
\end_layout

\begin_layout Standard
This can be gotten 
\begin_inset Quotes eld
\end_inset

directly
\begin_inset Quotes erd
\end_inset

 from the AtomSpace,
 but more rigorously by using the query engine with a lone unrestricted Variable,
 followed by a TypeOf,
 followed by an IncrementValue.
 So I did this in atomspace–viz,
 and it works.
\end_layout

\begin_layout Standard
After this,
 I run into trouble.
 Some maybe–interesting questions:
\end_layout

\begin_layout Itemize
What is the average size of the incoming set of an Atom?
\end_layout

\begin_layout Itemize
What is the average size of the incoming set of an Atom of type T?
 (visualized as a graph that is a function of T)
\end_layout

\begin_layout Itemize
What is the average size of the outgoing vector of a Link of type T?
 (visualized as a graph...)
\end_layout

\begin_layout Itemize
How many Links are there having outgoing vector size 1,2,3,...
 ?
\end_layout

\begin_layout Itemize
How many Atoms are there having an incoming set of size 0,1,2,3...
 ?
\end_layout

\begin_layout Itemize
What is the average height of the trees?
 (of type T,
 visualized as a graph?) i.e.
 starting with those Atoms (Links) that have an incoming set of size 0,
 what is the distance to the farthest Node under them?
\end_layout

\begin_layout Itemize
How many disconnected components are there?
 That is,
 unreachable by tracing through incoming,
 outgoing sets?
 How does this change if I don't allow reachability through certain types?
 This is hard – the query engine does this under–the–covers,
 as it walks,
 and imposes a zillion constraints while walking,
 but it does not keep a tally of connected components.
 I don't have a 
\begin_inset Quotes eld
\end_inset

match any tree
\begin_inset Quotes erd
\end_inset

 query.
 Hmm.
 Maybe with GlobNodes,
 stacked ...
 trees of specific size enumerated in ChoiceLink.
 Whatever.
 This is an interesting question,
 and exposes a missing API to the query engine.
\end_layout

\begin_layout Itemize
Pair–wise correlations.
 Pick any two locations in a tree (in the abstract) and show the MI between those locations (or rather,
 the MI distribution over all trees.)
\end_layout

\begin_layout Standard
I'll stop here,
 and meta–critique.
 All these are interesting questions that may have curious answers,
 but they don't offer much in 
\begin_inset Quotes eld
\end_inset

understanding
\begin_inset Quotes erd
\end_inset

 the dataset.
 Having these could be useful for categorizing different datasets with respect to one–another,
 but,
 for just one dataset,
 the value is low.
 That they have any value at all is only due to my own personal experience with trees and graphs,
 that has been gained over time.
 That is,
 these numbers are interesting to me,
 personally.
 The intrinsic value is mostly arises when they are used to compare to other datasets.
 The only other value is if/when a system has some general abstract 
\begin_inset Quotes eld
\end_inset

knowledge
\begin_inset Quotes erd
\end_inset

 of 
\begin_inset Quotes eld
\end_inset

what is a tree
\begin_inset Quotes erd
\end_inset

,
 and therefore can align specific information about collections of specific trees with respect to this general abstract knowledge–base of what trees are.
\end_layout

\begin_layout Standard
Well,
 so one issue is that if I place myself as a proxy for a 
\begin_inset Quotes eld
\end_inset

system that posses abstract knowledge of how trees work
\begin_inset Quotes erd
\end_inset

,
 the above statistical information still does not give me very much insight.
 I know that this tree data is a snapshot of my file system,
 but so what?
 Grep,
 find and locate are still my tools of choice,
 for the file system.
 I don't have anything like grep,
 find and locate for the AtomSpace;
 I have something more powerful,
 but also more verbose.
 Which is OK,
 because the mantra is machine–accessibility,
 not human–accessibility.
 But now that I have made it machine–accessible,
 now what?
\end_layout

\begin_layout Standard
There are several side–tracks to go off on.
 
\end_layout

\begin_layout Itemize
How does the machine obtain a collection of inbuilt knowledge about trees?
 This is about axiomatic knowledge.
\end_layout

\begin_layout Itemize
What is the human interface to this knowledge set?
 I would like to use chat to interface to it.
\end_layout

\begin_layout Itemize
What happens when I apply my age–old day–dream of leveraging up the structural stack from pair–counting?
\end_layout

\begin_layout Standard
The first and third bullets are supposed to be interconnected:
 infer axioms by observation of structure in data sets.
 I now have to revise these ideas.
\end_layout

\begin_layout Standard
The original idea was that tokens arrive in a time–like stream,
 and I do correlational counting of pairs of tokens that appear near each other,
 in a window of some size in the stream (e.g.
 six words,
 or a sentence that ends with punctuation;
 whatever.) Then after pair counting,
 obtain LG–style disjuncts,
 and then perform clustering on those disjuncts.
\end_layout

\begin_layout Standard
Now,
 my tokens no longer arrive in a stream;
 they already have some innate structure with respect to one–another.
 My tokens have already been organized into a graph.
 Although this is a file–system graph,
 in the present case,
 it could have been a parse–graph:
 something that arises after the first step of counting –> disjuncts –> parsing has been performed.
 Curiously,
 I never had a good plan for that before.
 Or rather,
 I have a naive plan:
 look for pairs,
 again.
\end_layout

\begin_layout Standard
The problem here is I have lots of explicitly different pair–relationships.
 Thus,
 I store not only the pair–count,
 but the specific relationship between the pairs.
 Well,
 the old matrix code already did this:
 the matrix was always defined abstractly,
 in terms of a pair of locations specified by an underlying pattern.
\end_layout

\begin_layout Standard
One experiment would be to make a catalog of all tree–shapes in the dataset,
 then,
 for each tree,
 insert variables in every possible location,
 and do pair–wise counting on those.
\end_layout

\begin_layout Subsection*
3 February 2026
\end_layout

\begin_layout Standard
Why am I procrastinating?
 Because I know heavy lifting lies ahead.
 And here I am,
 evening,
 blowing it after wasting a nice morning taking care of mundane stuffs.
 Alas.
 Well,
 lets give it a try,
 anyway.
\end_layout

\begin_layout Standard
What is knowledge?
 What do I mean by 
\begin_inset Quotes eld
\end_inset

knowledge
\begin_inset Quotes erd
\end_inset

?
 What do I want my hypothetical AI to accomplish?
 How can it be accomplished?
 How could it all work?
 These are the questions to explore.
 I have some answers,
 but lets work up to these.
\end_layout

\begin_layout Standard
So I was talking about a record of the files held on my collection of computers (real and virtual,
 lxc and docker containers,
 datasets,
 backup drives...) What is there to know about these files?
 Well,
 some are direct copies (backups,
 archives...) some are functionally similar (different versions of /bin/bash compiled for different OS versions...) some are same in format,
 but otherwise random (mp3 collection,
 photo collection,
 email collection,
 datasets from experiments,
 source code archives...) What is there to 
\begin_inset Quotes eld
\end_inset

understand
\begin_inset Quotes erd
\end_inset

 about this file collection?
 Not much,
 beyond what I describe here.
 Want to know more?
 Don't ask me;
 as an LLM front end that can 
\begin_inset Quotes eld
\end_inset

see
\begin_inset Quotes erd
\end_inset

 the file–system contents.
 What is there to 
\begin_inset Quotes eld
\end_inset

see
\begin_inset Quotes erd
\end_inset

?
 Well not much more than the above.
 ...
\end_layout

\begin_layout Standard
So what might I ask the LLM about my file system?
\end_layout

\begin_layout Itemize
How many lxc containers do I have?
 Which ones have I not used recently?
 Can I delete some of them?
\end_layout

\begin_layout Itemize
I seem to have misplaced the dataset of Project Gutenberg books ...
 where are they?
 What about the SFIA dataset?
 Surely I have some copy somewhere that is NOT in an lxc container,
 right?
\end_layout

\begin_layout Itemize
Crap,
 when I moved to CephFS,
 I did not know that it has the terrible flaw of sometimes zeroing out file contents;
 do I have some copy somewhere of this file that is not all zeros?
 Can I find all the files for which I have a viable backup copy?
 Can I identify the files which have been permanently lost?
\end_layout

\begin_layout Standard
So how would one make the above work?
 Lets review what does not work:
 the old idea of 
\begin_inset Quotes eld
\end_inset

pair–wise MI
\begin_inset Quotes erd
\end_inset

 –> disjuncts/jigsaws –> higher–level correlations mostly does not work,
 because there's not much to correlate.
\end_layout

\begin_layout Standard
Most of the above questions can be solved by applying old–fashioned find with grep.
 Since Claude knows about find+grep,
 I could just let it loose on my file system.
 But since I don't want to (can't) let it loose on everything (because some of the contents is offline,
 (e.g.
 backup disks,
 USB sticks...) and others require remote login or firewall hole–punches.
 This would require grep/find–like abilities for the AtomSpace.
 Which already exist via MeetLink,
 etc.
 and so this is about teaching Claude how to write custom Atomese queries.
\end_layout

\begin_layout Standard
Next,
 I have the problem of 
\begin_inset Quotes eld
\end_inset

current Claude
\begin_inset Quotes erd
\end_inset

 (for which there is a monthly subscription fee) vs.
 
\begin_inset Quotes eld
\end_inset

what can I run privately on my own dinky GPU
\begin_inset Quotes erd
\end_inset

,
 and then 
\begin_inset Quotes eld
\end_inset

future less dinky GPU that I can own that will be capable of today's Claude
\begin_inset Quotes erd
\end_inset

 vs.
 
\begin_inset Quotes eld
\end_inset

future subscription service
\begin_inset Quotes erd
\end_inset

.
 The issue is that,
 although I can design and test only for today,
 I takes a long fucking time to design and test,
 and so therefore I should target future systems.
\end_layout

\begin_layout Standard
Lets take a step back.
 The old–fashioned locate/plocate/slocate systems build an index of file names,
 and this pre–built index is searchability so easy to do.
 I could also build an index in the AtomSpace.
 How should I package it?
 Well,
 I could have a core AtomSpace,
 containing the file–system model,
 with no indexes,
 and then a layer on top of it (child AtomSpace) holding index data.
 Current infrastructure does not allow me to store this higher layer without storing the lower layer,
 so current StorageNode design would need some work to make this run.
\end_layout

\begin_layout Standard
If I think abstractly in terms of 
\begin_inset Quotes eld
\end_inset

science data sets
\begin_inset Quotes erd
\end_inset

,
 then I can imagine all sorts of abstracts and summaries can be developed.
 I once read through the Vera Rubin telescope data product catalog,
 and it was quite remarkable,
 what kind of 
\begin_inset Quotes eld
\end_inset

products
\begin_inset Quotes erd
\end_inset

 one might have available,
 and the various time–scales at which they are available.
 What are these products?
 They are (abstractly) different kinds of algorithms (some very complex and sophisticated) applied to raw pixels,
 and then categorized into indexes.
 Which can then be filtered.
 All this is hand–built.
 I suppose someone has already attached an LLM query system to this product catalog ...
\end_layout

\begin_layout Standard
And so another step back:
 the meta–meta layer.
 Ordinary (but smart) grad students in astronomy are very busy fiddling with LLM's and attaching them to astronomy datasets and discovering new astronomical phenomena).
 Other ordinary grad students are applying LLM's as coding wizards to the various astrometric algorithms,
 to improve their operation.
 And so these advances are happening 
\begin_inset Quotes eld
\end_inset

full speed ahead
\begin_inset Quotes erd
\end_inset

 without the intervention of any kind of AI I might imagine myself to be designing,
 and so turn around and ask 
\begin_inset Quotes erd
\end_inset

what is knowledge,
 anyway?
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Standard
So again,
 using my own file system as 
\begin_inset Quotes eld
\end_inset

a dataset
\begin_inset Quotes erd
\end_inset

,
 what do I hope to 
\begin_inset Quotes eld
\end_inset

perceive
\begin_inset Quotes erd
\end_inset

 in it?
 Well,
 I can hand–design various custom MeetLinks that extract some of the data needed to answer the questions above.
 How?
\end_layout

\begin_layout Standard
lets tackle the first:
 where are all the lxc containers?
 Well,
 on one system,
 I happen to 
\begin_inset Quotes eld
\end_inset

know
\begin_inset Quotes erd
\end_inset

 that they are located at /home/lxc,
 but on another system,
 I put them somewhere else.
 How do I know they are LXC?
 Well,
 not just due to location,
 but tell–tale signs,
 like the names of config files,
 and the fact that some config files have a certain very peculiar form.
 That,
 plus the fact that there is a root directory,
 with /bin,
 /usr,
 /var and /tmp...
 this common FSSTND file hierarchy.
 How can I discover this hierarchy?
 (How can my algo discover this hierarchy?) Well,
 if I really do want to ratchet up 
\begin_inset Quotes eld
\end_inset

from nothing
\begin_inset Quotes erd
\end_inset

,
 then perhaps the pair–count -> jigsaw approach can discover that there are similar fs hierarchies scattered about.
 The experiment that has not yet been done is to prove that this is indeed discoverable in this way.
 So I guess that is a big to–do item.
\end_layout

\begin_layout Standard
But there are two meta–issues.
 One is that I would like to attach this subsystem to an LLM,
 today (using current technology).
 The other is the 
\begin_inset Quotes eld
\end_inset

how do I discover algorithms?
\begin_inset Quotes erd
\end_inset

 question.
 So,
 a higher–order structure of data correlation is ...
 a 
\begin_inset Quotes eld
\end_inset

static
\begin_inset Quotes erd
\end_inset

 structure,
 in that it just 
\begin_inset Quotes eld
\end_inset

exists
\begin_inset Quotes erd
\end_inset

;
 it is NOT an algorithm.
 This higher–order structure might be describable as a collection of jigsaws,
 with the connector semantics describing how these connectors are to be connected,
 but this requires a parsing algorithm,
 and where did that algorithm come from?
 Curry–Howard correspondence says 
\begin_inset Quotes eld
\end_inset

proofs are programs
\begin_inset Quotes erd
\end_inset

 but this is too distant from the more immediate situation of having a structure,
 and having an algorithm that can perceive that structure (never mind perceive it efficiently).
\end_layout

\begin_layout Subsection*
4 February 2026
\end_layout

\begin_layout Standard
So I'm still struggling to close a circular loop,
 here.
 I can do pair–counting,
 if I can get a crawler agent.
 I can get a crawler agent,
 if I hand–code it.
 But then I have to hand–code where it is going to go,
 and what it will do.
 So I have to think hard about algorithms.
 And when I think hard about algorithms,
 I say to myself 
\begin_inset Quotes eld
\end_inset

I know how to do pair–counting
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

pair–counting does not solve my problem of understanding file system contents
\begin_inset Quotes erd
\end_inset

.
 I also don't have a user interface:
 some why to talk to my system.
\end_layout

\begin_layout Standard
What options do we have on the table?
\end_layout

\begin_layout Itemize
Continue to agonize about the big plan.
 But this is best done in the background,
 quietly,
 not writing:
 I jumble things around in my head that are too hard to verbalize and formulate into a plan.
 Architectural issues.
\end_layout

\begin_layout Itemize
I can port a minimal atomspace to the GPU,
 so that the graph network sits there,
 in the GPU RAM.
 Maybe later I can have algos that access values on some atoms,
 i.e.
 a port of the matrix code to the GPU.
 This seems like a needed step at some point,
 but when?
\end_layout

\begin_layout Itemize
I can search for a small,
 non–cloud non–proprietary LLM that I can run locally.
 But why,
 when the cloud versions are usable,
 even if they cost?
 I can also generate my own electricity,
 grow my own food,
 have some chickens in the back yard ...
 which unplugs from the existing social infrastructure.
 So,
 vaguely appealing,
 and vaguely impractical.
\end_layout

\begin_layout Itemize
I can have Claude write mountains of ad hoc code for me,
 with the risk that it is all an unmaintainable pile of slop,
 collapsing under it's own weight.
\end_layout

\begin_layout Itemize
I can design and modify a storage proxy that allows me to layer index and analysis on top of other AtomSpaces,
 pursuing a line of research resembling membrane computing.
 In that each AtomSpace is behind a membrane,
 and there are information flows between AtomSpaces.
 For example,
 an AtomSpace might be located in GPU RAM,
 but how does info flow to it?
 For example,
 and AtomSpace might contain filesys info,
 and a layer on top of it contains analytics and indexes.
 All this needs some heads down tinkering with that code.
\end_layout

\begin_layout Itemize
I can resume pair–counting with words and just try to leverage as far as possible,
 ignoring everything else.
\end_layout

\begin_layout Itemize
I can revive pair–counting of image or sound data.
\end_layout

\begin_layout Itemize
The pair–counting recursive step requires ...
 well,
 the path is clear and direct,
 but there's just a lot of work in front of me.
 And,
 in some sense,
 the results are guaranteed to be underwhelming,
 because 3-4-5 recursive steps are required before it is possible to compete with present–day industrial LLM's.
\end_layout

\begin_layout Itemize
But what would be the point of that?
 Suppose I can build some system that does structural analysis at a lower and more basic level than LLM's ...
 so what?
 I still have some soul–less machine.
\end_layout

\begin_layout Standard
And thus back to agonizing about next steps.
 I've got competing urges,
 one of which is to just code as fast as possible,
 the other is to step back and take in the big picture,
 and develop a plan that 
\begin_inset Quotes eld
\end_inset

goes somewhere
\begin_inset Quotes erd
\end_inset

.
\end_layout

\begin_layout Standard
At the crux of the plan are two issues.
 They are:
\end_layout

\begin_layout Itemize
My structuralist approach is still several steps away from a 
\begin_inset Quotes eld
\end_inset

recursive Curry–Howard system
\begin_inset Quotes erd
\end_inset

,
 where I can (a) observe nature (b) extract structures from that nature (c) extract algorithms appropriate for (dual to?) those structures (d) deploy those algorithms.
 I know how to do step (b),
 but I don't know how to do step (c).
 I can,
 for example,
 read the wikipedia article on Curry–Howard,
 fine.
 But if I have a link–grammar parse of some structural data (which could be words,
 photos,
 sounds) I don't know how to convert a parse of,
 say,
 and English–language sentence,
 into an algo.
 Now,
 the parser itself was a program,
 and there is some Curry–Howard corresponding structure to my parser program,
 but that corresponding structure is large and complex.
 I don't yet know how to 
\begin_inset Quotes eld
\end_inset

learn a parsing algorithm
\begin_inset Quotes erd
\end_inset

 by 
\begin_inset Quotes eld
\end_inset

observing data
\begin_inset Quotes erd
\end_inset

.
 I don't yet know how to learn any algo...
\end_layout

\begin_layout Itemize
Hmm.
 Well,
 
\begin_inset Quotes eld
\end_inset

generative
\begin_inset Quotes erd
\end_inset

 is the conventional answer.
 So,
 generate a bunch of puny,
 weeny algos (using some external,
 a priori,
 engineer-given generator),
 and see which of these puny weeny algos generate patterns that correspond to the observed dataset.
 This goes back to the L–systems idea mentioned above.
\end_layout

\begin_layout Itemize
The problem here is similar to what MOSES faced:
 the evolutionary,
 selective pressures are weak,
 and one hits a wall of Kolmogorov complexity and combinatorial explosion quite early in this process.
 There's no direct way to evolve the system automatically without taking a million years.
\end_layout

\begin_layout Itemize
So instead,
 we do engineering:
 we,
 as humans,
 invent algorithms.
 Before LLM's,
 we relied on printed literature,
 research papers,
 peers and collaborators,
 and open source to provide the needed infrastructure to add one more pieces–part to whatever system we were envisioning.
 At any rate,
 we did engineering,
 in the conventional human sense.
 
\end_layout

\begin_layout Itemize
The arrival of the LLM's allow engineering to be done more quickly.
\end_layout

\begin_layout Itemize
To what degree can I take 
\begin_inset Quotes eld
\end_inset

human out of the loop?
\begin_inset Quotes erd
\end_inset

 Because that is roughly what I am imagining,
 unspoken,
 unvoiced,
 in the previous ruminations about structural learning:
 building a machine that can perceive structure,
 and doing so without personal intervention.
 Have it run on automatic,
 as it were.
\end_layout

\begin_layout Itemize
If I ignore the fact that I'm talking about software,
 here,
 this is akin to what a CEO does:
 he builds a corporation that 
\begin_inset Quotes eld
\end_inset

runs on automatic
\begin_inset Quotes erd
\end_inset

:
 a zillion employees 
\begin_inset Quotes eld
\end_inset

doing their job
\begin_inset Quotes erd
\end_inset

,
 and the CEO with some light hand guiding the processes.
 The obvious problem here is that there are few CEO–less corporations that can truly run head–less and not crash and burn.
\end_layout

\begin_layout Itemize
The other problem with the above is that many such supra–human systems are toxic.
 As argued before,
 WWI and the tobacco industry were both toxic to humanity.
 WWI explicitly so:
 a young–man killing machine that only stopped when it ran out of young men.
 Nicotine is more interesting,
 since it is an explicit neurotransmitter,
 so the tobacco industry is a supra–human feedback loop that hooks into a very low cellular level in the brain.
 The toxicity of tobacco smoke is an unfortunate side–effect.
 The coffee industry is more neutral,
 but does not get the shock value of tobacco that makes people sit up and pay attention.
 And pointing to the agricultural industry,
 or the cell–phone industry is too abstract;
 people stop listening because they think they know what these are,
 without much considering or grasping the feedback effects of the supra–human and the micro–biological ties.
\end_layout

\begin_layout Itemize
Anyway,
 we live entangled in all of these networks,
 nominally called 
\begin_inset Quotes eld
\end_inset

economics
\begin_inset Quotes erd
\end_inset

,
 but also 
\begin_inset Quotes eld
\end_inset

culture
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

society
\begin_inset Quotes erd
\end_inset

 (and government ...) and so tying on yet another feedback loop into the system is not extraordinary per se.
 What is different is that Moore's law changes everything.
 All the cognizenti talk about this already,
 endlessly and breathlessly in every forum imaginable,
 so why am I repeating it here?
 Hmmm.
\end_layout

\begin_layout Itemize
Because when I pull on the thread of 
\begin_inset Quotes eld
\end_inset

how do I build an autonomous system?
\begin_inset Quotes erd
\end_inset

 that is where the thread takes me.
 
\begin_inset Quotes eld
\end_inset

what is autonomy,
 really?
\begin_inset Quotes erd
\end_inset

 if we're all plugged into the same system.
\end_layout

\begin_layout Itemize
It also hints at the scaling problem.
 When I attempt to create system out of pure Atomese,
 it is as if I am trying to create systems (metaphorically) using electrons and protons as my building blocks,
 when common sense says that I should not build with electrons and protons,
 but merely ask society to provide me with those in bulk,
 in the form of concrete and rebar.
 My retort is ...
 I'm not a structural engineer,
 designing housing,
 I'm a research scientist,
 so I work with ..
 protons and electrons,
 directly.
\end_layout

\begin_layout Itemize
And yet I feel obligated to try to leverage LLM technology somehow,
 because I am,
 after all,
 attempting to build a system that 
\begin_inset Quotes eld
\end_inset

sees the world
\begin_inset Quotes erd
\end_inset

,
 and in so seeing,
 interacts with it.
\end_layout

\begin_layout Itemize
I'm just deeply confused,
 because my natural predilection is to build little ant–like robotic systems that do cute things,
 and yet I nominally claim to work on AGI,
 and therefor should be working on creating something large and complex and all–encompassing and all–powerful.
\end_layout

\begin_layout Itemize
And then I go around:
 building the all–powerful is well,
 dangerous.
 Not unlike the atom bomb is dangerous.
 So wtf...
\end_layout

\begin_layout Itemize
So this says that attempts to build the 
\begin_inset Quotes eld
\end_inset

all powerful
\begin_inset Quotes erd
\end_inset

 are foolish and misdirected.
\end_layout

\begin_layout Itemize
So I can instead tinker with little itty–bitty pieces,
 doing the scientific equivalent of stamp–collecting or bug–collecting or any one of the other non–sexy minimally–valued scientific pursuits:
 stuff that will never get you on the cover of some premiere science journal,
 never mind main–stream media that pays attention only when Nobel prizes are being awarded.
 That is,
 it is OK to toil in obscurity on unimportant things.
\end_layout

\begin_layout Itemize
So is that my plan?
 To toil in a semi–focused,
 haphazard fashion,
 slowly knocking off each of the small little baby–steps that I need to get done,
 to realize the next stage of my plan to build some cute little mechanical device that does something vaguely cool–ish?
 Again,
 nothing wrong with that,
 I guess...
\end_layout

\begin_layout Itemize
I'm just playing the prairie dog here,
 popping up my head to look around,
 and see what else is up.
\end_layout

\begin_layout Standard
Ohh.
 Shit I almost forgot:
 I thought of this three,
 four months ago,
 and still have not really written or designed about this.
 This is to give Claude the ability to remember things (i.e.
 in the AtomSpace) and to recall things 
\begin_inset Quotes eld
\end_inset

as needed
\begin_inset Quotes erd
\end_inset

 (priority and attention queue) and act one them (incorporate them into current thinking) and then file results back into the AtomSpace (long–term memory.) This single sentence sketch evokes a rather traditional AGI engineering plan:
 short–term and long–term memory,
 the bridge between the two,
 the need to provide an attention mechanism that focuses the thinking of the LLM in a certain direction.
\end_layout

\begin_layout Standard
This is a kind–of fun thing to do,
 and I kind of got started doing exactly this,
 3–4 months ago,
 with Claude.
 It got put on hold.
 I did not hit any brick wall,
 but it was definitely an uphill climb.
 basically,
 I was trying to create a long–term memory as a collection of prompts,
 and an indexing system of prompts–that–are–indexes of other prompts,
 and then,
 because text does not scale,
 to somehow do this in the AtomSpace,
 and then somehow use this as a tap into the attention system.
 And this is kind–of–ish where I hit the wall,
 sort of,
 thinking that I could tap into the attention vectors much more directly,
 if I could have the LLM running locally on my machine,
 and twinking the weights directly.
 Roughly speaking,
 this is the IIT issue:
 I need close ties into the system;
 the connection via prompts is too distant,
 too remote,
 and I wanted to have a much closer and tighter integration between weight–vectors in the NN and the specific symbols held in the AtomSpace.
\end_layout

\begin_layout Standard
So this minimal sketch suggests that:
\end_layout

\begin_layout Itemize
I should in fact look for some minimalist LLM that I can run fully locally on my local GPU,
 so that I can directly tap into it's weight vectors,
 and
\end_layout

\begin_layout Itemize
I should port some minimalist AtomSpace that will sit in the GPU RAM,
 so that it can be used to directly poke weights.
\end_layout

\begin_layout Standard
Which seems like a good plan.
\end_layout

\begin_layout Standard
The agony queen here is that I still don't have a good observer of my file system,
 which I really do want,
 and my gut sense is I really need to push forward on pair counting anyway.
\end_layout

\begin_layout Standard
And I can twink and twerk with all of these all at the same time,
 except I have an external constraint that I have to build something along a path that appears to be immediately and intuitively obvious and useful to my current employer,
 and prospective future employers.
 Or even to my fan club...
 And this last constraint is a source of stress.
 Urgh.
\end_layout

\begin_layout Standard
Well,
 it seems all I've got so far is to continue to twink and twerk whatever sets me afire that day,
 and if its not the right thing to do,
 well,
 its a tragedy,
 but life (or rather,
 death?) is coming at me like a bullet,
 and I'm trying to dodge and weave as best I can,
 and it seems like the future,
 as it comes,
 will come in an inevitable way.
 OK,
 so that's a plan,
 I guess.
 Enough.
 Later,
 dude.
\end_layout

\begin_layout Subsection*
4 February 2026 Ongoing planning
\end_layout

\begin_layout Standard
Now for some LLM slop.
 I ask:
 
\begin_inset Quotes eld
\end_inset

Are there any (open source) LLM's that can run on a small-to-midrange commodity GPU board that also have MCP support in them?
\begin_inset Quotes erd
\end_inset

 The answers are:
\end_layout

\begin_layout Itemize
Ollama with tool calling models like Qwen3 ...
 but also Llama 3.1/3.2 8B,
 Mistral 7B DeepSeek R1 8B ...
\end_layout

\begin_layout Itemize
Use ollcmp or ollama–cmp–bridge or AnythingLLM or LibreChat or Chainlit to provide explicit MCP interfaces.
\end_layout

\begin_layout Standard
So there's a panoply of options and pieces–parts.
 Should I mess with this today?
\end_layout

\begin_layout Subsection*
8 February 2026
\end_layout

\begin_layout Standard
In a discord chat,
 I wrote this:
 
\end_layout

\begin_layout Standard
Well,
 if you as the scientist,
 expect a dataset with all pairs in it and instead you get one with low_MI pairs discarded ..
 that comes as a surprise.
\end_layout

\begin_layout Standard
but also ..
 MI depends on everything so if you discard the low–MI stuff,
 the MI of everything else changes.
\end_layout

\begin_layout Standard
a science question might be:
 "what do I get when I sample randomly?" and when you get bored of that,
 then the next question is "what do I get when I cut away parts p,q,r of the dataset?" and ...
 well,
 understanding the answer to the second question is hard,
 if you (I) don't know what p,q,r were cut.
\end_layout

\begin_layout Standard
nordizvaus
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
4:12 PM
\end_layout

\begin_layout Standard
Ok separate question ,
 so you recommend not cutting out mi < 1.0 pairs ?
\end_layout

\begin_layout Standard
Linas
\end_layout

\begin_layout Standard
—
 
\end_layout

\begin_layout Standard
4:17 PM
\end_layout

\begin_layout Standard
if you sample randomly and keep all pairs,
 then you get a nice bell curve.
 Bell curves look like parabolas on a semi-log plot.
\end_layout

\begin_layout Standard
If you cut off the left half of a bell curve,
 and recompute the MI,
 you'll get some lop–sided curve,
 but I have no intuition for that shape,
 or how to think about it.
\end_layout

\begin_layout Standard
Turns out that bell curves correspond to points uniformly distributed over a very high–dimensional sphere.
 Now,
 pick a point x on that sphere,
 call it "the north pole",
 and throw away everything on the southern hemisphere of that point.
 Do that from the perspective of each point.
 ...
\end_layout

\begin_layout Standard
It's like an event horizon.
 throw away all points on the other side of the event horizon from "me",
 where "me" is some given word,
 and the event horizon is located at 1.23 < MI("me".
 "other word") 
\end_layout

\begin_layout Standard
So I wrote the above,
 and as I wrote it,
 my head started exploding ...
 what if this is not just a weird analogy,
 but instead there is something literally true about it?
 Things on the other side of an event horizon are literally unreachable to us.
 And we know that QM wave functions live in a high–dimensional (
\begin_inset Quotes eld
\end_inset

infinite–dimensional Hilbert
\begin_inset Quotes erd
\end_inset

) space,
 so is there something where there's some cut–off,
 where the high–MI stuff is our past lightcone and the low–MI stuff is spatially separated?
 What if this analogy is literally true in some way?
\end_layout

\begin_layout Standard
So that's one.
 And then a far more grounded question:
 what does happen to a space,
 when you discard all low–MI pairs?
 I have no intuition for that.
 It also needs thinking and re–thinking.
 (and scratching out some formulas.
 maybe later.)
\end_layout

\begin_layout Subsection*
8 Feb 2026 Later
\end_layout

\begin_layout Standard
Its late,
 I'm tired,
 this will probably go badly.
 But I will agonize if I don't try.
 Claude math session.
 I ask Claude questions and try to stitch together a coherent story.
\end_layout

\begin_layout Paragraph*
Cosine Distribution for Random Vectors in High Dimensions
\end_layout

\begin_layout Standard
Suppose I have a sphere in 
\begin_inset Formula $N$
\end_inset

 dimensions,
 where 
\begin_inset Formula $N$
\end_inset

 is large,
 say 1K or 10K.
 Suppose I sprinkle 
\begin_inset Formula $M\times N$
\end_inset

 random points in that sphere,
 for 
\begin_inset Formula $M\approx\sqrt{N}$
\end_inset

.
 What formula gives the distribution of cosines between pairs of vectors?
\end_layout

\begin_layout Standard
(Below is my imaginative rewrite of what Claude told me.)
\end_layout

\begin_layout Standard
The number of points doesn't matter for the pairwise distribution.
 Each pair of points is independently sampled.
 For two independent vectors drawn uniformly from the 
\begin_inset Formula $N$
\end_inset

–dimensional sphere 
\begin_inset Formula $S^{N-1}$
\end_inset

 the cosine is 
\begin_inset Formula $c=\mathbf{x}\cdot\mathbf{y}/|\mathbf{x}||\mathbf{y}|$
\end_inset

 and has density 
\begin_inset Formula $f\left(c\right)=\cdots$
\end_inset

 and I have to coax from Claude why its that formula.
 
\end_layout

\begin_layout Standard
Picking random vectors in 
\begin_inset Formula $\mathbb{R}^{N}$
\end_inset

 normalized to unit length gives the sphere 
\begin_inset Formula $S^{N-1}$
\end_inset

.
 The 
\begin_inset Quotes eld
\end_inset

ring
\begin_inset Quotes erd
\end_inset

 at polar angle 
\begin_inset Formula $\theta$
\end_inset

 is 
\begin_inset Formula $S^{N-2}$
\end_inset

 and has radius 
\begin_inset Formula $\sin\theta$
\end_inset

 so it's area scales as 
\begin_inset Formula $\sin^{N-2}\theta$
\end_inset

.
 So the distribution of sine's is 
\begin_inset Formula $\sin^{N-2}\theta\;d\theta$
\end_inset

 which picks up a Jacobian 
\begin_inset Formula $ds=\cos\theta\;d\theta$
\end_inset

 so 
\begin_inset Formula $ds=d\theta/\sqrt{1-s^{2}}$
\end_inset

 so the distribution of sines is 
\begin_inset Formula 
\[
g\left(s\right)\propto\frac{s^{N-2}}{\sqrt{1-s^{2}}}
\]

\end_inset

and so for cosines,
 the distribution is 
\begin_inset Formula 
\[
f\left(c\right)\propto\frac{\sin^{N-2}\theta}{\sin\theta}=\sin^{N-3}\theta=\left(1-c^{2}\right)^{\left(N-3\right)/2}
\]

\end_inset

The normalization is given by the beta function 
\begin_inset Formula 
\[
\text{Beta}\left(a,b\right)=\int_{0}^{1}t^{a-1}(1-t)^{b-1}\,dt=\frac{\Gamma(a)\,\Gamma(b)}{\Gamma(a+b)}
\]

\end_inset

and so putting it together,
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
f(c)=\frac{1}{\text{Beta}\!\left(\frac{1}{2},\,\frac{N-1}{2}\right)}\,(1-c^{2})^{(N-3)/2},\qquad c\in[-1,1]
\]

\end_inset


\end_layout

\begin_layout Standard
Equivalently,
 
\begin_inset Formula $\left(1+c\right)/2\sim\text{Beta}\!\left(\frac{N-1}{2},\,\frac{N-1}{2}\right)$
\end_inset

.
\end_layout

\begin_layout Standard
For large 
\begin_inset Formula $N$
\end_inset

 this is is Gaussian,
 so 
\begin_inset Formula 
\[
c\approx\mathcal{N}\left(0,1/N\right)
\]

\end_inset

or 
\begin_inset Formula $c\sqrt{N}\approx\mathcal{N}\left(0,1\right)$
\end_inset

 In other words,
 in high dimensions,
 random vectors are almost orthogonal to one–another.
\end_layout

\begin_layout Standard
Well,
 OK.
 This is the correct answer to my question,
 but I guess I asked the wrong question...
 Let me think again...
\end_layout

\begin_layout Standard
Or not.
 It's late.
 I'm tired.
 Good night.
\end_layout

\begin_layout Subsection*
11 February 2026
\end_layout

\begin_layout Standard
Well,
 I was going to write about something entirely different,
 but then I re–read the above,
 and decided that I have that unfinished business.
 So I ask Claude a new question:
 
\end_layout

\begin_layout Paragraph*
Thresholding Random Vectors in High Dimensions
\end_layout

\begin_layout Standard
Suppose one has a collection of points,
 randomly distributed on a very high–dimensional sphere.
 These points can be understood as vectors,
 with vector components 
\begin_inset Formula $-1\le v_{i}\le+1$
\end_inset

 and of course 
\begin_inset Formula $\sum_{i}v_{i}^{2}=1$
\end_inset

.
 I want to pass these coordinates through a filter,
 to create a new vector 
\begin_inset Formula $t_{i}$
\end_inset

 such that
\begin_inset Formula 
\[
t_{i}=\begin{cases}
v_{i} & a<v_{i}\\
0 & v_{i}\le a
\end{cases}
\]

\end_inset

I would like to understand the distribution of these new vectors.
\end_layout

\begin_layout Standard
The answer?
 
\begin_inset Quotes eld
\end_inset

This is a nice problem with clean results in the high–dimensional limit.
\begin_inset Quotes erd
\end_inset

 But of course.
 I paraphrase Claude's reply below,
 shaped to my liking.
\end_layout

\begin_layout Standard
In high dimensions,
 each coord is approximately Gaussian,
 so 
\begin_inset Formula 
\[
v_{i}\approx\mathcal{N}\left(0,1/N\right)
\]

\end_inset

The typical scale of each coord is 
\begin_inset Formula $\sigma=1/\sqrt{N}$
\end_inset

 and the natural parameter for the cutoff is the scaled variant:
 
\begin_inset Formula $\alpha=a\sqrt{N}$
\end_inset

.
\end_layout

\begin_layout Standard
I'm not really interested in the energy 
\begin_inset Formula $\left|\vec{t}\right|^{2}=\sum_{i}t_{i}^{2}$
\end_inset

 but Claude told me and its interesting enough to repeat (and anyway,
 we'll need it as normalization,
 for later):
\begin_inset Formula 
\[
\left\langle \left|t\right|^{2}\right\rangle =\int_{\alpha}^{\infty}x^{2}\varphi\left(x\right)dx=\alpha\varphi\left(\alpha\right)+\int_{\alpha}^{\infty}\varphi\left(x\right)dx
\]

\end_inset

where 
\begin_inset Formula 
\[
\varphi\left(z\right)=\frac{e^{-z^{2}/2}}{\sqrt{2\pi}}\qquad\Phi\left(z\right)=\int_{z}^{\infty}\varphi\left(x\right)dx
\]

\end_inset

so for 
\begin_inset Formula $\alpha=0$
\end_inset

 we get an average energy of 1/2.
 
\end_layout

\begin_layout Standard
The expected value of each component is 
\begin_inset Formula $\left\langle t_{i}\right\rangle =\varphi\left(\alpha\right)/\sqrt{N}$
\end_inset

 and so the expected length is 
\begin_inset Formula 
\[
\left|\left\langle \vec{t}\right\rangle \right|=\sqrt{\sum_{i}\left\langle t_{i}\right\rangle ^{2}}=\varphi\left(\alpha\right)
\]

\end_inset

so the mean vector points in the all–ones direction.
\end_layout

\begin_layout Standard
The mean dot product between two truncated random vectors 
\begin_inset Formula $\vec{a}$
\end_inset

 and 
\begin_inset Formula $\vec{b}$
\end_inset

 is 
\begin_inset Formula 
\[
\left\langle \vec{a}\cdot\vec{b}\right\rangle =\varphi^{2}\left(\alpha\right)
\]

\end_inset

The expected cosine similarity is 
\begin_inset Formula 
\[
\cos\theta=\left\langle \hat{a}\cdot\hat{b}\right\rangle =\left\langle \frac{\vec{a}\cdot\vec{b}}{\left|a\right|\left|b\right|}\right\rangle =\frac{\left\langle \vec{a}\cdot\vec{b}\right\rangle }{\sqrt{\left\langle \left|a\right|^{2}\right\rangle \left\langle \left|b\right|^{2}\right\rangle }}=\frac{\varphi^{2}\left(\alpha\right)}{\alpha\varphi\left(\alpha\right)+\Phi\left(\alpha\right)}
\]

\end_inset

so for 
\begin_inset Formula $\alpha=0$
\end_inset

 one gets 
\begin_inset Formula $\cos\theta=1/\pi\approx0.318$
\end_inset

.
\end_layout

\begin_layout Standard
More interesting is this,
 which Claude gives without proof:
 if two non–truncated vectors 
\begin_inset Formula $\vec{s}$
\end_inset

,
 
\begin_inset Formula $\vec{t}$
\end_inset

 have an inner product 
\begin_inset Formula $\vec{s}\cdot\vec{t}$
\end_inset

 then,
 for the corresponding truncated vectors 
\begin_inset Formula $\vec{a}$
\end_inset

 ,
\begin_inset Formula $\vec{b}$
\end_inset

 one has,
 to leading order,
\begin_inset Formula 
\[
\vec{a}\cdot\vec{b}=\varphi^{2}\left(\alpha\right)+C\left(\alpha\right)\vec{s}\cdot\vec{t}+\mathcal{O}\left(\left(\vec{s}\cdot\vec{t}\right)^{2}\right)
\]

\end_inset

with 
\begin_inset Formula $C\left(\alpha\right)$
\end_inset

 some bivariate truncated Gaussian integral.
 Claude gives this the name of 
\emph on

\begin_inset Quotes eld
\end_inset

signal preservation
\emph default

\begin_inset Quotes erd
\end_inset

 and the second term 
\begin_inset Quotes eld
\end_inset


\emph on
carries the actual geometric signal
\emph default

\begin_inset Quotes erd
\end_inset

.
 Well,
 OK,
 Claude clearly learned this from some texts,
 but is this supposed to be metaphorical,
 or does it have a more formal validity?
 That is,
 is there a claim that there is some 
\begin_inset Quotes eld
\end_inset

information content
\begin_inset Quotes erd
\end_inset

 in the two non–truncated vectors 
\begin_inset Formula $\vec{s}$
\end_inset

,
 
\begin_inset Formula $\vec{t}$
\end_inset

 and that truncation reduces the information content?
 Clearly,
 truncation discards something,
 but what is that something being discarded?
\end_layout

\begin_layout Standard
The most interesting thing to pop up is this side–comment along the way:
 
\begin_inset Quotes eld
\end_inset


\emph on
The thresholded,
 mean–centered vectors then live approximately on a lower–dimensional manifold of dimension
\emph default
 
\begin_inset Formula $\sim N\Phi\left(\alpha\right)$
\end_inset


\begin_inset Quotes erd
\end_inset

 Wow!
 Really?
 So truncation is dimensional reduction!?
\end_layout

\begin_layout Standard
Maybe because I got a bump on my head,
 but I'm annoyed I don't know the above by heart.
 I got hospitalized,
 day before yesterday,
 for a concussion.
 I'm feeling woozy and carefree just right now.
 I refuse to think hard,
 and instead go with the flow here.
 So let's woozily (as I write this,
 I realize that it is no accident that this rhymes with 
\emph on
boozily
\emph default
...) The flow says 
\begin_inset Quotes eld
\end_inset

what dimensional reduction?
\begin_inset Quotes erd
\end_inset

 Let's find out...
\end_layout

\begin_layout Standard
While I wait for Claude's answer,
 I notice that it is bandying about the word 
\begin_inset Quotes eld
\end_inset

sparsity
\begin_inset Quotes erd
\end_inset

.
 Now,
 almost a decade ago,
 In my 
\begin_inset Quotes eld
\end_inset

sheaves
\begin_inset Quotes erd
\end_inset

 paper,
 I made the claim that one could (approximately) factor a sparse high–dimensional matrix 
\begin_inset Formula $M$
\end_inset

 into 
\begin_inset Formula $M=L^{T}KR$
\end_inset

 with 
\begin_inset Formula $K$
\end_inset

 being a dimensionally reduced dense matrix,
 and 
\begin_inset Formula $L$
\end_inset

,
 
\begin_inset Formula $R$
\end_inset

 being left and right factors.
 This was in the context of Link Grammar,
 where 
\begin_inset Formula $R$
\end_inset

 was the sets of words,
 organized into grammatical classes:
 viz lots and lots of English words,
 but only a few hundred(?) (almost a thousand?) grammatical classes,
 
\begin_inset Formula $K$
\end_inset

 was the collection of disjuncts,
 and the 
\begin_inset Formula $L$
\end_inset

 a bit harder to perceive,
 but was the recurring patterns in the disjuncts.
 Strictly speaking,
 the Link Grammar dictionary was just 
\begin_inset Formula $L^{T}K$
\end_inset

 and some work would have been needed to factorize this.
 At any rate,
 it was clear that writing a natural language parser was an exercise in dimensional reduction.
 For English,
 this is clearly a syntactic exercise;
 for Lithuanian,
 its clearly morpho–syntactic.
 The word–order is relatively free;
 the factoring is more explicitly in the lexis of suffixes.
 Hmm.
 I have to think about that.
\end_layout

\begin_layout Standard
The point is that sparsity does imply dimensional reduction,
 but perhaps the mistake being made here is that it is linear.
 And the fact that there's this huge statistical component to everything makes it confusing.
 What now pops into my mind is a botanical bush:
 the leaves are 2D,
 they are sparse,
 but they are arranged into an approximate space–filling arrangement,
 occupying a 3D volume.
 The point of that volume is that air is transparent to sunlight,
 and so the bush is optimizing for sunlight collection,
 while maintaining short branches,
 and encoding biological growth rules in some simple DNA encoding.
 That's the factorization.
 How much of this analogy can I push into more abstract settings?
\end_layout

\begin_layout Standard
OK,
 Claude's answer has arrived.
 
\begin_inset Quotes eld
\end_inset


\emph on
My original answer was imprecise and deserves pushback.
 What I should have said is
\emph default
 ...
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Standard
Well,
 duhh.
 OK,
 If I have an 
\begin_inset Formula $N$
\end_inset

–dimensional vector,
 and zero out some of the components,
 then,
 on average,
 there are exactly 
\begin_inset Formula $N\Phi\left(\alpha\right)$
\end_inset

 non–zero components left,
 and this is the dimensional reduction.
\end_layout

\begin_layout Standard
Next,
 it points out that the truncated space is not a manifold.
 So,
 this is more interesting.
 For every point 
\begin_inset Formula $\vec{v}\in S^{N-1}$
\end_inset

 define the 
\begin_inset Quotes eld
\end_inset

support set
\begin_inset Quotes erd
\end_inset

 
\begin_inset Formula $S\left(\vec{v}\right)$
\end_inset

 as the set of indexes (or basis vectors) that pass the thresholding:
\begin_inset Formula 
\[
S\left(\vec{v}\right)=\left\{ i\vert v_{i}>a\right\} 
\]

\end_inset

The cardinality is variable:
 
\begin_inset Formula $\left|S\right|=k$
\end_inset

 for some 
\begin_inset Formula $0\le k\le N$
\end_inset

.
 The support set defines an 
\begin_inset Quotes eld
\end_inset

orthant
\begin_inset Quotes erd
\end_inset

 (quadrant ...
 octant ...
 orthant) and the space splits into a union of orthants:
 
\begin_inset Formula 
\[
A^{N-1}\left(a\right)=\bigcup_{S\subseteq\left\{ 1,\cdots,N\right\} ,\left|S\right|=k}\mathbb{R}_{+}^{k}
\]

\end_inset

with 
\begin_inset Formula $\mathbb{R}_{+}=\left\{ x\in\mathbb{R}\vert x>a\right\} $
\end_inset

.
 
\end_layout

\begin_layout Standard
There are 
\begin_inset Formula $N$
\end_inset

 choose 
\begin_inset Formula $k$
\end_inset

 orthants,
 and as 
\begin_inset Formula $k$
\end_inset

 varies,
 we have
\begin_inset Formula 
\[
\frac{{N \choose k+1}}{{N \choose k}}=\frac{N!}{\left(k+1\right)!\left(N-k-1\right)!}\cdot\frac{k!\left(N-k\right)!}{N!}=\frac{N-k}{k+1}
\]

\end_inset

Hmm.
 I can't find anything interesting to say here.
\end_layout

\begin_layout Standard
Thus rapidly turns into binomial distribution games.
 If I take heads and tails to stand for keep/cut,
 then for a random draw,
 and a cutoff of 
\begin_inset Formula $\alpha$
\end_inset

 then I will have 
\begin_inset Formula $\Phi\left(\alpha\right)$
\end_inset

 tails in my string,
 on average.
\end_layout

\begin_layout Section*
The End
\end_layout

\begin_layout Standard
This is the end of Part Ten–F of the diary.
\end_layout

\end_body
\end_document
